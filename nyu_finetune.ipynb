{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from model import *\n",
    "\n",
    "from src.modeling.run_model_single import (\n",
    "    load_model, load_inputs, process_augment_inputs, batch_to_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../OncoNet_Public')\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchgpipe import GPipe\n",
    "import onconet.utils.parsing_with_line as parsing\n",
    "import onconet.models.factory as model_factory\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_parameters = {\n",
    "    \"device_type\": \"gpu\",\n",
    "    \"gpu_number\": 0,\n",
    "    \"max_crop_noise\": None,\n",
    "    \"max_crop_size_noise\": None,\n",
    "    \"batch_size\": 4,\n",
    "    \"seed\": 0,\n",
    "    \"augmentation\": False,\n",
    "    \"use_hdf5\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_number_generator = np.random.RandomState(shared_parameters[\"seed\"])\n",
    "\n",
    "image_only_parameters = shared_parameters.copy()\n",
    "image_only_parameters[\"view\"] = \"L-CC\"\n",
    "image_only_parameters[\"use_heatmaps\"] = False\n",
    "image_only_parameters[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "model, device2 = load_model(image_only_parameters)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "    \n",
    "def grayscale_images(image_path, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    print(\"Converting images to grayscale...\")\n",
    "    for img_file in os.listdir(image_path):\n",
    "        ii = cv2.imread(os.path.join(image_path, img_file))\n",
    "        gray_image = cv2.cvtColor(ii, cv2.COLOR_BGR2GRAY)\n",
    "        cv2.imwrite(os.path.join(output_path, img_file),gray_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "ii = cv2.imread(\"images_png_2048x1664/00001447.png\")\n",
    "gray_image = cv2.cvtColor(ii, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAN8AAAD8CAYAAAD3wXG0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eYyc+Xke+Pzqrq+Or+6qvqdJDjnDmeHMaDSrEUfWamDLlo0kWgGbOP5DMRI7ThZrb9bIH7a0i10jQQDvbo4/1oi9CmIkAXKsAdsrI1AOxXLkA5pDlKXRUBySPc1udld3133fx2//qH5e/oqag2xSZFHzvQDBZnVX11fF3/u9x/O8z6u01nDMMccevLke9gU45tiH1Rznc8yxh2SO8znm2EMyx/kcc+whmeN8jjn2kMxxPscce0j2wJ1PKfUZpdRVpdSWUupXH/TrO+bYoph6kDifUsoN4BqATwPYB/AGgJ/RWn/vgV2EY44tiD3oyPffANjSWm9rrYcA/h2Azz7ga3DMsYUwzwN+vRUAe8a/9wF87PYfUkr9AoBfOP7nCw/guhxz7AdmWmv1bo8/aOd7t4v4vrxXa/0lAF8CAKWUw39z7IfSHnTauQ9gzfj3KoCDB3wNjjm2EPagne8NAI8rpTaVUj4AfxXAHzzga3DMsYWwB5p2aq3HSqlfBPCfALgB/LbW+vKDvAbHHFsUe6BQw0nMqfkce9TtvRouDsPFMccekjnO55hjD8kW3vmSySRWV1fhci38pTrm2F3Zwp/oVCqFX/u1X8NP/uRPIh6PQ6l3TZ8dc+yRs4V3vl6vh1QqhV/6pV/CL//yL+PjH/84IpGI44SOPfL2oBkud23j8Rivv/46lpaW8Pzzz+OJJ57AG2+8ga997Wu4cuUKut3uw75Exxw7kS2882mt0ev1cPnyZRwcHCAajeLcuXN44YUXcOnSJXzlK1/B9evXMRwOH/alOubYXdnCOx8ATKdT9Pt9VKtVKKVw8+ZNpFIpPPvss3j88cfxR3/0R/jjP/5jFAoFjMfjh325jjl2R7bwzjedTjEejzEejzEYDKCUgtYa165dQz6fx9mzZ/FjP/Zj2NzcxNe//nW8/fbbqNfrmEwmD/vSHXPsfW3hGS6ZTEb/9E//NCaTCTqdDtxuN+LxOKbTKcrlMqbTKWzbxtraGsbjMa5fv47vfve72NnZQbPZxHQ6fdhvwbEPuS3KSNGJTGuN8XiM6XSKwWCATqeDpaUl+Hw+1Ot1DAYDuFwupFIpLC0twbIsnD59GtevX8fNmzcdJ3RsIW3hnc/tdgMA2u02ut0u/H4/tNao1WpIJpMIhUKo1+sol8totVpwu93y98c//nGsr69je3sb+/v7aLVaWPRI79iHxxYe55tMJphMJtBai+NNJhP0ej0Mh0Ok02nYto3RaIRut4vBYIBQKITJZIJ8Po9wOIyPfexj+Imf+AmcO3cOlmU97LfkmGMAHhHnK5VK6Pf74oRutxtaa9TrdfR6PbhcLng8HrjdbrjdbozHY0SjUSilkM/ncXR0BK/Xi09+8pP45Cc/iVOnTiEQCDzst+bYh9wWPu2cTqfQWiMajYrzjUYjeL1eeDwe1Go1ZDIZTKdTdLtdDIdDTCYTeL1erK6uwu12o9lsolwuYzKZYG1tDevr69jf38dbb72FQqGAwWDwsN+mYx9CW3jnA2YO2Ol0EI1G0ev1MBgMoLWGUkrAdbfbjW63i36/D4/Hg3a7jVAohHg8jslkgna7jWq1il6vh3A4jPX1dayurmJ3dxdXrlzB0dGRA9Q79kBt4aGGWCymX375ZbjdbgQCAbhcLgyHQ2itEYlEoLWGy+WSSNhqtTAYDCQ1tSwLLpcLlUoFzWYTwWAQfr8fSikEAgEEg0F4vV5cu3YNV65cQblcdoB6x+6rPbJQg8vlQigUwmg0wmQyQb/fl6jWbDbh8XgwHA4xHo+RzWbh8XhQr9elBmy1WrBtG7ZtSwRst9sIBAKYTqdot9uwLAtnz55FLpfDzs4Orl275gD1jv3A7cQNF6XUmlLqj5RSV5RSl5VSf+f48V9TSuWVUt8+/vNTxnO+cCwTf1Up9RN38jpaa/h8PkQiEfj9fgSDQcTjcfj9fkQiEcTjcYl6zWYT4/EYw+EQvV5Pvu73+/B6vUgkEgiFQphOpxiNRvL9VquFfD6PwWCA8+fP45VXXsHTTz+NeDzuzBE69gOze4l8YwB/V2v9LaVUBMAlpdRXj7/3T7TW/9D8YaXUeczUyp4CsAzgvyilzmqt3ze8KKVQr9dlesHj8cDlcsHn88GyLITDYbhcLvR6PbTbbXi9Xti2jV6vh+l0iuFwKLWhy+WC3++Hz+cT0N7n80kHtVqtotVqIRQK4YknnsCpU6dw7do13Lx5E+1228EIHbuvdmLn01ofAjg8/rqllLqCmSL1e9lnAfw7rfUAwA2l1BZm8vHfeL/XmU6ncLvdCAaDAin0+310Oh30+32pAWu1GgKBAJRS8Pl86HQ6mEwmsCwLSimMRiNMp1N4PB5Eo1GUSiVUq1WEw2HBDwOBAEajEWq1GoLBoKSj6XQa+XweBwcH6Ha7jhM6dl/svuRUSqnHADwP4LXjh35RKfWmUuq3lVLx48feTSr+XZ1VKfULSqlvKqW+ORwO4Xa7JeX0er0IhUIIhUKwbVscwePxYDQaoVgsotPpYDQaIRgMYjqdYjKZYDQaYTgcotFoYDKZIBwOYzweo91uC5DP2lFrjW63i1qthmazCZfLhSeffBLPP/881tbWHIzQsfti9+x8SqkwgN8F8D9rrZsAfhPAaQDPYRYZ/xF/9F2e/q4hRGv9Ja31R7XWHw0EAgiFQgiHw0I16/f7GA6HwvPsdrtQSoljBoNBAJBUkxEvEAhAa41mswm/349EIgEAEhU5OzgejwWwV0phMBigXq/Dsiw88cQTOH/+PLLZLLxe771+fI59iO2eup1KKS9mjvevtda/BwBa64Lx/X8G4N8f//NEUvFKKSFFK6XQ7XYxmUyglBKKGceORqOR1IGxWAzj8VgaJqz3otGo1G8+n08gCY/Hg1gshmw2i1arhX6/D7/fj3A4jG63i16vJ4+lUilkMhkUCgXs7Oyg0Wg4xG3H7tpO7HxqJqLyzwFc0Vr/Y+PxpeN6EAA+B+Ct46//AMC/UUr9Y8waLo8DeP2DXof1Hmu8brcLn8+HQCAgHE+llNR6rAmn0+lc2sm0lDUkn0euZ6VSQaPRgG3bCIVCAGaRs9PpYDqdSpRjLel2u5HL5RCPx7Gzs4NCoeDUg47dld1L5HsZwOcBfFcp9e3jx74I4GeUUs9hllLuAPhbAKC1vqyU+h0A38OsU/o/flCnE5hFO7/fj263KzXfZDKBz+fDaDSC1hr9fl+I18QDAcDr9QqkMBqNEAgE0O124XK54HK5MB6PEQ6HAUBI2f1+H4PBQCYjyCkdj8eS/jYaDYzHY5RKJdi2jdXVVcTjcRwcHKBUKjkgvWN3ZAvPcMlkMvpzn/scXC4XbNuG3++HZVkYDocoFosoFotot9tS3/l8PsHxWMdx+t3r9Uodx/rO7XbD4/HA5/MJThgMBmWGkFHO7XZLE4fQBiMd4Y9AIIDDw0Ps7e050IRjYo8sw8Xj8WB5eRlerxeWZSESicDn82E4HCKRSCAWi6HZbApYzgjm8XgwGAwk7WT0IqxA6UFigN1uFx6PR5gxbLoopTCZTOD3++H3+wFgjs7G1xgMBhiPx0gmkwgGg8jn8w5VzbH3tYV3vslkgmKxKIyWWq0Gt9stEIFlWUgmk/D7/eIER0dHyOfzkl7SARmltNbyx+VySY1IzJARlMA+ndOyLImeLpdLakpGSNLV/H4/zpw5A9u2sbe358gbOvautvDOp7VGu91Gp9NBrVZDo9GQBgubIdFoFKFQSJgqwWAQ6+vr6Pf7GI1Gc2mpiQsOh0NJGVk3kj/qcrkk6rG+ZI1JpwuFQvJ7er2epKiTyQSDwQArKyuwLAs3b95ErVZzOqKOzdnCO5/H40E8Hkez2USr1QJBdx5kl8uFZrOJdrsttRkAhMNhcRKOGZ05cwZ+vx97e3sYDocIhUIyAcHoFQ6HoZSaG9A1HY0gPDAT9A0EAhIJmdrydzFaPvPMM9jb25PXdcwx4BFwPgLklmVhNBpJB9Ln8wlzheklHYXNEgAYDAbSvbx27ZpQyyaTCdLpNIbDoXREGen492g0kuvgGBJBd601hsOh1J+sA8nG4c2BlLdnn30WiUQCW1tbqNfrTjPGscV3vslkgkqlgul0CqUUbNtGKpWak5fgz7GLCUCm09m99Hhmb7VWq0n06fV66HQ6wv1USokTknxNYN3r9co1sM4DZs5FbJEwh+l4vBmMRiOsra0hnU7jypUryOfzThT8kNvCOx8AiTisz0ghc7vdqFQqaLfbCIfD8Hg86HQ60lhhCqqUQiKRwHA4xHA4RCAQgGVZGI/H8Hg80mwhG4ZO5vV6pY6kQzJSsmM6GAwQiUQkGtLZWDvS6QeDgdSPL774IrLZLK5cuYJms+lEwQ+pLbzzuVwuxONxIT0PBgOp1xKJBCzLklEgOlyj0RCnoGNpreekA3u9HqLRqDBczAja6XTEKfn3eDyGz+cDcIszyq4reaOMZExbaZQ8NKctTp8+jUQigTfffBOHh4fO4O6H0Bbe+SaTCer1OgAIwN1sNiXNMzVcKKwUiUTm5CTMiEbwnTgfWS6MUIyIJmfUhCQAIBaLSUfU5XIJ+8bv90vXlEwck5rWbDYBzJg3nKa/ePEirl27hmvXrjmQxIfMFn5Mmy3/6XSKSqUCn8+HcDgMTjt4PB70er053M7n8yEWi4lT8fmcdqAzcjqCID0Hb9PptDgo1dCm06kopDFdBSApKWtFRkASwHu9nlyf3+8XYJ7d0OFwiAsXLuDixYtIpVLO3sEPkS288ymlkEwmkcvl4PP5RJma6VssFhPuJ8FypoGBQAB+v1/m7waDgYgw0ZkASKOEwkxm59Jkw7Deo0MSXqDj93o9eDwe6YzyxkGckOZyuTAajdDr9dBqtVCv15HNZvH8889jc3PTGVX6kNjCp52MeIlEAtFoVLC20WiETqcjdRhrOaaKSim02224XC6Ew2FkMhmp+8jlJEDO5zFFdbvdSCQSIjPR6XS+D99j5PP5fAJnEPi3LAs+n09uBiY3lI7KyQwAcjOIRqO4cOEC0uk03nrrLXQ6nQf9cTv2AG3hnY+pX7PZlGYLoQN2FFlXUc0sEAig0+mg3W4DmNVYlUoFAJBOp6V+I2DPWpFd1UgkgkgkgkqlIl1MQhFKKXFSwhGcnuCEBZ02FApJysmoSgdmRxaA0OK63S6m0ynOnz+PVCqFN954A6VSyemG/pDawjsfu4DUbeEBJ3uF6V00GoXP5xMn8vv9wtWkg0wmE1SrVZlkoIgSCdUc0C2Xy1hbW0MwGESr1RL4gOkoHYfKaIRAeK3T6RS9Xk/YL+12W66Njkj80O12o1QqSTeUDaXHH38ctm3jjTfewPb2ttMN/SG0hXc+AEJYZuSgKpk5OGtqurBOI9RA2hcnD5iOErejpDyVzLrdrkgV+v1+NJtN+R0mmO/1egVs59Q8O6putxuDwWAOXmC6yvdAGASA1JVutxvD4RA3b97E+vo6PvOZz+D111/Hm2++iV6v99D+Dxy7/7bwDRe32w3LshAMBhGJRISuxQYGoQVGFrfbPScNGIlEpEnC2oxsFfP5fr8fk8lEVMs4L0jNTzoeh3ApLWFOTgwGAxk7YnRkw4fNG+DW/gk2cBh56dj8faVSCS6XC5/61Kfw8ssvIx6Pv/uH5NgjaQsf+SaTiWik8I/b7YZt2+IUrJ263a5AAWxycGmKmRIyLSQmRwfzer2SWgIQcN6cZjdFdJn2EmrgTglyO6fTqTif1+uVyGsyYcjc4de8qRBn7Ha7CAQCOHv2LOLxOL7xjW/g4ODAmZD4IbB7inxKqR2l1HePlam/efxYQin1VaXU9eO/48bP37ViNQCJSnS85eVlZDIZBAIBJJNJAECr1UKlUhHx3EajgXa7LY0Sbq+lYzESjcdjcUQAc6vIPB4P0um0dFSBW0RtOrMpW8FOqcvlQiQSkShpQhuj0UickA5Lx2O9at446PwHBweIx+P4qZ/6KZw9e1ZuOI49unY/0s5XtNbPaa0/evzvXwXwh1rrxwH84fG/b1es/gyAf6qUcn/QL2dkcLvdCIVCcqg5y2dKw2utZbKcB77VaqFWqwkUAECaJ36/X3RZCD2YnE2TQM1FLfwd5hAtGzichGB6yZqSwDsjKX+eTsafN28IZkeV4k83btzAaDTCK6+8ggsXLjj6oY+4/SBun58F8Knjr/8lgP8K4FdwQsVqYOaAwWBQtDspC0FHYCrIFJPpIx3BbHKQbE05iXg8LkrVnU5HUtler4dcLicwBkeZTKqZSbxmo4cOWa/XZfqe/FDKW7B2pJmTEMCtm4PZmGHqenBwAL/fL1t233zzTaGtOfZo2b06nwbwn5VSGsD/o7X+EoAspQO11odKqczxz64AeNV47vsqVgP4BQAi585WPDE30rPC4TBCoZBsJKKRCM2IRyqaOW3g9Xrn6GUE5CkhMRwOkUwmZY8DF2zSWRj1mG7Soc26jmvJgsGgvAafd7vDmVoz1Ihh2sznhMNhaK3R6XTwxBNPwLZtvPrqq4JjOvbo2L0638ta64NjB/uqUurt9/nZu1KsBvAlAEgmk5pTBiaFy7ZtRKNRuFwuxGIxHB0dzRGimdrRMcizZJpJhyRJm2D+aDRCIpGAbdsAgIODA2xubiIajcrgbrPZlNSUNaTWGuFwWIgATCuJNcbjcVk7xskJRmFGUADyPtmRNYWd+P6DwSCUUmg2m3jsscfg8Xjw6quvolgsOoD8I2T35Hxa64Pjv4tKqd/HLI0sUDhXKbUEoHj84ydSrGYdNxwO4fV6hbZFLM3j8SAYDGJ5eVmiDQBxKHIr+RxgNrlAClg8HhcWCRsnrPNs2xZJwXQ6jaWlJQCz2oyTFqwtb3dCOj5wK21OJpPY398XiIJ4IeER1oAmNgncIhqYiml8XqfTQSKRwMsvv4xLly5hb2/P6YQ+InYvitUhAC4921AUAvDjAP4eZsrUPwvg14///vLxU06kWM3hWX5tjucQxL5x4wYSiYREJjoDmyrT6RThcBiWZcG2bXg8HhSLRWGU8EDbti38zHa7LTSx4XCIw8ND+Hw+LC0tYTqdYmtrS5g2FN4lzADcoowBs4l2NnBMeIHOeTvrhTUi5TKCwSDG4zFarZY0WZgK87rj8Tg+9rGPwev1Ynt723HAR8DuJfJlAfz+cdPDA+DfaK3/o1LqDQC/o5T6OQA3Afxl4OSK1cAt0rNSSgZiWcuNx2MZNbIsC5VKBZPJBKFQSByR7f5oNCrjRaz3KAFoWRba7TaazSZisZg4AInYzWZTIk84HMaTTz6JarWKaDQKAJIecn9EKBSS2rDX66FWq0naSejC1Jvh7CF5piY8YXJC2VRiJ5RLXPr9PhKJBF566SUAcBzwEbB72c+3DeDZd3m8AuBH3+M5/wDAP7jb1yL+ZS5EYUrGtFMphVwuJ+Rkyvf5fD7Yti16KpwUoASgKRHIJSjsXvp8PkSjURlhYqOD0vEmwVsphXg8jng8PsdBpWwhIxthEHOdNa+Xo0imoBOAueaMOV1hTlZwftCyLHzkIx+BUgrvvPOO44ALbAuP1PLAu91ueL1e2RzE5sZ4PJbDHggEEI1GpdXPrmav15M0lake+Z1aa1iWhVgsJrN+TO3W1tbE8b1er+B8dBqC6JFIRAZpSVHz+XwiddhqtSSiMZpxyoHAO28ujPLmrCGvk7AJmz1MR+nQBP8jkQg+8YlPwOVy4fr1644DLqgtvPMRUvB4PKjVaoK5MdqwGTOdTrG3t4dwOIxcLod6vS5gfLvdFs4ktTpNnidZJblcTlr5tm2jUCigVqshl8vh/PnzaDabaDabIoCbSCQEWCe+SDqZOVnh8XhQLpeFCG7ifKwH+fOsB9lkIbQC3MoA+D12VQeDwZyTcpPTpz71KUynUycCLqgtvPPxMAKzg0i1r+l0Csuyvo/YbFnWnFQ8ndSMGHRYRhJOTVSrVbhcLjQaDYxGI0kbmdKurKxIdGTtx5sCbwg8+LFYDF6vF8lkEo1GA0tLS+h2uygUClJH0pESiYRcC6/bFHZi44YObi6BYS3barWkHtRao1AoYHl5GS+//DKm0ylu3LjhOOCC2cI7H3E6rvgiu4QMFqZwnGzgBAOjV7lcFv0UOiLFjtrttmwbMtkjrVYLPp8PlUoFtVoNrVYLvV5PmCbJZBLD4VAcxLIsdLtdkYZgBEun0wgGgygUCvD5fMhms7AsC7VaTWrBTqcj6Ssl6X0+n0RjQg5sLnFSwoQm2NFlhsDnFQoFZLNZXLx4EX6/H1evXnXmAhfIFt75AAjUYFkWOp2ORCamYeFwWIjR3JkQi8UQj8dF94UtfrN2NBkvNKaWgUAAtm1LcyUQCKDRaKDX6yESiYgYU71ex8HBgUQdRtnRaIS9vdkKejqW3+9HMplEOBxGs9nEaDRCuVwWxg6paSaGSVCenVMzxSWEwhsOdWx4HQBQrVaRyWTwiU98AoPBANvb2w4QvyC28M5H3RWzu7m2tjZX+xBsNlNJTokzMpidRb/fL5Qvn88ng7qBQAD1el0wNWKD/BmuAKvX61heXp5LSdkQaTab2NnZQa1Wg2VZojtDZ6GjME2k+JMpM2hGdL4/cySKEXAymeDo6Gju/ZKUzb/ZaU0mk3j55ZcxHA6xv7/vOOAC2MI7H2lg0WgUXq9XpPjYuWSqx7Z/MpmUA8q0kl1RNjuods3Dns1mEQ6HUa/XUa1WJcWr1+sIhUIIhUKydJPShS6XS1Sw6/U6ut0u2u026vW6UNZMEJ00sGKxiHK5LL+TWjKDwQChUEhuEqZzkKnDms2kthEn5E3FXF3WbDbFuUnwfuGFFzAajXB0dPRQ/j8du2WPhPMxbSO5mDXQcDiUSYdGowG32y0LKW9P09jgIC4YCATk0FKewZxa9/v9onqdyWQwnU6RSqWwtLQk+9wBSIQhLhgMBjEajRCPx+X3t9ttFAoFYcJQG2ZtbQ3xeFxqUTZR2A1ldGfKPJlMJOKRksb3Qc4rrysUCsmNKRqN4ujoCNFoFH6/Hy+99BL+9E//FOVy+eH8pzoG4BFwPmAWOYrFojRLvF6v7EXnwTWHU9nAYPqolIJlWeIg3W5Xxoc4yUDSMg+/1+tFNBqV73FJJ1ktyWQSk8kEtVpNGh2hUEjquvF4jGKxKBAAIx4wgwj8fj9KpRKefvppRKNRXL16VbRi+B74vkgIMEep6PDUimGzhkwgdax32u/30Wq1EIlEYFkWWq0WMpkMLl68iK997Wui8ObYg7eFdz4e/H6/L3d4guA8kIxu7IgCmAPLSURmilYsFiUlY3OCnUnO3DE1HQwGaDabqNfrCIfDaLfbsG1bUsVeryev2+/3EY/H0W635xTWOL1grqlmfbq7u4uLFy/C6/Xiz//8zyUCEqNkJGPNR+fzer1CtSNNjmNU/P0UkTKnJADg8PAQjz32GF544QW89tprcw0nxx6cLbzz8bBaliUHkHNzHIIFbk0iMBqy5W5ZlgDsjAwE5dkI6XQ64nR02mg0KhPyjUZDoArOD7bbbXFqdSzQ2+l0hLJmSlmQdcIUlPXacDjE0dER3n77bbz44ovw+Xy4du0adnd3hWTd7XYFV+Q1BwIBAd9NAJ5aNcQa+XlwHtLEEUulEh5//HG0Wi28+eabzu74h2AL73x0BsoxxGIxDAYDWRFNKIDpFgBZFQbMIkUymUS3250TKCJJezwey1gRnWJjYwOWZaHX64mTcAIiHA4LDYwND4L83W5XJP9WVlbEMXu93tyGIzZ7WC/m83k88cQTePrpp5FIJJDL5UQwlw5HsSbWgMQCAQjZgGLBAOR9ci02Fdo8Ho90V0ejES5cuIBWq4WtrS2nA/qAbeGdD4A0SJh+tlotqeUo70AJwMlkIlxKwg6ccDclKAhuc4TIxApbrZaINBGAv13cyO12IxqNCn0tkUiIZGG73YZlWYjH47JDkA5KZozWGvV6HdPpbIFLtVrFuXPn8Mwzz+C5557Diy++iC9/+cu4cuWK1HRMN4FbBGumnOycUuqCg7ysfff29iQyMxLyRvAjP/Ij6HQ6ODj4wPFKx+6jLbzz8YBxSoFpFe/+pr4Kyc5cI83JAVPv5XYVak4b0DFHoxFqtRquX7+Op556Cqurq8JS4YbcXC4nDQ8SnKfTqWzN5RrqYDCIer0uc4StVkvSSKbGjIA7OztCQ7NtG5Zl4cd//MeRyWRw6dIllEol2THBjirrWDo1a1qygii7wc+RThwMBmXPfaPRwPLyMl588UV87Wtfm5PicOwHawvvfADkbm6uUTYnAuhc7CLS8TqdjlDS2JCp1+vwer0y78dxITYqmCqWSiW88847UiOalK5GoyEYXjgclrqRFo/HYdu2OOj+/j4ikQhs2xanoYQ8oY7hcIi9vT1cunQJoVBIHP/UqVMYj8f41re+JZ1JarxwlCoUCkkqHAqFMB6PZUsTn2OqqlG+MJPJIJ/P4+rVq1hZWcGzzz6L1157bW6jkmM/OHsknI/pJlMvU3bB3KvHw0wnIkBOkjTTSkYJUr5MTVASnQOBgGBjq6urmE5nW4RYO1JNjNErFovNbU7i7oh4PI5oNCqT80tLS7AsC8ViUepWUuKGwyHa7TZKpRIajQY2Nzfx5JNPYnl5GQBw5coVkZ7n+83lchJhOajLdJufC1NeU0C4VCrB6/UiFotJBDxz5gwqlQrefvttp/57APZIOB8JzXQuczrdtm3B56iJUqvVANwS2x0MBlKH+f1+mWjnhIPJl2SN1G63EY1GhXVCR2fqS2EkKlHHYjE5+MTnbNsWEgDl5clESSQSgr8dHBzI9xih6vU6bty4IVS2WCyG5557DoeHhyiXy5I6ZjIZhMNh4bE2Gg3k83kZqeKEPBtH7P4OBgPs7+8jlUpJ9zaZTOKjH/0oyuUySqXSQ/5f/+G3hXc+3uEBSEPDXMXQJsAAACAASURBVMMFQByAd3k2IajwZaal3BrEyMUuaSAQEIoYaz/uUl9ZWUEikZDUkypnvV5PIBDWcoQzSMr2eDxIpVLI5XIoFApoNBrY3d2V1JdrqAuFgsgVMlKVy2WJhpZl4cyZM9jY2EAqlZL35Pf7sby8LFzQZrMpeChwS9KesAzhEzpgtVqVmrVQKGBzcxMf+chH8PWvf93B/37Adi8CSucA/L/GQ6cA/G8AYgD+JgDeOr+otf7K8XO+AODnAEwA/E9a6//0Qa9jNlwajYZ0HZmqscHCumc4HCKdTqPT6aBWq81JuRO8pkxDJBIRvihwS8LP5/PJY/V6HZlMRpyC9R3FjW7XkyH2R44lOaDJZFLElba2ttBoNERhze/3Y21tDTdv3pQbC52mXq+jXq8jlUrh6OgIp0+flvcwGo2wvb2NWq2GbDYr6TQw6+ySBschW3NlGrMIvi9G50KhgKWlJZw9exZvvfWWMwP4A7R70XC5CuA5AFAz2fc8gN8H8NcB/BOt9T80f17Ny8UvA/gvSqmzHySixMhHNTF2+HgA2TkkmEz6GScGWEuResYmCOs8slzMDimjzng8RiqVknGinZ0dSe9MJgowm0ivVqvfxykFIN1Iyl3Yti2NFkpdxONxXLx4Ebu7u9jf359bqsLdhJzeIF2uWq2i1Wqh3W7D4/EI9MHfy/drSkywHjalKcwh3Ol0tgl4fX0dh4eHTvr5A7T7lXb+KIB3tNa7bOm/i51YLj6bzWI6naJer0vnkV1MHip2ITn1TYaIy+VCt9uVER46MDCLqtVqVXicBKGJh/EwNhoNkYzI5/NyDalUSgZnqRPT6/VEWToQCMjvY6Om1WrB7/cjk8lIWsxr8Xq9ePHFF7G+vo4rV66gUCjIzYFivZSwZ0OHSt3FYhGVSgX1el3eK+s93izMsSZTuMnUqWFU7PV6OH/+PL7xjW/MdZkdu392v5zvrwL4t8a/f1Ep9dcAfBPA39Va13BCuXjLsqQzR3zLVDK7nTrGmmY0GonTkGbGaEBsjs5FvijHe7jSmVMDbMpwOoB1YbValehCrJEORkchA4e1HiGJWCyGU6dOodFoSKeyUqlIxHz66aexvr4ujsWJBfJH/X4/4vG4MH/q9bpgmcRB+RlRbNjj8SCTyYjgL7/PGxblDrkUhlDE1tbWfTomjpl2z86nlPIB+EsAvnD80G8C+PuYScH/fQD/CMDfwD3IxVOHk61+NkRMaXaO1ph8S+AWnYvpHZ2FtZ65zJKHkV1RTimYE+dMfymkxGsjrsbXYoQh24bTFI1GQ0R4OWnB2mswGMwpsZ07dw4+nw+FQkGaH7FYDCsrKxLFT58+jX6/Lw5HyYtCoSB0NGrbsDa9ePEivvOd7wg9j/VzKBSSVJmf10c+8hG54Th2f+1+RL6fBPAtrXUBAPg3ACil/hmAf3/8zxPLxY/HY1SrVWxsbMDn8+Ho6EjSpdt3G/Aws66h5svx9UhqSR4nW/BMw0zgWmuNXC4nhOdarYZKpSLSFTzsrVYLsVgMmUxGUtFcLodoNCoCSEopPPbYY3Itg8EApVJJIjdZNmz0EKdbX18XXubtWi+lUknWYS8vL8sMYiKRwBtvvIFCoSCOzz9aa+zt7c29Z4LqzWZTVLuVUjg8PMSZM2dw+vRpvPnmmw72d5/tfjjfz8BIOdXxnobjf34OwFvHX59ILh6AOEq/30cymZTmAKcbODVO/ic1VJhyketodjuPr1WclCJNTEO5T4HCSJyAZ61pWZbgj4xunU5HuqNM4dhxJDieSqWkIWTuDHS73QgGgzJ5T6iDejSmpgtpat1uF263G6urq1Kzra2tYXV1FalUCu+88w5cLhd6vR4ajYbALaSpsZbjNTAT4OfU7/dRLpfx7LPPYnd3V9JVx+6P3ZPzKaUsAJ8G8LeMh/9PpdRzmKWUO/yePqFcPNkao9FItvBsbGzIoCoAiUJaayQSCcGwqM3CBgK7fdyDTniAEwqUhGfqOp1OxbEAiDgumyikhTGCplIpRKNRTKdTVKtVUSejQlo4HEYikZCWP2f6zIjC2oxpZL/fRz6flxsDGzkcYzo8PESz2RS4xe124/HHH8czzzwDn88nWqfVahW1Wg3lclkYOIeHhzJZAUCu13TAcrmMc+fO4fz583jttdcc9bP7aPe6pagLIHnbY59/n5+/a7l4HnbWczdv3sRkMplTjyZD32S5kD5GKIAOZlnWnHw7AKnVTAkH/k3ZCtZv/FkC17Zty1SBZVmCl21tbSGVSolzA7e2Iw2HQ0SjUWHjmFgka0wqtJkUOoL5VF9jmjyZTLC9vY1nnnlGnh8IBLCysoJut4tarSaqbBwQBoBoNIparTZHPyPlzswUarUazp8/j6tXrzp7AO+jPRIMF9u251K1crksB52zflQFM7f/xONx6WBSfpCpKNknACQSAhAtFH6fh5JpXyQSkddjpEmlUkilUgAgqeJ4PJ5biMlZQh5wdmrZnOFgL9+zKf/HBhEAeT/mos16vY5isYjBYIBoNCqTG6ajApAoTc4rJTGY/nIPIFN43sRarRai0SjOnDkjat2O3bstvPNprYWszO4dpfdIhGaEYrpJXiUdjrQ0ExN0u92IRCLiLOb0Ow+ducKZDkXOJxsmTBGPjo5w5syZuWHbcDgsEZZNEr/fj2w2C2DmVMvLy3PCuawRydhhelyv1yUS0pn7/b5ogFLk98/+7M/w0Y9+FKlUSurCyWSCTCaDXC6HYnG2LpFdXU6MkG4HQP7mjCAx1M3NTbz99ttoNBoP4ST88Nkj4Xx0ON6Z6RSEFwBIZxO4Nb1NsjKdTWstLfd+vy/wBalcfD3WQeRrAhD2DB0cgNDHCJxfvnxZlrgQHF9ZWZnTFR0Ohzh16hSCwSAqlQo6nQ6azSaKxaKkt3RYc+EmIycjKqMTpzaYbl67dg2j0QhnzpzB+fPn4Xa7BQrhRD07tOSzssPKtJcOfrsaWrfbxerqqkj2O3ZvtvDOB0Ba6OzOkTrGemw4HKLX64mMuzlmZEY3HlKSmRkFzchJtgoVw0hBMzukhAG8Xq80RrrdLgCIMJNlWTg4OEC73cbKygoikQgAiEoanSaRSKBQKEhHklQyQgSc3ohEIgJzcDLDpNKFQiFJSYvFouycoMDwysoKstksKpUKGo0GtNaIRqNYWVlBPp+X9JXpPCc/mE6TyXP69Glsb2+Lwzp2clt452NHkhGBrXMeStYfbMET8yOHkWJJlB0koM1DxmYHmSeseag03W63Jd2jQ2qtRRWMXVRzaoKT7NFoFNvb2xiNRlheXhax3UajIXsDzUhKqQcqYXO9GUencrmcNJhMGQh+TdHgYrGIw8NDyRCoDcoonE6nBXrIZrNQSuHo6Ehq2VarBcuysLa2hmQyiVqthoODA6RSKSSTSaRSKZHCd+zktvDOx9k2NiTIvGB3kc0TRr9OpyNdQE4hcJcfUyw2IVjbcbrB5DxGo1HpNJqpHpXIKN3HpgojKwd/PR6PqJ61221UKhX0ej2Ew2FJmal+DUBYKCbtjbN9XAADQKAUjkO5XC7ZhEQuJ7fslkolccpsNjuXXrJLa1kWQqEQYrGYkMkpexGLxQRuiUQisnp7Y2ND9lM4dnJ7JJzP7/cL/5J1F2UfyPPkUsxWqyV1GqMQHZK6mXQ8OgE7i6FQSBombHJQhp7NDw7Y1mo1weEoO2gK2TKqNhoNdLtd7O/vI5FIIBwOo9vtimraqVOn8NixlHwulwMAeQ4bNyRtE14AIPQyZgBMpQmvsEPMn51MJtjc3BTWy5NPPonvfe978vnyhtJoNJDJZOB2u3FwcIDV1VVsbGxgf38flmUhmUzKNL6j93JvtvDOx4mDWCwmUcXkQjIqUKuS0ANwS8vTbKjwuawJm82m1HqMYpFIBO12W/icVCIjFWwwGEizg1GZIzoAxPmYIlNwl9gZwe3d3V1xMnI2w+GwqFubDRamvsCt1Jg3l0qlIlGckY6SEryWd955R9LGYDAolLRr164hn8+j1+theXlZtGdIYojH45hOpyiVSqjVarLzIZlMOs53j7bwzsfIxm4fowGxOeJlpH0xspiOSqoXt/kwapi1FXfscXTJjGCcNI9Go7JuenV1VaQFyXQhHGJyTc2FJoQrkskkCoWCrOzq9Xp45ZVXsL6+LtcUiUTQarWwt7cnqTObQmwIATPgPpFICBRhMlSIb47HY8RiMezu7gIAXnzxRamfn3/+eUSjUVy+fBnRaBSj0UhI2dzEVCgU4PF4UK1W5TPKZrO4efOmg/ndgy2885GpcjtZmgeZ0Q6ATBYQB+QmI+JdVBrj9DmAOSl3AMLHZCODVDDeBCgRSKJyLpeTWtPsqPLaCcoToKczkzDAevXrX/86Pv3pT2N9fR1KKRwcHKBarcLn8839HId7KQnhdruxtLQkjRISwpkSE98k1gjMoJjV1VUZQLZtG0888QT29/eFS2rb9tyURKvVks9uNBohlUoJDujYyWzhnQ+4tZedUYoEZ0YCppnc9mqyWBiJAEj6x5qPaRmBZr/fL89hdOSdnhPsrDn39/fF+ThxTofMZDLweDw4PDwUHiXTVabLfC90mr29PbzxxhvScSQJgOM8nBOs1Wry/tn1JOTAbixTcN6sGAXb7Ta+/e1vY29vD+VyGYFAAJZlYXl5GWtra6jX66jVaiIAzOjMz5jparValUaN43wnt0fC+ag6PZlMEI1GAdwC34EZJYwEYlKxTIoWhWLNSQcySBhJGVmZOrJxQd5mOByWEaFyuSyRkAD70tKSQAyhUEigCw4BdzodmTtkWsgGB7ukOzs7CAaD8ruIszF6E/6YTCZzi17K5TKi0ajslSDmyFqWRi5pPp/HeDxGLpebq2dXVlZQq9VEFZw3MzJgOp0OVldXJQomk0mH63kP9kg4H7uI8XhcajoAciCIhTGt4lwcFc6YtrJRwaFRU9OEUYnsFjrK0tKS4Hm7u7tSQ1JXhjNwsVhs7vcqpZDJZKT25CbcbrcrizAJpNO5BoOBbMMlI4WaL5ZlCdZJXG84HEp9Zi7WZI1Mvqp5A3G5XMjn89jd3YVSSmpW0umy2Sx2d3dRq9Xk9QhhEHPlEHA2m8XW1pZT953QHgnnY1poEpAZZRjNzFSOvM/bF4xwoqBer0uKaO42IKOEA7d8HteFsckzHM72nwcCAbkhpNNpuN1ubGxswO12o1AooNlsYjqdSoeQ+yWok8l6k1GW23A5QhSJRKRei0ajiEQiQujWeraCulAoSC1K/JO1IPcyUD4iGAwiEolIl5afW6VSwbe+9S0sLS1hfX1dwHzCNuwax2IxYdhorZFKpeTzcOzu7ZFwvng8PtdkoIaKKYhE+hMbE5SAoI4lZ+6o1cJpBeAWdxKAzPUNBoM55kgqlUIsFhPMkLDDcDhErVabS2fZ6OAhNZetcPUYF2qyQcSfIZGc15ZIJARf5Gua68bIXuHNxLZt5PN5qTdNJs3W1hY6nQ6SyeTc8PDh4aGwbijAy8+AEoskLKTTaVmdzRrTcb6T2SPhfMTMCFpTNIjNES61ZDpKrIpgOwCBEkzpPN7FTbC62WzOpYOsHdl4AWYRgEsxqZLGNJBbadvtNkKhEFKplCzLpHYnNVeYJieTSemKMkWkmhr1ZG4nB7TbbRSLRbTbbUkFg8EgisWirKQmhsmpkE6ng0ajgVqtJg0TALKlaXNzU+q7XC4n0TGdTiObzaLRaCASicjYFcndzpTDyWzhnY/RjMwTn88nWB0PMHE/TjAwjWPLfH9/f46JH4vFBJYIBoNoNpsyXkNMjY7FmpHOGQ6H5/BCdiYJxk+nU/T7fdRqNZRKJUwmE2SzWdH05Gvw50ghi8ViOH36NDY2NqT+o/NprVGpVCSicsSn3+8L5FKv12VYdjgc4sKFC9ja2pKajqNKZN4Qg2SU5WdIpbVwOIynnnoKN27cQCQSQTqdRj6fR7/fn9v1F4lEUCgUPuB/0bF3sw90PqXUbwP4CwCKWuunjx9LYKZW/RhmUhF/5VgeEOo9VKmVUi8A+BcAggC+AuDv6DucS+l2u7IA0qRd8aARQKecBOsUwgEA5LCY0YoyCeRbskXP6QDWgXQQ0sh42fF4XMZwSEfb2dmRG4TX68XR0ZHs4SMeGYvFJJIzZaMkIqUBKTE/Ho+xtbUlTRvukuD7ByDvn2QDprPnz5/HlStXJPLS8YjVMap7PB70+31sb29Lx3R9fR0vvfQSut0uDg8PJSVut9vIZDLSGQ6Hw3NwjmN3bncS+f4FgN8A8K+Mx34VwB9qrX9dKfWrx//+FfX+qtS/iZkW56uYOd9nAPyHD3pxt9sty0C63a7MlZkHhw0BNmVYq/APayIAksJxpo2/u1QqSUTRWgvhmSNDjJQExwEI/NDpdHB4eCjLSdjIINnb3N1AKIQOyyaJUgq1Wg3ValW4lhcuXMCTTz6JjY0NeL1ecSJOIRASMbc3ARCFN6/XKzWhuT+e1x+LxaQZ5Pf7UalUZBpjZ2dnbpyIv5c6pnw980bi2N3ZBzqf1vqPlVKP3fbwZwF86vjrfwngvwL4FbyHKrVSagdAVGv9DQBQSv0rAP8d7sD5yNTgXZdS6MSpzANMZ+Nd3uxmMjowxTKjWqPRkENl27ZorJjaKZ1ORybkY7GYgOGj0WyZZq1Wk4l01lJ0enZoiTtycsG2bZmSIDDPeo6OWCwWsba2hmg0Ctu2ZfIgnU7P7aswG0DdbheRSEQAefIw6eRMV0lFI87ILjKxPkbx1dVVUYhLpVIol8siCuw438ntpDVflvKAWutDpVTm+PH3UqUeHX99++MfaIwaWmvBxqjdwlqLqRsbLBQVMgWJ6AScbGAHkXQyzvDxIHGWj6NDnAvkghG+VrValZSLkRiAqIwBEK6mqWJGoJpyF6wH6QjAjDxwcHAApZTslZ9Op8Ll5KgUPxMzDW02mzJlb1kWMpkM9vf3ZX02p/SZyjMqJhIJ6Sbv7++LY5PnSdCdWKo5juXY3dn9bri8lyr1HatVA/Ny8WRucJ0W6zVyHimZZypwEWJgk4EAN2XTAchzzD0KjAiMpkwhTXmFbrcrEoOsdRjZgsGgOKcZiQEIWTsSiSCVSsmoEtd00YhHsj6r1+uIxWKi+8Imj1JKPg9GPEZ4kgKUUojH41LjRSIRkZXn58BRJA4ok1JnOlS5XMZTTz0Fj8cjQ7QcTA6FQkIecOzu7KTOV1DH4rhKqSUAxePH30uVev/469sff1fThlz86uqqJmhdrVZlDi6TychGHVNcCICM2/BgsslCahqdgmrVxPk4okQNFUYxslYYXejkbHLQURlVCeQT16Nzki62tLQkO94JonP7UDweRygUEtyRjjEajaQxxHSXKTR1bZgmE/tkcyiVSkktS+jBnD00IRmzgdTv92WZ5ng8xtraGvL5vNxExuOx0OUcu3s7qfP9AYCfBfDrx39/2Xj8+1SptdYTpVRLKfUSgNcA/DUA//edvBAPkcfjQSwWE/kDc0876zc2Rvg4O5rALUUuRgw6CsWN2L0jI4UHGYBgbKz1uK/BhEFyuZwI9bIZZFkWIpEIotGoNEwsy8L29rZwKn0+H0qlEm7cuCGQBql0HJPiSJPL5UI2m0Wz2RQnAyBsHkYjRkZGN+AWO4Wfi4kzkg3D55DQwE4w1au9Xi/W19dRKpVED4ZwjWN3b3cCNfxbzJorKaXUPoD/HTOn+x2l1M8BuAngLwMfqEr9P+AW1PAfcAfNFhrb9sS2eEdntOBqYx68arUqh4IplCk4ywNOoJ5pojkUy8YNI+PnP/95pFIp/MZv/Ib8HGsgbrANBAJSL7GDyMjGxSsEx4FbIrqBQADRaBSlUkk2Fo1GI6ysrAghnF1INomoxg1AGkxMTxkp6aBut1vUyjY2NnDjxg0EAgFEIhGEw2FpFJnziLzJ8P0Ui0Vks1mcOnUK2WxWVmqT1ePY3duddDt/5j2+9aPv8fPvqkqttf4mgKfv6uqOjTgbGy4ul0s6hkz3KM9u1mputxvJZFLSLdZYrA/b7TZcLpdQuBjNTEYNU7vXX39dUlheD52MpOpOpzNHcDYlBkk7Y7SqVqvS0OBkOLuSo9FsCcp4PMby8rLUt5QZ5HswQfrhcIhGo4FgMIhYLCbXR6iDDJ9MZtYbu3z5MrxeLzY3NxGLxfCd73wHAIStwzk+Npc4lRGNRgVwZzfVnJxw7M5t4RkuTBtNapjX6xWKV6PREGyPd3p29LTWKJVKiEQikoJRs4UAN1PHYrEoDQRyQ3moJpMJ3nzzTQCYU5am5ifrnslktmMvFovNKWezUcPpAXZrWYsyesTjcezu7koDZjwey8gS+ZgHBweo1+tot9totVpyk6GYFG8W7FLGYjHEYjGJ9vv7+7BtG4lEQtJdyiAy8vGGxc+WOjIkYW9ubgp3ljctx+7eFt752NxgZy8SicghZxQ01aAZder1utRCps4KABFj4vdrtZpECILYhBBYF5mDqZwrZPOiWq1iZ2dHeJhaa7z00kuYTGabbEul0pxkBQ86WTtaaxwdHQmFjO+b4kzs1rJWLZfLMq3OKE8ntSxLOr6c4qBc4XA4FBwzHo+j0+nIIk+mtc1mUzBNTvMzpWdji2Tr1dVVlMtlyRgclsvd2cI7HwAhUHu9XnQ6HWGwmIA7Uyzie+Z0N+s3HhxqsTA6tFotiZhmWsnnkoAN3FLGNiMYu4NerxeZTAZ+v19wPJPBQkI1ACENMEqRBE1ogtGS+yDcbrdM4TMNJJzCFLfdbqPRaIgKG7u1XGPGSMnov7KyIrAEAJHTJ3GAuCTZLqztSqUSEokEYrGYNIIcu3tbeOfTerb2q9/vixIYGfrshAKQNI4NFDoH6zfK9ZFexc20BLeBWxAE2/eEFczWPTuQ7I5ms1m5IQwGAxFa4miTUkoAd7fbLYRrtvvpnFQiM/VaiDnati1zeKx1zbSZU/4md5UwCpksdEym6MBsXGl9fR0+nw9Xr15Fo9GQSGp+tvxcLctCo9HA3t4eLMua22XhsFzu3hbe+Vh3sbPIA8HINBgM0Gw2JQ1kSkccy2ynh8PhOY0WpmfJZFIciDUlu4A0KpwxMpGMTIYJow8dm9GDqmbJZBKPPfYYTp8+LdfJSE5GDuunQqGAbrc7h9nRKbiAk58LO7LEIdnVJRNmeXkZqVRqjnzADi1VzUgO4O9nh5fDuIy2rJvZAALgzPLdgy288wFAoVCQ0RdSwaihQpiA0cusy3iIwuGwtOwZTRhRRqORDNmajJhGoyGsFaZ7BKqZbt4+fkQWjj5W/aKj01HYSPH5fEilUlhbW/u+cSHeOJrNpky7UzaQRO1isSgwAX83nYNpKGXuE4mERHCv14tsNiuCv8wW9vf35bV402L0X1lZgWVZWFlZkbqaXWPCKE7D5WT2SDgfVaLr9bpoigAQahnl+Fj4U8zVXH7CZgeZLQCkTmq32yIoS3n3bDYreirc6GrWgWzMkBHC6NbpdKQZRG4nQehOpyMUNrfbje9973sCMZjNIDZZEokEgsGgpIbmqjPeNJhWM60lUZsOXCqVhPxMB11fX8fBwYHcRLivj1S6WCwmw79ra2uwbRubm5solUpzFDr+fzjOdzJbeOdjJKKKMusnSqgTfqCuCVkkrKUou2DbNnq9HqrV6hwIzTs5/7jdbtlJUK1W0Wg0ZJSHjRhS0ahc3W63BdDnQCtHl0hBa7fbcrPg9dTr9bkxKRKsSfDmsOrGxoZEuXQ6DZ/PJyvFyL5h/QrMOqXBYFDUyUyeK+UrTp06hXA4LEO+nAPkspkzZ87g3LlziMViSKfTaLVaKBaLyOVyEom5TclxvpPZwjsf6zamZKb8XywWE/aHOYVuHlx2IgGIGBJb5Zw4Z3eQQDYxPtZInU5HajROjjOlZQ04Ho9xdHQ0R1tjvUZH58Cvy+XC0dERer2epLWJREJwy8FggFwuJ/sblpaWEAwGRTKCwDsdtdVqoVQqodlsol6vo1qtirTg2bNnZUKDUYobbFdXV2XvhMvlwtLSEobDIfL5vKT1yWQSmUwG9Xod+/v7MqHBJSp8j47dvS2889EJ2NkzDzPrO9ZzvBObg6/EyxjBiPFRVoITBIyiHMPxer0CWXAXHiEE0rmWlpaQSCRw6dIlVKtV2RnhdrulLU9ntCxrTogWmGnNpNNpLC8vi1YmO431eh3pdBpnzpyRNV1MCTkkyyl5NlzMYVnOGx4dHQnUQfiBDliv15FMJnHu3DkUCgUkk0kR0aXwE6lnp0+fRj6fR7lcFjlFU77Dsbu3hXc+AFI7kWFBwJg1DFMmdirZOGDNF4/HAUDqIWJtfA5TU25qZRTlgCknEuhUsVhMovDVq1fFIQEIuM0mDCcsqLrG6wsGg0in03jyySeFgcLOKCNaIpHA9evXUa/XUS6XJb0l/YyKY8CsiZNOp2WFGAFxTrwDkOUo6XQafr8f5XIZa2trSKfT2N/fx97eHuLxuKiVkTZHR6RTLi0tSRpOgrtDMbt7W3jnI3fTJPnyQLOB0mq15mo8chIBSJcyEongqaeeko7g0dGRqFwzOrIxw+YFoxkdk3zKYDCIWq2G7e1tuftzsgKASBwyWjMKAxBnTqfTkvZZlgUAc7SwZDIp2qQHBwfy3gitcKU090Yw2sViMZw6dQp7e3sim0isr1QqYW9vDysrK5JSjkYj7OzsYH9/H8vLyzIKxX0MJBfE43GUy2XZ7X7jxg2JoCZ7yLE7t0fC+dhZ438wqVkAREeTTgDcWqFlMjb8fj/q9brM9JGbyCl1Cgcx6pCrybqu1Wrh8PBQHLRUKkl0YHpGwJ6OR4ekbgswi+LZbBbLy8tzZOx0Oo1IJIL19XVMp1ORi6AYE5s1TJ3NpZvE8Kjp4vV6kUql5DqY+vLmUCwWpRPKRgphFEZ2NogODw+xsrIC27axsrIiqty1Wk3qSGYeGLDdVAAAIABJREFUDuZ3d7bwzkfnAeabL+R1cviVg7DE+AgB8LAwouTzeWk8sCa8XV6doDJ3AjYaDdkqS+XobDaLt99+W5gj3CxLPI3RmRABl5JEIhHkcjkhSnc6HRwcHEjTp9VqCYmckZPjVPz97NoSzyMxmpt7w+GwyL2bkxqTyUQWXzIFNwdqi8Wi1KFLS0tQaraOjCku09BCoSBjUUzF+dk5due28J8YoQbOy7FOY+ucDRc2FYiT8VCYUAJrMzoeoxGpVObUO52SB4uvYTZnSEQ2N+bS8U0VaY4tUTqi1+uhUqlISscOaKVSweHh4ZysBKfcSadj/UcWizn0a2qZEnpg7cp0m80qfl7tdls+IwDY3NzE2bNn4Xa7cfnyZRm76vV6uHDhAoLBIG7evCnSHlwvzc/GoZjduS288zFdisfjctDa7bakUmz9E2wHMMelBGZT3JVKRQZQGeXMuoxpIms0ppftdltWgbFxQpyPQDjTVjoIABnzYXMllUqJY7hcs7XNjKKcwu92u7J8s9vtCteSNLRKpSKpIzMC1on8OXZ2TUEmczknNVp4Q+AEAzfQ+nw+kb/f2dkRyl2xWEQqlZKbASfZuaOCcoiOlsud28I7H/EyppaMTqQ4sSYi3ub3+0X2LxAIIJ1OS7OGkxBmHUQnI0uFh4tYIbHDRqOBfD4vwHo8HpeWP+sdStZTpo9dUg7rcrSJbBoz8jYaDSF2s8HU7XZlnIgdWx5ywhJU1WZNyAn6YrEoNS8jOSMynaXZbM7J2PNGw1STTJ3Lly9Da43Lly8jm81iOp3iscceE5l58lK5t4H/F469vz0SztftdnF0dISdnR1JdRgpKNNuCsGm02nBoQqFAra3t6VZYwoP0bEZ7UwxJRK1i8Ui6vU6AMxBEDzQzWYTkUgEAGQIlZEYgJC/OaGeSqUkzeQE+mQymdtgRKYOx5DYpCGjhWk4m0wU9l1dXZWUm3Ib3LPHmwxr5lQqJTxP3gwajYYM2ZbL5blxJdu2UalU5oZtuVOCN0OSsB3nuzM7qVz8/wXgLwIYAngHwF/XWtfVTFz3CoCrx09/VWv9t4+fcyK5eALUuVwOlUpljl0SjUahtZZZON7ZydVkM4RisCQTs240uZokQdu2LRGLrXrqfhLDY/RhR5SygJPJBMlkUmQJK5WKKG5HIhEUi8U5iXtGLt4YptMpwuGwRE0AwudsNBoimssdEKxlGSmpds05QXPeEYBEplAohEAgIA2kc+fOYWVlBYVCQdgtnH5g3Wh+vnxdDg6TUmfqljr2wXZSufivAviC1nqslPo/AHwBM8VqAHhHa/3cu/yeE8nFk7bFCDAYDNBoNGQSmz9DoJx6KUwz2Qgxx38ajYYcHmJwZgrIBkU0GpWowoaCbduCKTISchiXUwKnTp0SJg1HlkxdF5PsTWKA2UginMAJg2aziUajgaOjozlGjkmbo+6KbdtS15nT5WTzkGva6/VQLpcBANlsFqdPn5aabXt7W/b+Ue2NfFU6LB9jXc260qGa3bmdSC5ea/2fjX++CuC/f7/foWbanieSi2cHkY62tLQkQ6PsTJp77rgOi07GqMDGA2seQgGEAzhWxCHUwWCAVColu+rovIxO5ggQZf5SqRQSiYQsteThPzo6QqPRkHXSlHIgHMIOqTkTyEhN8J8DxKPRSGpaTkmYzkuZDX4mTI8DgYB8LuZUBPHBS5cuoV6vy7AyAJmNZDRm2ktc8fYdDqa4FafjHXtvux85wt/AbGMRbVMp9ecAmgD+V631n2AmDX/HcvHKUKzmJiA6SLfblVoFuDXBTrDd5/NJZOPsn3mIAUj6xJY7cTlGUDpFp9NBIBAQPiVTWMuyhFhMhbR0Oi0TBpxwqFarMvxqgubU12RkIa7HQVa32y34HQCk02npXJI8YMrWa62RTqeRSqVw+vRpWVW9s7MjtDfioVQhoxwjscyDgwMhlm9sbCCfzwudjWuwucCFEApxSt4oJpMJPvvZz+Lnf/7n8cUvfhF/8id/ch+O1w+v3ZPzKaX+F8z0Of/18UOHANa11pXjGu//U0o9hbuUi9fvolhNjI51m8/nQ7lcFgl0GtM1dt940BKJhNSLZqpEKhVTJq57pvM1Gg34/X7kcjmcPXsW/X4fpVJJIhtfs1wuy+QC9/+x8VGr1STimSudea0mNtdut2HbtizRnE6nKBaLEpEZHVkzciaP0APrOcIe0+lUyNqsb8kaisVi2NjYwM2bN3F4eCg6okopoZpxmNekr5n8Wa21QBOTyQRra2t46qmnsLJyR6s4PtR2YudTSv0sZo2YH2XjRM+2Ew2Ov76klHoHwFncpVy8adTX5Iorj8cjjRe23CnJx3qGeidM7UymP2ssHlaOJ9FR2J2kI/D3ko4VCoVQLpfnBmW5ipkpWzgclpk3Ohl3BwIQrJKQBruWhFIoRUG4g5IXZOlQUdt8z/V6HYPBADdu3BAnozoZFawZ3QEgk8lgc3NTVoMBs+j49NNPy6xgIpGQm8iNGzcQDAZx+vRpuYHwulgCDIdD/NZv/RZ+7/d+D1tbWyc9Wh8aO5HzKaU+g1mD5b/VWneNx9MAqnomD38KM7n4ba11VZ1QLt4k79JxRqMRisWipEOs90wlMaagrIsACEjOhgo7j4wQzWZTsD1GTL4+JwkqlYpIBFL6jzUkmw4Ey/l7p9MpqtUqMpmM1FmMepzCJ8eU0ZR0LnJHzfEej8cD27Zh2zY6nY5ovozHY0nHGfls2xYYg9GPz49EItjZ2ZEO8Orqquzzo6On02ns7e0JdtlsNmW6gfUfb2zcUcFGjmPvbyeVi/8CAD+Arx4fFkIKnwTw95RSY8w20/5trTW5UieWize3z7LpwLqIUEQ2mxV5djJVGCHY+WPKybSVKRlTMY4CmYtUmHbRqUifMqMP1aVjsRiOjo7meKOmavTR0ZGklYxYwC380NwzQVI36y7WVdwNQa2YQqEg74tzj6FQCNFoVNJRXut0OkU8HkcwGMSpU6dw/fp1IWYz1WWziUO+fE1+nsRUqbbGeptNIme2787tpHLx//w9fvZ3Afzue3zvxHLxvPuTPmUOcrJ+Y3pkThfwe3RKc60ydU14N2eKynSSBGkeMmJddKZ8Pg8AsoiS+i10Cl4fJQLJ0jH3KjDaVqtVuTkwOplT84RECGa73W5RR2Mjhq/HyMe6k8O9FIGKRqPI5XIIBAJSuyYSCWitsbu7K3L0bFLxvVDrhWkyIzaHcokpOnbntvCIKOskNkxIxTJ5iSY/k00IqjQzuvHQuly3Nsdy4JQtdXYbW62WOCLxODY/WNtwMJaRytQBZYeUYzpk3zASkmXDhZl0eK1vLX/hBAMdTR9L+TFasnNpRmmm271eTxo0ZmNkaWkJS0tLWFtbQ7FYRCwWk/XX+/v7QvgGMMdS4Z54rubO5XLyvhuNBmq12lxW4Nid2cI7n8m24JQBJwwIXnMFFwAsLy/PTRbwUHCzD7mSFFIiQ4QygdPpVLAydk1brZa06NkhJfjO+UDqvdAZyIxhJ5AMEAL2bIS4XC4hJ1NMlzUlJwoY4QlzkEtK0jZTQs4h0ompWUPuaiwWQzQaFdzRtm243bP9D+xW1ut1jMdjJJNJ5HI52WzL6x4MBtLJpIzh0dGRM8t3Alt45xuPx6hUKiL3QDYKd+WxPc7oQ+YFWR4E3c0J+EajIVovbObwuZZlybZaRgyC0wAkBWQDg8yaTCaD8XiM/f19abKYkxXArdrV6/Uil8sJrYziSGyomFo17FICkKYOO67E+RKJhHRXiW+SbM3HKFFYLBZRrVYRDoclWjPis9PKNJbX2Ww2JVsgXW40GqHZbGJrawvFYtFhtpzAFt75AGBlZUWcZDqdYm1tTaa4eTgty5ImQ7VaRb/fF0iCADs7lnQkTgGwriJnks5UKBQkAiYSCemSmuC8UrP9gGfPnsXu7q6kfDz48XhcyM+cIDh37hzq9TqOjo5kvIf1k/k3ABk3MiUmzA4uh3SpIkZcklov5r7573znOyiXy7KYs91u4+DgAJVKZU4JgARvYp4+nw/Ly8vo9XpIp9OwLEui3ltvveVEvRPawjsfawuO4yilUCgUZLMOAOmwtdttAd3N+oz1Gg814QiqV6dSKYl4dFSOH9Hh+BjrKtaPlNpjW5/Kzow8Wmth+nu9XqytraHb7Yp0g4npsR5lpGYUp1NQPoM8V74Gr5NjRax3OVhMecBGoyGQxM2bN6WWNSEOTk0QrGe6y2mG9fV1eDweNBoN0X5x7GT2SDgfDw4A2aHAThzrG8qdUzKPMgvmGA0bGRRDYh3ZbDbR7XalhQ/MRo9s2xaYgmRqpoMmRMGJArb7GRUp7z6ZTEQoaTweC9WMEYbRl47KaxsOh9IQsSwLmUxGajjeDNh4ofOSq1qr1QS0J4TASfbJZCKNFZPcDUCiOqfSTdVu6s5w8cz29rbD4bwHW3jnY7eSHUROVbPhwfSOK8EqlcrcWMvtokqTyQTlclkOKnBrkoEAOFv6nJtjG53OYKqNsVGjtUY+n5+brmi1WuIATEWZ4jFNJB7I62STiNFaKYVMJoPz588jHA7LkCxvCJw+oFOyxiXPlHghMwCm0cQWGWVp5mZbTumXSiX5DNgAqtfrwoRx7GS28M4HzK8J44ZXNjTMDiUlDJhuArcm4TnTxwYEU1UznTRXbJmp6e2zgOxeEuvq9XoC9DO1LZVKAui73W6RHzSl5wkh8JqY4pEjSpmKlZUVuFwu2RdB4jZrWuqIEtogUM412mS4MLqaGjOmyjdBeC5XSSaTuH79uuiDsu5utVqo1WqyQNSxk9kj4Xxs5/f7fViWhWAwiGazKbhZMBj8vsPu8XgEZuAKLtZrZgODQ6U8fOboDg8pgXIeZoonEaBnJGMkZbeQjQ/+LLU22VFkSghAurG8UXi9Xqyvr2N1dRWBQACVSkX2KPDwE1Pk/KLWGplMRlJvar/wtZiWM5Ogc1NTBgAef/xxuT4A0hmmZD1T9Uqlgmaz+RBOww+PLbzzUeKBNRwpTHQM1j3EqQglmMJCJtMkGAyK+O10OhVpeEaGaDSKM2fOiLMeHBzMEYdNFWpihPydbJIAkJTTxBVrtZo4KBs25pwcX4PaMF6vV3A3snY4aMtalJABYQnSwdjZZYRnegvMIItwOIy1tTWBTjKZjDRwCPRziFkphdXV1TkstFQqOWJJ92gL73ws7gFIE8DcCTedTqXtzpSL0Y0/z1TQdETOp3EUiJGr1WqJNB6J2qx9ut2uNESYzvl8vrnFnXRIOno4HEa5XMbu7q7UlZzBY6TTeiZWS+oZ8beDgwNJIQl8mx1N4nnJZFLqRA4dM8qzOcM02VzOwkYNU00u5EwkEjg4OJDIxl0VbMA0m00UCgWJjo6dzBbe+SitYLL+CXzTsQiGswtJ3M+cPufwqDkLR2fg18Tw9vf3peZjZGGNaSqARSIRqYHMDigHZskMMVNfOhtfk6kuIwodfjgcymZb4BbTx+v1ygoxUsl8Pp84L4F81rpMNalhSidkdEulUlhdXYVt29KVPTo6wvb2NlKpFLTWSKVScyyg3d1d5PN5h0R9j7bwzsdOoclS4fApGyCmKBGbKWRsML1jqmlKN1Cdy9zXwKhEShZ/L7E+ysHToTiaw4jDqEQqGesjdmYZLUwJDLfbLeRqEz5hGkseKSMZ6WDhcFjAcEIhnAukqC4dJBKJYG1tDTs7O2i32wBmE/Jnz54VXdPJ5P9v71xj48zu8/4ccngbznCG1xmSyyWllbauBBvbxJfYcb3dokiTfAlSoK37oZsPRt0ADtoCqQEn6QfDQIC2qFOgKBogQQqkRWvXQHpZB6170Vpe73oDiSuJWUlcXoakOLyOyJnR8G6RPP0w8zs8o0hrkSuSQ/k8AKHhy5nh+1Lvf/7X5/nvKZ/Pa2FhwZ0HHnx3d9eN3S0vL2tpael0bojnCDVvfORDVCTpheE9yNfo4fk8PXIbihEwBcjnJLmyvHTQOqCBTeGBWUpCRUJM2gKErcjoweVjoaUv3UfLA+OW5MJCci5CTT502tranAfO5XJuioaNsnh9ClDMhDISBnF2fn7ejcal02mlUikVi0Vls1nHXSR8peXCBwlN9o2NDd27d88ZcMDRUfPGJ8mV+aHL+NJ/GJIfFtXV1amnp0dSea1YLBZzhRmpbGRUQ6leUvzAozEXSoEExS96gRiz7z0psqBqHY1G3XAAXpCCCkbW0NDgdibgffF4yBSm02m3xgujZlU2LQwqoai2IeybTCaVSqWUz+ddJbitrU1dXV2am5tzyz/hDCLQy/USIfD7CoWCJiYmQr73DFDzxodXYLpeOsgDfQk+5Oz8RSqoObP1lTlPqpbke3giPBNNZ0JCQk6KGJLcamrCOzwoYS/vjSFQzaR36Kth19WVF3Lye2kfYJBM+XzmM5/Rj370I9dfy+VyikajjpyLnAasD4w0Eok4SQgkB8fGxlxBhVBbkquo+lqd8XhcsVhM+/v7ymazymazob/3DFDzxifJ8d4aGhqUSCSq+ksoaRHOUTTZ3d1VKpVyWi7IqxtjXGkeA+AGp3LY1NTkViA/Op5GiEl+SNOdAgiSeq2trS4v4wOEdgajcQjsMprGNcBAp3C0tramT33qU/rqV7+qN954Q9/4xjdckaevr8+NrNF+aGpqcnOm7e3tSiQSbr+eMUazs7POSGm8U4ghdJXkJAIx4L29PY2Pj7vRtICPhqMqVn9d0j+QdL/ytN+21v7Pys9+S9KXVJaR+EfW2v9dOX4kxWqKFPv7+24jLaV9PySkGNHc3Fw1DcPw8qOEWl/rk7lGjJHNQuSV0gGpFyNkWJrwl3CR5Sfnzp3TysqKK8kT1mG8bAaiPUI+K8n1KZnrNMYok8lodnbWGTnnRbWV6ReuraenR4ODg7pw4YJaWlo0Pj6uO3fu6MGDB44Ngj6nn28ytcOHlM+ffPDggW7duhVYDM8IR1WslqR/ba39V/4BY8wlSV+UdFlSn6T/Z4x52Vq7pyMqVu/t7alUKrncjMdMcPiTIggIUQzwNSvh59F+8MNJf+yqWCw6DydVKzEbU14xTYFDUlUvjZCTEJixNOlgiackN/mysbGhfD7vhqgZaqYySt8uEoloZWVFr7/+urtWPiR6enq0sLDgKERSueL66quv6tKlSyoWi3rnnXc0Pj6uUqmkpaUl7e7uOhoSQlH0PSlUUaxBPj8SiejGjRuanZ19ilsm4GlwJMXqD8GvSPq2LUsIThtjJiV92hgzoyMqVltr3SIShoTb2tpcfkNut76+rmQy6cJEhqCRcKAoQ1hKscVXr6ZxzftS6aNvRnEHdgI9RH/ByubmpnZ2dpzSGRMqPnugoaHBrbj2WRQ+D1CSK3jwYVEqlZxqGlMnqVRKU1NTLkeLRqP67Gc/q49//OOamJjQ1atXtbq6qu3tbS0tLVXJVtDnpM9IjskHkySn0La+vq5r164FnZZniI+S8/2GMeZ1ScOSftNaW1BZhfrPvOegTP1Qh1Cs9gERdH19vYpNzU2Px5Hk5h5pT/g6nYR4kqrY7r58vD8+RUsBr4c3Ij+iuugXRR4dZysUCm6PA1VSDJd2BSE1YR7hsE8T4hx9ulIsFtPQ0JDjLvKh9PnPf16vvfaabt++rStXrri+JCpn/iJMeoPxeNwVqfj7+ipvkUhEw8PDymazR7xVAh6Ho26x/31JL0l6RWWV6m9Wjj9JmfpQitXGmC8bY4aNMcM+E4FeFJVL8h5faIhKpiT3HG42Kp0NDQ3q7OxUMpl0TXJIrFQukWUgT/NzQPY0MCRNVZB8rKWlRZLcuBkTKB0dHY7U6xdfCE/9zbUYaywWc+dPYae5uVlDQ0Pa3y+vQKOvd+7cOV2+fFmFQkHvvvuu269H3smHC71MJAa5Ls4BjiGh8Orqqq5fvx683jPGkTyftXaZx8aYP5T0p5Vv5yQNeE9FmfpQitXWk4vv6uqyzDzu7u46rU6feV1XV17OsbxcPi1/6QnFAvJEmvAQSzEOf1MRkyHMcT7KcmDYmOf5XEGpbHSw2wnbkP5bX193xkReiZHiBTFgP8yl+tnS0qLe3l73XtB8ent71d/fr5WVFd29e1eTk5OOhYECAFEC0hN87xONCTfZdyFJb7/9tpNKDHh2OKpida+1drHy7a9Kul15/Iak/2yM+T2VCy4XJV2zZQXrIylWw7rmRsWQMELylYcPH7rtQdy0fPE9gkl8omNw5H7wBKEP0YYglGTMrLm52YVpfsWVHK5UKrl+Gt4UDh9tBDw6+RXng4elmksPUyr3/yiAUDQxxmhoaEhDQ0OujTA/P181ScPfBZEohgQIwxm3Y7gAT5xIJDQ7O6ubN28GWcBjwFEVq/+aMeYVlUPHGUn/UJKstXeMMd+RdFflBSpfqVQ6pSMqVlOoYNofSg2eCUPCK/hrivFkVBrJBf0wEa9A7oZnI88iJ2Kuc3d31xm7r9wslauM/f39Lgys/E1cjge3b3193fHpGHfzGQ4MQDPBgremYptOp9XU1KRCoaDe3l43VM10DdfoL4ChUc5QOX8bcl0Ws/D3RNT3rbfecnOrAc8Wz1SxuvL835X0u485fiTFaj6du7u7HUfNJ7aura05D+ErPFOOpyhDaEVuw24Hn5mOR2Hq32fBM+pFvkgeCjuA8JWbGs9JWEjOSgGJ/NUvHNF8j0QizugYkkZx7Pz584pGoxobG3NbZCORiC5duqSWlhZNTU05I+V6eT+GD1Di5ndh+OSwTMb88Ic/1OLi4of99wR8BNT8hAtzmtygiUTC3XAoS5Nv0SLAKKHnEJ5SwfOHo8kd8UB4yUgkUkUnKhQKqqurUzKZ1MrKituOROWRG9aXokeYiV7hzs6OK/PTgsAz0lvkQ4Acl7lLQs7R0VHdv3/f5YjNzc0aGBhQa2urJiYmqgiuTPzg1aioUlXl2n2Ge0NDg7q6ujQ8PKyxsbEwRnaMqHnjk+TkyH3lLtS5yEX8yiY5Gdt9aKbTdmDJI4bY1NSkfD7vxrzID33xWDwpY1y+LCBq0b4wr89M8PuI9Bjh88FskA6a7/yLTHsqldLm5qZu3Ljhrrm+vrzv4ROf+IRisZjm5+ddFBCJlHcEQhqm3UHOyocIfUQqxJFIROl0Wjdv3tTIyEgQRzpm1LzxcdMQFvkyDOQ00oHchN+KwPOR1+ARfVEkQjNY2l1dXW7fOopdeA4kDAlr0WbxtVKgE5Er4nW3t7dd0YUCkN+fpJ+GURtTloB/+PChFhYWVCwWq1oee3t76u3t1QsvvKCbN2+6CqnPVyRsJSSnagxdiDG5nZ0dRaNRxeNxDQ8P68aNG4G1cAKoeeNDzZnJDPISQjYmO8j3yKOYi8TzkXsxyYEHKhQKLhSjCY1Bx2Ixtbe3V63sokzf09PjCjbFYtEZtL8UhfdtbW2tmqAh9JQOZlcJTff29tTR0aHGxsaqddJ4fEit58+f18DAgK5cuaL9/X319va6qR52olONpWIL+Zb2Q0dHh9Mn3dnZ0dWrVzU+Ph4M74RQ88YHOZSczG8IQ8VBpMhvAeDNyAMJTykykOMwS4nh+KvCyIlowCO2lEwmdeHCBadlQuMczwXjQZKjPpGz+ddAvoo6dF1dnfr6+tTU1KTJyUk3FkaYGY1GdeHCBfX19WlyclKZTEb7+2X5fN8j4n2p6sLhw/AoqqBcNjk5qStXrji2Q8DJoOaNj9zG2vIec18eEG9AJZKci2Y6+QyG4YdtkhzHzpcN9FsXq6urqq+vd140mUzqpZdeUktLi6anp7W4uOjWdHGzI+nul/35PXhBSVWMCJgY6XRae3t7eu+995xIFIJJHR0dbiPsyMiI82pIKfL+y8vLrhfp68LEYjHnwbu6unTu3DlJ5Qb61atXlc/nH/8fEHBsqHnjk6R0Oi3pYK2zJJfLIXTEgDUalISVKEbv7u66G186aNYzMI038osmbPeh5xWJRLS4uKhcLuc2+xCqUqAht/Nl+piSwdh4Pl4vkUjoxRdf1NLSkkZHR13xg2mW9vZ2bW1taWZmxn0w0Npoa2urEvBltI0PKVjpSFn09fUpkUhoYWFBb775pu7cueNC5YCTRc0bH6x1n6+Hp0KmzxcYkg6KNMwq+g1nQjxWNfu6lj7NKJFIqK+vT83Nzdre3nZ7yamo+vkmITAhnXTA+4NUi4elmd/S0qL+/n5duHBB29vbyuVyWl1ddWNxnZ2dbrPs+Pi4C7vpz9EvJJ+k4Y+yma9PSnEqnU6ro6NDw8PDunLlihYXF0OYeYowtf7H7+zstK+++qrrwZG/wXOjeOKPSjF3idchtPOFi+D8UeonT2xsbNTQ0JCi0ag2NzfdwkrUr2nM+xMxeD3ODY/KXCmGjpd88cUXNTAwoGQyqVwup4WFBe3tlVeT4XHz+byWlpaqwmuuiTCVvYS0RHxmBuvB2tralEqlFI1Gtbq6qnfffVc3btwIAkgnCGvt44gFtW987e3t9nOf+1wVhQfKDkbg706Q5G5gbthIJOKoPHg6mtmwBggXaZIzP8r2I/IrdFAo2PhD3ngY5iTR7YSdYIzR4OCgLl68qIcPH2p8fNxVSglhKZq0tbW5/ps/JtbY2KhkMulWmiF6xDniDSn0dHV1aXt7WyMjIxoeHtbS0lKY0zxhPMn4aj7sJHSj+giTXZITriXHgqnuy7ozqcJ8pS/HBwkX+b9EIuHWOyNGi9GgMEbfDIMzxjjPRsWV949Go24+MxKJqL+/X6lUSoVCQYVCwbUDOA/CSanMKkgmk465AC2JJjz5LsK66LQwP/ryyy9rbW1N7733nq5fv65MJhMoQTWGmjc+WgORSKSKycDcJFP/EGnxDoD8jgooYkz379+v2gbU2toqY0wV8zwSiai7u9uFtojf+lqf9NV8pTIUyTY2NhSLxdTd3e30Q998803HKMejMfZFyEyLAA8jQ20SAAAPpUlEQVRIHhmJRNTe3u68LQJN6HpubW0plUopnU5rcnJSb7/9tkZGRlzrIaC2cGbCTl/efWtrq4rZQPhJOOr3AvGY5H8YFh7E59JREWREjYIFEynMlzJ7SaUSL4vRUfxIJBJqbGzU4uKiFhYW3NwoDXe/D8e50JhnzpLCCUx13o9hgWg06lgOFICuXbumd955R3Nzc2FErAZwZsNOQjHGpSS5cS1jjBOqlVRlIFQGCQ/5ORXP1tZWRz2CpMv0CUbBZA0GGo/H3SwnedbGxoYzeNoAsCBmZmaqzg+PSdUWIySfIxfzdyBYa3Xx4kV1dna6MTPmM+vr63XhwgW1t7e7qui1a9c0MjISCipnADVvfPX19W6DDsPShGv+tlWqfRiU7xH9qiQ5GK2BfD7vwldf62Vra8vljBz3Q9P5+Xnn4eLxuDo7O10uurOzo6WlJVdsofCC0eFpOX8KRXguri8Wi7nJmkQioUKh4ELNrq4uDQwMqK+vT+Pj4/r+97+v69evK5fLhQUmZwQ1b3wQTH2hIm5iQjOKLnhDjNH3MlQ8GSTmueRwtCD8fNHXTNnd3dXq6qq2trYUi8UUj8fV1dXlPBgG6yuZMe/JtI3/uxkAZ6BaOmDDY3i0QTY3NzU3N+cKOJcuXVJDQ4NKpZK++93v6gc/+IGmpqaCnuYZQ80bH7kchRY4bpT5/ckSDAhD5TV+xbFQKDhvwxc3v0/vgfWNihhMBjwg23GXlpacmjNEWnJN1jHzOnpyvsCS/+HgizHBMsALdnZ2uumUzc1NDQ8P66233tLo6GhYUnlGUfPGZ4xxStXsjyNkA/5WWrwV/Tt/wJnGMzkTy0jIFWGPNzU1aWtryxk578/77OzsVElYwKKIxWKuzxeJRFyBRDpgORDWcpxiDgUTJDMgwvb29mpoaEjxeFz5fF4jIyOuikl1N+Bs4qhy8f9F0l+qPCUpqWitfcWUxXVHJY1VfvZn1tpfr7zmSHLx3Ji+TDy9Om588jV/6YnfSPZlE3ymw9ramtNbQTzIGKONjQ2tra25fBJ+nC+YRCGIUBZhWaqZPg/R3wnBufM8X6QJ1e14PK7BwUENDQ258blbt27p5s2bev/997W6uhryuucAR5KLt9b+XR4bY74p6YH3/Iy19pXHvM+R5OLJpRDxwVB87h6kVRZK+opl/k3Kph2mU+jxMU9ZX1+vYrHoVKObmpqcEWJoktzrOR9Jjo5EDxKDQr4CKXk83KNryXZ3d9Xe3q6+vj6lUinV1dVpZWVFmUzGGV0+nw9G9xzhI8nFm3Id/+9I+usf9h7GmF4dUS6enMgfUIa/t7dX3qSKrEMkEnF705F991XMCoVClc4nBsFcpyS3w4AWAZ6XUNDX6aQnSA8RyQi87qP0JVofeEVC2o6ODg0MDCgejyuRSGh1dVWjo6MaGRnRxMSEO++A5wsfNef7q5KWrbUT3rFzxpibkkqS/pm19ocqS8M/tVy8MebLKntJRaNRdXR0uMkOWgDkO7Qf8IIQW8m76MX5DW0KHbQSksmke74v745k4I9//GM3QuYPcPtSFhg5fUWM01dTI6SlZ9jR0eFoQ/X19Zqbm9ONGzd069YtZTIZlUqlkNM9x/ioxvf3JH3L+35R0ovW2tVKjvffjTGXdUi5ePuIYnVnZ2dV4YRRL4ouhJ3os+Dd9vfLa44xMt97ER4aY5yeC9J9TI5IquLWoVbG+2BYFIFoIfiLSBiwxvBisZij9jCbOTs7q8nJSY2OjiqbzbqmfcDzjSMbnzEmIulvSfpZjtnydqKdyuP3jDEZSS/rkHLxPvb39zU9Pe2m/2mWS3IsBH+PAhMueDjf4Crn5bweDHe/ognBlbXQ/oiXT2HyJ2hgVdCzo28oyXm9vb09dXd3a3Bw0PX67t69q5mZGWUyGS0vL4fB558yfBTP9zckfWCtdeGkMaZbUt6W5eHPqywXP2WtzZsjysXv7++rWCxW7TFgvIyqpSS3tRZVLhSq/fXRfv7Hii96feR2ra2tzjh8uT/CSFoBhJG+FITPikchOhaLuZyVIsr09LTGxsY0PT2tQqEQBIt+SnEkuXhr7R+pvATzW488/QuSvmGM2VV5M+2vW2sRBzmSXDw9u/X1dT18+NApQROasXXHlwBks6uvykyYybA08OXj/aHt5uZm9/4YGFVUVlHDZqdvSKM9mUzqhRdecPy67e1tt0NhdHRUMzMzThQq4KcXNc9qSCaT9rXXXnMCtEyObG9vuz6dHzIyCiaparaTBjahqCRXgOF9qUD6f5NoNOokIBi6puKKF6X40t3drc7OTnV3d2t/f1+5XE5LS0saHx/X+Pi4lpeXXRU24KcHZ5bVQDiI7gkscjwURFjCSulgeSZGB/xlJBgdjW2YB3g4jJQeIK/h/f0xs/b2dg0MDKilpUUrKyuanZ1VNpvVBx98oMnJSeXz+UDtCfgLqHnjk+RUuRjnYsSLZjfTJhic7+0kuUY2oSUGxXJK+n5MwPgFE9oJklw+2d3drY6ODvX396u3t9dNoWSzWY2NjbmqJRMvAQGPQ80bH6RV1J6ZdiGERJvFHylj0JlpEooj/vQLhkTPD0Omge8XU2gvQB3q7u52vcG5uTlls1lNTk5qfHxc8/PzoWoZ8FSoeeOTyqEnOR6aLlI5RPS3s+L9WAYpyTXmGenCq/nTKQB2O6+jl9fQ0KB0Oq2+vj4lk0k9ePBA2WxWc3NzymQymp6e1v3790PVMuBQqHnjI4QslUqKRqNuGxD0IMJMiiWSquT0aAFIqmKpS3KhJTkdCs8Mbzc3NyudTiuZTLo2wsTEhObm5jQxMaF79+65iZiAgMPiTBgf6lwYHLkUe/LI+fB8hJ+P6nfi6fwxMb/3Rwjb0NCgoaEhx8Pb2trS8vKy5ufnlclktLS05EbaAgKOipo3PhS78vm8W9m1s7PjuHn+ELNfbMHYeA9/y6z/3tKBtHoymVRPT4/b3b66uqpisahsNqvp6WnlcrkgrR7wzFDzxre3t+eIsPDpGhoaXJWScJDnElLSVKchzkiaP2aG12tra1N/f79jwZPTzczMaHp6Wvl8PuRzAc8cNW981toqSo1f4ZTkhqmpcFLlpBcHM4GCiy8b0dPTo9bWVvX29mp3d1dra2uan5/XzMyMpqamQn8u4FhxJoyPlsHOzo5jmT9uBRdVTGQa0Hyhgomna29vl7VWPT092tvbU6FQ0OLioqampjQzM6PV1dXg6QKOHTVvfDDIGXKmR4e8unQg6cC/EGsxwMbGRrW0tLhNs77WytLSkqanpzU1NRWMLuBEcSaMLx6Pa3Nz0zW9Ge2C1YBUgzHGSe4Rnra1tamtrU2dnZ2O4Y6u5r1795TJZJx0fEDASaLmjc9fTuKzwxkz8ylFfEENSqVSam9vdxIUGxsbztPNzMxoZWUlVC8DTg01b3ywCeDIoRTm8+BoiCcSCaVSKVdIYVnlxsaGcrmc5ubmQk4XUDOoeePzmefIq6MsTf+O3QjpdNqJ3eZyOe3u7qpUKmlxcVHz8/NObj0goBZQ88ZHBZM2Qnd3t4rFotNFSSaTTuh2Y2NDxWLRzXMWi0Xdv39fpVIpEFcDag41b3yRSEQ9PT3K5XJqa2uTVJ7RzOfzbgMrokooie3s7KhUKjlBo4CAWkTNG59UZop3dXU5qT1rrTKZjFMwo23AemWMMCCgllH3k55gjBkwxnzfGDNqjLljjPnHleMdxpj/a4yZqPzb7r3mt4wxk8aYMWPM3/SO/6wx5v3Kz/6N8ZfnfQgaGxvV39+vwcFBfexjH1Nra6tGR0dVLBb14MEDtzsdOflgeAFnAT/R+CTtSvpNa+1flvRzkr5ijLkk6WuSrlhrL0q6UvlelZ99UdJllSXh/50xpr7yXkjGX6x8/eJP+uXGGA0NDeny5cvq6elRU1OT7t69q2w268JNpPwCAs4SfqLxWWsXrbU3Ko/XVF6E0i/pVyT9ceVpf6yy/Lsqx79trd2x1k5LmpT0aV8yvrIg5T94r3kimpqaNDg4qM7OTnV0dCiXy+l73/ueE04KOV3AWcXTeD6Hys6Gv6Ky9mbKWrsolQ1UUk/laf2Sst7LkIZ/asl4Y8yXjTHDxpjhtbU1tbW1qampSaVSSTdv3tTExERoGQSceTy18RljYpL+RNI/sdaWPuypjzlmP+T4Xzxo7R9Yaz9prf1kPB6XVFanzufzun37dtg3HvBc4KmMzxjToLLh/Sdr7X+tHF6uhJJsIcpVjs9JGvBejjT8kSTjo9GoGhsb3aTKBx98EKZTAp4LPE2100j6I0mj1trf8370hqRfqzz+NUn/wzv+RWNMkzHmnMqFlWuV0HTNGPNzlfd83XvNE4E+y87OjorFohYWnmrFQ0BAzeNp+nw/L+nvS3rfGHOrcuy3Jf1zSd8xxnxJ0qykvy1J1to7xpjvSLqrcqX0K9ZaSpGHloz39x8sLCyoWCw+9cUFBNQyal4u3hizpoM1088LuiStnPZJPGOEa3o8Bq213Y/7wVmYcBmz1n7ytE/iWcIYMxyuqfZx3Nd0qFZDQEDAs0MwvoCAU8JZML4/OO0TOAaEazobONZrqvmCS0DA84qz4PkCAp5LBOMLCDgl1KzxGWN+scIHnDTGfO20z+cwMMbMVHiLt4wxw5Vjh+Y/niaMMf/eGJMzxtz2jp0Yh/M48IRr+roxZr7yf3XLGPPL3s+O95rQwqylL0n1kjKSzktqlDQi6dJpn9chzn9GUtcjx/6lpK9VHn9N0r+oPL5Uub4mSecq111fA9fwBUk/I+n2R7kGSdckfVblwfr/JemXauyavi7pnz7mucd+TbXq+T4tadJaO2Wt/bGkb6vMEzzLOBT/8RTOrwrW2rck5R85fCIczuPCE67pSTj2a6pV43sSJ/CswEr6P8aY94wxX64cOyz/sRZxbBzOU8ZvGGP+vBKWEkof+zXVqvE9NfevRvHz1tqfkfRLKstufOFDnnvWr1V6BhzOU8TvS3pJ0iuSFiV9s3L82K+pVo3vSZzAMwFr7ULl35yk/6ZyGHlY/mMt4kQ4nCcJa+2ytXbPWrsv6Q91EPIf+zXVqvFdl3TRGHPOGNOosiDTG6d8Tk8FY0yrMSbOY0m/IOm2Dsl/PNmzfmqcCIfzJMGHSQW/qvL/lXQS13TaVbUPqUz9sqRxlatMv3Pa53OI8z6vcpVsRNIdzl1Sp8oqbxOVfzu81/xO5TrHdIrVwEeu41sqh2EPVf60/9JRrkHSJys3dEbSv1VlqqqGruk/Snpf0p9XDK73pK4pjJcFBJwSajXsDAh47hGMLyDglBCMLyDglBCMLyDglBCMLyDglBCMLyDglBCMLyDglPD/Aa1rTLNudrDPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ii)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(gray_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "\n",
    "# Store variables with pickle\n",
    "def load_pkl(filename):\n",
    "    var = pickle.load(open(filename, 'rb'))\n",
    "    print('Loaded data from:', filename)\n",
    "    return var\n",
    "\n",
    "def store_pkl(var, filename):\n",
    "    pickle.dump(var, open(filename, 'wb'), protocol=4)\n",
    "    print('Stored data in:', filename)\n",
    "\n",
    "# Display images\n",
    "def display_image(num):\n",
    "    return Image(filename='images_png_2048x1664_1_channel/{0:08d}.png'.format(num)) \n",
    "\n",
    "def display_tensor(tensor):\n",
    "    plt.imshow(tensor.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_image(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mit_risk_data import MammoBCRiskDataset, Resize, FaceLeft, FaceRight, NormalizePix, ToTensor3D, mammo_collate\n",
    "trans = transforms.Compose([FaceRight(), NormalizePix(7047.99, 12005.5), ToTensor3D()])\n",
    "risk_dataset_t3 = torch.load('../time_set/risk_pred_GEHolo_4view_matchedSepCase_T3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    <mit_risk_data.Resize object at 0x7f137b51ab50>\n",
       "    <mit_risk_data.FaceLeft object at 0x7f137b51ac10>\n",
       "    <mit_risk_data.NormalizePix object at 0x7f137b51ac90>\n",
       "    <mit_risk_data.ToTensor3D object at 0x7f137b51ad90>\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_dataset_t3.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    <mit_risk_data.FaceRight object at 0x7f137bab4310>\n",
       "    <mit_risk_data.NormalizePix object at 0x7f137bab4bd0>\n",
       "    <mit_risk_data.ToTensor3D object at 0x7f137bab4f10>\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_dataset_t3.transform = trans\n",
    "risk_dataset_t3.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'same-cc': tensor([[[ 1.9369,  1.8823,  1.8103,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 2.1556,  2.1010,  1.8702,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 2.2770,  2.2276,  1.9316,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          ...,\n",
       "          [-0.5871, -0.5871, -0.5871,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [-0.5871, -0.5871, -0.5871,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [-0.5871, -0.5871, -0.5871,  ..., -0.5871, -0.5871, -0.5871]]],\n",
       "        dtype=torch.float64),\n",
       " 'same-mlo': tensor([[[ 4.4759,  4.4290,  4.4517,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 4.5457,  4.4545,  4.4679,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 4.5443,  4.4558,  4.4196,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          ...,\n",
       "          [ 1.8437,  1.4614,  1.6171,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 1.9162,  1.4493,  1.4548,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 1.9712,  1.6063,  1.5473,  ..., -0.5871, -0.5871, -0.5871]]],\n",
       "        dtype=torch.float64),\n",
       " 'opposite-cc': tensor([[[ 2.7569,  2.4570,  2.1676,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 3.1596,  2.8823,  2.5529,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 3.6943,  3.3956,  3.1756,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          ...,\n",
       "          [ 4.2796,  4.1023,  3.9529,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 4.3477,  4.2770,  4.1423,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 4.4196,  4.2996,  4.1703,  ..., -0.5871, -0.5871, -0.5871]]],\n",
       "        dtype=torch.float64),\n",
       " 'opposite-mlo': tensor([[[ 4.4889,  4.3036,  4.3573,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 4.3922,  4.3144,  4.3036,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 4.3412,  4.2875,  4.2727,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          ...,\n",
       "          [ 2.0342,  1.9979,  1.9308,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 2.0530,  1.9361,  1.9496,  ..., -0.5871, -0.5871, -0.5871],\n",
       "          [ 1.9429,  1.8516,  1.8462,  ..., -0.5871, -0.5871, -0.5871]]],\n",
       "        dtype=torch.float64)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_dataset_t3[0]['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2048, 1664])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_img = torch.zeros(1, 1, 2048, 1664).to(device)\n",
    "dummy_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(dummy_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3773, -3.0506]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, :2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6623, 0.3377]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "print(F.softmax(a[:, :2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'benign': 0.09280401468276978, 'malignant': 0.04732837155461311}\n"
     ]
    }
   ],
   "source": [
    "predictions = np.exp(a.cpu().detach().numpy())[:, :2, 1]\n",
    "predictions_dict = {\n",
    "    \"benign\": float(predictions[0][0]),\n",
    "    \"malignant\": float(predictions[0][1]),\n",
    "}\n",
    "print(predictions_dict)\n",
    "del dummy_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['same-cc', 'same-mlo', 'opposite-cc', 'opposite-mlo'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_dataset_t3[0]['images'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from: ../ys_t3.pkl\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../ys_t3.pkl'):\n",
    "    ys_t3 = np.array([ sample['label'].item() for sample in risk_dataset_t3])\n",
    "    store_pkl(ys_t3, '../ys_t3.pkl')\n",
    "else:\n",
    "    ys_t3 = load_pkl('../ys_t3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - [64 21], val - [64 21], test - [64 22]\n",
      "train - [64 21], val - [64 22], test - [64 21]\n",
      "train - [64 21], val - [64 22], test - [64 21]\n"
     ]
    }
   ],
   "source": [
    "n_folds = 3\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=12345)\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t3), 1)), ys_t3):\n",
    "    train_y = ys_t3[train_ix_]\n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(train_ix_, test_size=test_prop, \n",
    "                                        stratify=train_y, \n",
    "                                        random_state=12345)\n",
    "    print('train - {}, val - {}, test - {}'.format(\n",
    "        np.bincount(ys_t3[train_ix]), np.bincount(ys_t3[val_ix]), np.bincount(ys_t3[test_ix])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(risk_dataset_t3, train_ix)\n",
    "val_dataset = Subset(risk_dataset_t3, val_ix)\n",
    "test_dataset = Subset(risk_dataset_t3, test_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = ys_t3[train_ix]\n",
    "val_y = ys_t3[val_ix]\n",
    "test_y = ys_t3[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0, f1 = np.bincount(train_y)\n",
    "train_w = np.zeros_like(train_y, dtype='float')\n",
    "train_w[train_y==0] = 1/f0\n",
    "train_w[train_y==1] = 1/f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cpu_threads = 4\n",
    "batch_size = 2\n",
    "\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    train_w, len(train_y)//batch_size*batch_size, \n",
    "    replacement=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, \n",
    "    num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "    collate_fn=mammo_collate)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, \n",
    "    num_workers=cpu_threads, drop_last=False,\n",
    "    collate_fn=mammo_collate)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, \n",
    "    num_workers=cpu_threads, drop_last=False,\n",
    "    collate_fn=mammo_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom time import time\\nstart = time()\\ntrain_loss_, train_auc_ = val_loss(model, train_loader, return_auc=True)\\nprint('Init avg train loss={:.3f}, auc={:.3f}'.format(train_loss_, train_auc_))\\nval_loss_, val_auc_ = val_loss(model, val_loader, return_auc=True)\\nprint('Init avg val loss={:.3f}, auc={:.3f}'.format(val_loss_, val_auc_))\\ntest_loss_, test_auc_ = val_loss(model, test_loader, return_auc=True)\\nprint('Init avg test loss={:.3f}, auc={:.3f}'.format(test_loss_, test_auc_))\\nduration = time() - start\\nprint('Time elapsed={:.1f}'.format(duration))\\n\\ntorch.cuda.empty_cache()\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from time import time\n",
    "start = time()\n",
    "train_loss_, train_auc_ = val_loss(model, train_loader, return_auc=True)\n",
    "print('Init avg train loss={:.3f}, auc={:.3f}'.format(train_loss_, train_auc_))\n",
    "val_loss_, val_auc_ = val_loss(model, val_loader, return_auc=True)\n",
    "print('Init avg val loss={:.3f}, auc={:.3f}'.format(val_loss_, val_auc_))\n",
    "test_loss_, test_auc_ = val_loss(model, test_loader, return_auc=True)\n",
    "print('Init avg test loss={:.3f}, auc={:.3f}'.format(test_loss_, test_auc_))\n",
    "duration = time() - start\n",
    "print('Time elapsed={:.1f}'.format(duration))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_epoch = 0\\nepochs = 20\\ntrain_loss = 0\\ncheck_iters = 5\\nbest_name = \\'best_model_2.pt\\'\\nbest_auc = .0\\nstart = time()\\nfor i in range(start_epoch, start_epoch + epochs):\\n    for j, batch in enumerate(train_loader):\\n        model.train()\\n        optimizer.zero_grad()\\n        # Each iteration is equivalent to 2*batch_size.\\n        bat_X_1 = batch[\\'images\\'][\\'same-cc\\'].float().to(device)\\n        bat_y = batch[\\'label\\'].long().to(device)\\n        # forward-backward CC view images.\\n        #bat_logit_1, _, _ = model(bat_X_1)\\n        bat_logit_1 = model(bat_X_1)[:, :2, 1]\\n        #print(bat_logit_1)\\n        \\n        del bat_X_1\\n        \\n        torch.cuda.empty_cache()\\n        \\n        bat_logp_1 = F.log_softmax(bat_logit_1)\\n        \\n        del bat_logit_1\\n        \\n        torch.cuda.empty_cache()\\n        \\n        loss = criterion(bat_logp_1, bat_y)\\n        loss.backward()\\n        train_loss += loss.item()/2\\n        \\n        bat_X_2 = batch[\\'images\\'][\\'same-mlo\\'].float().to(device)\\n        # forward-backward MLO view images.\\n        #bat_logit_2, _, _ = model(bat_X_2)\\n        bat_logit_2 = model(bat_X_2)[:, :2, 1]\\n        \\n        del bat_X_2\\n        \\n        torch.cuda.empty_cache()\\n        \\n        bat_logp_2 = F.log_softmax(bat_logit_2)\\n        \\n        del bat_logit_2\\n        torch.cuda.empty_cache()\\n        \\n        loss = criterion(bat_logp_2, bat_y)\\n        loss.backward()\\n        train_loss += loss.item()/2\\n        # accumulate gradients from both CC and MLO images and \\n        # then take a step to update.\\n        optimizer.step()\\n        total_iters = i*len(train_loader) + j + 1\\n        if total_iters%check_iters == 0:\\n            with torch.no_grad():\\n                avg_val_loss, val_auc = val_loss(model, val_loader, device, return_auc=True)\\n                \\n                scheduler.step(avg_val_loss)\\n                \\n                avg_train_loss = train_loss/check_iters\\n                print(\"Iter={}, avg train loss={:.3f}, avg val loss={:.3f}, auc={:.3f}\".format(\\n                    total_iters, avg_train_loss, avg_val_loss, val_auc))\\n                if val_auc > best_auc:\\n                    best_auc = val_auc\\n                    torch.save(model.state_dict(), best_name)\\n                    print(\"Best model saved.\")\\n                train_loss = 0\\nduration = time() - start\\nprint(\\'Time elapsed={:.1f}\\'.format(duration))\\nprint(\\'Best model loaded.\\')\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "start_epoch = 0\n",
    "epochs = 20\n",
    "train_loss = 0\n",
    "check_iters = 5\n",
    "best_name = 'best_model_2.pt'\n",
    "best_auc = .0\n",
    "start = time()\n",
    "for i in range(start_epoch, start_epoch + epochs):\n",
    "    for j, batch in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Each iteration is equivalent to 2*batch_size.\n",
    "        bat_X_1 = batch['images']['same-cc'].float().to(device)\n",
    "        bat_y = batch['label'].long().to(device)\n",
    "        # forward-backward CC view images.\n",
    "        #bat_logit_1, _, _ = model(bat_X_1)\n",
    "        bat_logit_1 = model(bat_X_1)[:, :2, 1]\n",
    "        #print(bat_logit_1)\n",
    "        \n",
    "        del bat_X_1\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        bat_logp_1 = F.log_softmax(bat_logit_1)\n",
    "        \n",
    "        del bat_logit_1\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        loss = criterion(bat_logp_1, bat_y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()/2\n",
    "        \n",
    "        bat_X_2 = batch['images']['same-mlo'].float().to(device)\n",
    "        # forward-backward MLO view images.\n",
    "        #bat_logit_2, _, _ = model(bat_X_2)\n",
    "        bat_logit_2 = model(bat_X_2)[:, :2, 1]\n",
    "        \n",
    "        del bat_X_2\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        bat_logp_2 = F.log_softmax(bat_logit_2)\n",
    "        \n",
    "        del bat_logit_2\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        loss = criterion(bat_logp_2, bat_y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()/2\n",
    "        # accumulate gradients from both CC and MLO images and \n",
    "        # then take a step to update.\n",
    "        optimizer.step()\n",
    "        total_iters = i*len(train_loader) + j + 1\n",
    "        if total_iters%check_iters == 0:\n",
    "            with torch.no_grad():\n",
    "                avg_val_loss, val_auc = val_loss(model, val_loader, device, return_auc=True)\n",
    "                \n",
    "                scheduler.step(avg_val_loss)\n",
    "                \n",
    "                avg_train_loss = train_loss/check_iters\n",
    "                print(\"Iter={}, avg train loss={:.3f}, avg val loss={:.3f}, auc={:.3f}\".format(\n",
    "                    total_iters, avg_train_loss, avg_val_loss, val_auc))\n",
    "                if val_auc > best_auc:\n",
    "                    best_auc = val_auc\n",
    "                    torch.save(model.state_dict(), best_name)\n",
    "                    print(\"Best model saved.\")\n",
    "                train_loss = 0\n",
    "duration = time() - start\n",
    "print('Time elapsed={:.1f}'.format(duration))\n",
    "print('Best model loaded.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.load_state_dict(torch.load('best_model_2.pt'))\\navg_test_loss, test_auc = val_loss(model, test_loader, device, return_auc=True)\\nprint('Finetuned avg test loss={:.3f}, auc={:.3f}'.format(avg_test_loss, test_auc))\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model.load_state_dict(torch.load('best_model_2.pt'))\n",
    "avg_test_loss, test_auc = val_loss(model, test_loader, device, return_auc=True)\n",
    "print('Finetuned avg test loss={:.3f}, auc={:.3f}'.format(avg_test_loss, test_auc))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest_auc_m = test_max_auc(model, test_loader, device)\\nprint('Finetuned max-score-based auc={:.3f}'.format(test_auc_m))\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test_auc_m = test_max_auc(model, test_loader, device)\n",
    "print('Finetuned max-score-based auc={:.3f}'.format(test_auc_m))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3+ Years Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Fold 1 ==========\n",
      "Test AUC at start=0.496, max-score-based AUC=0.515\n",
      "Iter=5, avg train loss=1.275, avg val loss=0.668, auc=0.523\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.741, avg val loss=0.685, auc=0.501\n",
      "Iter=15, avg train loss=1.117, avg val loss=0.659, auc=0.493\n",
      "Iter=20, avg train loss=0.943, avg val loss=0.635, auc=0.514\n",
      "Iter=25, avg train loss=1.016, avg val loss=0.627, auc=0.505\n",
      "Iter=30, avg train loss=0.720, avg val loss=0.643, auc=0.506\n",
      "Iter=35, avg train loss=0.780, avg val loss=0.616, auc=0.539\n",
      "Best model saved.\n",
      "Iter=40, avg train loss=0.827, avg val loss=0.596, auc=0.569\n",
      "Best model saved.\n",
      "Iter=45, avg train loss=0.778, avg val loss=0.583, auc=0.592\n",
      "Best model saved.\n",
      "Iter=50, avg train loss=0.753, avg val loss=0.585, auc=0.593\n",
      "Best model saved.\n",
      "Iter=55, avg train loss=0.694, avg val loss=0.579, auc=0.606\n",
      "Best model saved.\n",
      "Iter=60, avg train loss=0.654, avg val loss=0.580, auc=0.616\n",
      "Best model saved.\n",
      "Iter=65, avg train loss=0.804, avg val loss=0.573, auc=0.625\n",
      "Best model saved.\n",
      "Iter=70, avg train loss=0.771, avg val loss=0.580, auc=0.624\n",
      "Iter=75, avg train loss=0.721, avg val loss=0.593, auc=0.633\n",
      "Best model saved.\n",
      "Iter=80, avg train loss=0.686, avg val loss=0.586, auc=0.645\n",
      "Best model saved.\n",
      "Iter=85, avg train loss=0.714, avg val loss=0.563, auc=0.668\n",
      "Best model saved.\n",
      "Iter=90, avg train loss=0.588, avg val loss=0.570, auc=0.665\n",
      "Iter=95, avg train loss=0.631, avg val loss=0.577, auc=0.653\n",
      "Iter=100, avg train loss=0.681, avg val loss=0.573, auc=0.648\n",
      "Iter=105, avg train loss=0.651, avg val loss=0.588, auc=0.651\n",
      "Iter=110, avg train loss=0.729, avg val loss=0.569, auc=0.646\n",
      "Iter=115, avg train loss=0.536, avg val loss=0.601, auc=0.644\n",
      "Iter=120, avg train loss=0.598, avg val loss=0.598, auc=0.635\n",
      "Iter=125, avg train loss=0.726, avg val loss=0.610, auc=0.615\n",
      "Iter=130, avg train loss=0.683, avg val loss=0.607, auc=0.627\n",
      "Iter=135, avg train loss=0.701, avg val loss=0.618, auc=0.607\n",
      "Iter=140, avg train loss=0.647, avg val loss=0.676, auc=0.602\n",
      "Iter=145, avg train loss=0.657, avg val loss=0.694, auc=0.595\n",
      "Iter=150, avg train loss=0.616, avg val loss=0.704, auc=0.596\n",
      "Iter=155, avg train loss=0.582, avg val loss=0.667, auc=0.605\n",
      "Iter=160, avg train loss=0.707, avg val loss=0.650, auc=0.602\n",
      "Iter=165, avg train loss=0.661, avg val loss=0.621, auc=0.610\n",
      "Iter=170, avg train loss=0.647, avg val loss=0.625, auc=0.616\n",
      "Iter=175, avg train loss=0.627, avg val loss=0.616, auc=0.614\n",
      "Iter=180, avg train loss=0.658, avg val loss=0.616, auc=0.635\n",
      "Iter=185, avg train loss=0.624, avg val loss=0.629, auc=0.639\n",
      "Iter=190, avg train loss=0.674, avg val loss=0.631, auc=0.631\n",
      "Iter=195, avg train loss=0.630, avg val loss=0.649, auc=0.619\n",
      "Iter=200, avg train loss=0.672, avg val loss=0.665, auc=0.625\n",
      "Iter=205, avg train loss=0.633, avg val loss=0.643, auc=0.618\n",
      "Iter=210, avg train loss=0.583, avg val loss=0.641, auc=0.609\n",
      "Iter=215, avg train loss=0.627, avg val loss=0.643, auc=0.610\n",
      "Iter=220, avg train loss=0.591, avg val loss=0.664, auc=0.616\n",
      "Iter=225, avg train loss=0.628, avg val loss=0.662, auc=0.610\n",
      "Iter=230, avg train loss=0.603, avg val loss=0.697, auc=0.602\n",
      "Iter=235, avg train loss=0.609, avg val loss=0.710, auc=0.616\n",
      "Iter=240, avg train loss=0.567, avg val loss=0.705, auc=0.607\n",
      "Iter=245, avg train loss=0.620, avg val loss=0.687, auc=0.603\n",
      "Iter=250, avg train loss=0.550, avg val loss=0.713, auc=0.585\n",
      "Iter=255, avg train loss=0.656, avg val loss=0.727, auc=0.580\n",
      "Iter=260, avg train loss=0.608, avg val loss=0.723, auc=0.586\n",
      "Iter=265, avg train loss=0.596, avg val loss=0.732, auc=0.578\n",
      "Iter=270, avg train loss=0.556, avg val loss=0.755, auc=0.575\n",
      "Iter=275, avg train loss=0.597, avg val loss=0.745, auc=0.586\n",
      "Iter=280, avg train loss=0.625, avg val loss=0.732, auc=0.582\n",
      "Iter=285, avg train loss=0.621, avg val loss=0.717, auc=0.586\n",
      "Iter=290, avg train loss=0.507, avg val loss=0.700, auc=0.600\n",
      "Iter=295, avg train loss=0.692, avg val loss=0.687, auc=0.596\n",
      "Iter=300, avg train loss=0.664, avg val loss=0.731, auc=0.604\n",
      "Iter=305, avg train loss=0.583, avg val loss=0.701, auc=0.599\n",
      "Iter=310, avg train loss=0.573, avg val loss=0.708, auc=0.596\n",
      "Iter=315, avg train loss=0.557, avg val loss=0.695, auc=0.598\n",
      "Iter=320, avg train loss=0.696, avg val loss=0.699, auc=0.595\n",
      "Iter=325, avg train loss=0.539, avg val loss=0.730, auc=0.590\n",
      "Iter=330, avg train loss=0.484, avg val loss=0.747, auc=0.579\n",
      "Iter=335, avg train loss=0.545, avg val loss=0.754, auc=0.578\n",
      "Iter=340, avg train loss=0.550, avg val loss=0.792, auc=0.574\n",
      "Iter=345, avg train loss=0.573, avg val loss=0.834, auc=0.563\n",
      "Iter=350, avg train loss=0.626, avg val loss=0.876, auc=0.556\n",
      "Iter=355, avg train loss=0.540, avg val loss=0.832, auc=0.567\n",
      "Iter=360, avg train loss=0.585, avg val loss=0.805, auc=0.565\n",
      "Iter=365, avg train loss=0.600, avg val loss=0.767, auc=0.577\n",
      "Iter=370, avg train loss=0.466, avg val loss=0.733, auc=0.595\n",
      "Iter=375, avg train loss=0.534, avg val loss=0.684, auc=0.583\n",
      "Iter=380, avg train loss=0.634, avg val loss=0.825, auc=0.593\n",
      "Iter=385, avg train loss=0.528, avg val loss=0.841, auc=0.584\n",
      "Iter=390, avg train loss=0.667, avg val loss=0.797, auc=0.578\n",
      "Iter=395, avg train loss=0.567, avg val loss=0.771, auc=0.580\n",
      "Iter=400, avg train loss=0.688, avg val loss=0.796, auc=0.578\n",
      "Iter=405, avg train loss=0.468, avg val loss=0.750, auc=0.597\n",
      "Iter=410, avg train loss=0.599, avg val loss=0.716, auc=0.615\n",
      "Iter=415, avg train loss=0.569, avg val loss=0.676, auc=0.600\n",
      "Iter=420, avg train loss=0.588, avg val loss=0.684, auc=0.598\n",
      "Iter=425, avg train loss=0.618, avg val loss=0.702, auc=0.592\n",
      "Iter=430, avg train loss=0.470, avg val loss=0.702, auc=0.584\n",
      "Iter=435, avg train loss=0.622, avg val loss=0.725, auc=0.582\n",
      "Iter=440, avg train loss=0.556, avg val loss=0.745, auc=0.578\n",
      "Iter=445, avg train loss=0.538, avg val loss=0.759, auc=0.571\n",
      "Iter=450, avg train loss=0.590, avg val loss=0.763, auc=0.557\n",
      "Iter=455, avg train loss=0.519, avg val loss=0.822, auc=0.557\n",
      "Iter=460, avg train loss=0.530, avg val loss=0.828, auc=0.544\n",
      "Iter=465, avg train loss=0.571, avg val loss=0.814, auc=0.537\n",
      "Iter=470, avg train loss=0.480, avg val loss=0.805, auc=0.540\n",
      "Iter=475, avg train loss=0.582, avg val loss=0.803, auc=0.534\n",
      "Iter=480, avg train loss=0.584, avg val loss=0.782, auc=0.555\n",
      "Iter=485, avg train loss=0.611, avg val loss=0.753, auc=0.541\n",
      "Iter=490, avg train loss=0.441, avg val loss=0.760, auc=0.536\n",
      "Iter=495, avg train loss=0.526, avg val loss=0.760, auc=0.547\n",
      "Iter=500, avg train loss=0.541, avg val loss=0.733, auc=0.545\n",
      "Iter=505, avg train loss=0.545, avg val loss=0.785, auc=0.544\n",
      "Iter=510, avg train loss=0.534, avg val loss=0.811, auc=0.537\n",
      "Iter=515, avg train loss=0.583, avg val loss=0.806, auc=0.544\n",
      "Iter=520, avg train loss=0.509, avg val loss=0.786, auc=0.555\n",
      "Iter=525, avg train loss=0.375, avg val loss=0.795, auc=0.555\n",
      "Iter=530, avg train loss=0.499, avg val loss=0.776, auc=0.560\n",
      "Iter=535, avg train loss=0.576, avg val loss=0.734, auc=0.566\n",
      "Iter=540, avg train loss=0.491, avg val loss=0.689, auc=0.580\n",
      "Iter=545, avg train loss=0.501, avg val loss=0.724, auc=0.552\n",
      "Iter=550, avg train loss=0.476, avg val loss=0.756, auc=0.552\n",
      "Iter=555, avg train loss=0.488, avg val loss=0.774, auc=0.534\n",
      "Iter=560, avg train loss=0.496, avg val loss=0.776, auc=0.515\n",
      "Iter=565, avg train loss=0.452, avg val loss=0.831, auc=0.511\n",
      "Iter=570, avg train loss=0.496, avg val loss=0.890, auc=0.515\n",
      "Iter=575, avg train loss=0.653, avg val loss=0.836, auc=0.537\n",
      "Iter=580, avg train loss=0.620, avg val loss=0.886, auc=0.552\n",
      "Iter=585, avg train loss=0.447, avg val loss=0.789, auc=0.569\n",
      "Iter=590, avg train loss=0.501, avg val loss=0.811, auc=0.558\n",
      "Iter=595, avg train loss=0.459, avg val loss=0.863, auc=0.547\n",
      "Iter=600, avg train loss=0.596, avg val loss=0.768, auc=0.565\n",
      "Iter=605, avg train loss=0.504, avg val loss=0.807, auc=0.530\n",
      "Iter=610, avg train loss=0.402, avg val loss=0.777, auc=0.541\n",
      "Iter=615, avg train loss=0.447, avg val loss=0.814, auc=0.524\n",
      "Iter=620, avg train loss=0.967, avg val loss=0.842, auc=0.509\n",
      "Iter=625, avg train loss=0.519, avg val loss=1.040, auc=0.516\n",
      "Iter=630, avg train loss=0.497, avg val loss=1.178, auc=0.506\n",
      "Iter=635, avg train loss=0.486, avg val loss=1.069, auc=0.512\n",
      "Iter=640, avg train loss=0.491, avg val loss=0.936, auc=0.522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=645, avg train loss=0.438, avg val loss=0.808, auc=0.532\n",
      "Iter=650, avg train loss=0.590, avg val loss=0.816, auc=0.526\n",
      "Iter=655, avg train loss=0.491, avg val loss=0.813, auc=0.529\n",
      "Iter=660, avg train loss=0.581, avg val loss=0.805, auc=0.542\n",
      "Iter=665, avg train loss=0.463, avg val loss=0.758, auc=0.548\n",
      "Iter=670, avg train loss=0.441, avg val loss=0.725, auc=0.559\n",
      "Iter=675, avg train loss=0.422, avg val loss=0.691, auc=0.540\n",
      "Iter=680, avg train loss=0.411, avg val loss=0.715, auc=0.549\n",
      "Iter=685, avg train loss=0.450, avg val loss=0.750, auc=0.527\n",
      "Iter=690, avg train loss=0.429, avg val loss=0.753, auc=0.530\n",
      "Iter=695, avg train loss=0.425, avg val loss=0.739, auc=0.529\n",
      "Iter=700, avg train loss=0.521, avg val loss=0.768, auc=0.540\n",
      "Iter=705, avg train loss=0.488, avg val loss=0.720, auc=0.530\n",
      "Iter=710, avg train loss=0.335, avg val loss=0.679, auc=0.541\n",
      "Iter=715, avg train loss=0.516, avg val loss=0.684, auc=0.549\n",
      "Iter=720, avg train loss=0.464, avg val loss=0.720, auc=0.538\n",
      "Iter=725, avg train loss=0.451, avg val loss=0.796, auc=0.545\n",
      "Iter=730, avg train loss=0.447, avg val loss=0.775, auc=0.541\n",
      "Iter=735, avg train loss=0.436, avg val loss=0.762, auc=0.541\n",
      "Iter=740, avg train loss=0.511, avg val loss=0.754, auc=0.539\n",
      "Iter=745, avg train loss=0.562, avg val loss=0.734, auc=0.534\n",
      "Iter=750, avg train loss=0.590, avg val loss=0.720, auc=0.546\n",
      "Iter=755, avg train loss=0.402, avg val loss=0.714, auc=0.561\n",
      "Iter=760, avg train loss=0.325, avg val loss=0.715, auc=0.562\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.605, max-score-based AUC=0.655\n",
      "\n",
      "\n",
      "========== Fold 2 ==========\n",
      "Test AUC at start=0.418, max-score-based AUC=0.387\n",
      "Iter=5, avg train loss=0.887, avg val loss=0.601, auc=0.604\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.950, avg val loss=0.580, auc=0.559\n",
      "Iter=15, avg train loss=0.860, avg val loss=0.576, auc=0.554\n",
      "Iter=20, avg train loss=0.857, avg val loss=0.565, auc=0.574\n",
      "Iter=25, avg train loss=0.830, avg val loss=0.568, auc=0.566\n",
      "Iter=30, avg train loss=0.822, avg val loss=0.575, auc=0.562\n",
      "Iter=35, avg train loss=0.932, avg val loss=0.571, auc=0.565\n",
      "Iter=40, avg train loss=0.810, avg val loss=0.570, auc=0.568\n",
      "Iter=45, avg train loss=0.668, avg val loss=0.602, auc=0.545\n",
      "Iter=50, avg train loss=0.704, avg val loss=0.620, auc=0.538\n",
      "Iter=55, avg train loss=0.771, avg val loss=0.601, auc=0.533\n",
      "Iter=60, avg train loss=0.699, avg val loss=0.613, auc=0.536\n",
      "Iter=65, avg train loss=0.733, avg val loss=0.637, auc=0.531\n",
      "Iter=70, avg train loss=0.694, avg val loss=0.638, auc=0.542\n",
      "Iter=75, avg train loss=0.664, avg val loss=0.639, auc=0.544\n",
      "Iter=80, avg train loss=0.650, avg val loss=0.639, auc=0.543\n",
      "Iter=85, avg train loss=0.717, avg val loss=0.619, auc=0.552\n",
      "Iter=90, avg train loss=0.627, avg val loss=0.641, auc=0.538\n",
      "Iter=95, avg train loss=0.732, avg val loss=0.635, auc=0.553\n",
      "Iter=100, avg train loss=0.623, avg val loss=0.618, auc=0.552\n",
      "Iter=105, avg train loss=0.689, avg val loss=0.595, auc=0.572\n",
      "Iter=110, avg train loss=0.708, avg val loss=0.600, auc=0.556\n",
      "Iter=115, avg train loss=0.737, avg val loss=0.602, auc=0.548\n",
      "Iter=120, avg train loss=0.735, avg val loss=0.625, auc=0.553\n",
      "Iter=125, avg train loss=0.718, avg val loss=0.644, auc=0.558\n",
      "Iter=130, avg train loss=0.671, avg val loss=0.656, auc=0.559\n",
      "Iter=135, avg train loss=0.667, avg val loss=0.666, auc=0.554\n",
      "Iter=140, avg train loss=0.719, avg val loss=0.638, auc=0.564\n",
      "Iter=145, avg train loss=0.665, avg val loss=0.649, auc=0.555\n",
      "Iter=150, avg train loss=0.669, avg val loss=0.658, auc=0.564\n",
      "Iter=155, avg train loss=0.702, avg val loss=0.664, auc=0.578\n",
      "Iter=160, avg train loss=0.645, avg val loss=0.681, auc=0.578\n",
      "Iter=165, avg train loss=0.668, avg val loss=0.657, auc=0.590\n",
      "Iter=170, avg train loss=0.683, avg val loss=0.663, auc=0.585\n",
      "Iter=175, avg train loss=0.657, avg val loss=0.623, auc=0.599\n",
      "Iter=180, avg train loss=0.659, avg val loss=0.632, auc=0.603\n",
      "Iter=185, avg train loss=0.660, avg val loss=0.585, auc=0.625\n",
      "Best model saved.\n",
      "Iter=190, avg train loss=0.647, avg val loss=0.587, auc=0.613\n",
      "Iter=195, avg train loss=0.705, avg val loss=0.597, auc=0.607\n",
      "Iter=200, avg train loss=0.668, avg val loss=0.620, auc=0.596\n",
      "Iter=205, avg train loss=0.642, avg val loss=0.652, auc=0.584\n",
      "Iter=210, avg train loss=0.655, avg val loss=0.647, auc=0.614\n",
      "Iter=215, avg train loss=0.623, avg val loss=0.655, auc=0.606\n",
      "Iter=220, avg train loss=0.637, avg val loss=0.652, auc=0.615\n",
      "Iter=225, avg train loss=0.594, avg val loss=0.667, auc=0.600\n",
      "Iter=230, avg train loss=0.654, avg val loss=0.606, auc=0.608\n",
      "Iter=235, avg train loss=0.597, avg val loss=0.605, auc=0.606\n",
      "Iter=240, avg train loss=0.617, avg val loss=0.627, auc=0.573\n",
      "Iter=245, avg train loss=0.686, avg val loss=0.607, auc=0.588\n",
      "Iter=250, avg train loss=0.731, avg val loss=0.599, auc=0.596\n",
      "Iter=255, avg train loss=0.640, avg val loss=0.599, auc=0.592\n",
      "Iter=260, avg train loss=0.639, avg val loss=0.611, auc=0.613\n",
      "Iter=265, avg train loss=0.594, avg val loss=0.642, auc=0.603\n",
      "Iter=270, avg train loss=0.643, avg val loss=0.656, auc=0.619\n",
      "Iter=275, avg train loss=0.657, avg val loss=0.589, auc=0.643\n",
      "Best model saved.\n",
      "Iter=280, avg train loss=0.686, avg val loss=0.583, auc=0.640\n",
      "Iter=285, avg train loss=0.650, avg val loss=0.596, auc=0.637\n",
      "Iter=290, avg train loss=0.648, avg val loss=0.612, auc=0.630\n",
      "Iter=295, avg train loss=0.605, avg val loss=0.600, auc=0.630\n",
      "Iter=300, avg train loss=0.588, avg val loss=0.554, auc=0.646\n",
      "Best model saved.\n",
      "Iter=305, avg train loss=0.669, avg val loss=0.543, auc=0.649\n",
      "Best model saved.\n",
      "Iter=310, avg train loss=0.683, avg val loss=0.579, auc=0.625\n",
      "Iter=315, avg train loss=0.625, avg val loss=0.614, auc=0.625\n",
      "Iter=320, avg train loss=0.698, avg val loss=0.611, auc=0.604\n",
      "Iter=325, avg train loss=0.531, avg val loss=0.663, auc=0.610\n",
      "Iter=330, avg train loss=0.624, avg val loss=0.692, auc=0.615\n",
      "Iter=335, avg train loss=0.577, avg val loss=0.644, auc=0.624\n",
      "Iter=340, avg train loss=0.612, avg val loss=0.642, auc=0.611\n",
      "Iter=345, avg train loss=0.630, avg val loss=0.617, auc=0.594\n",
      "Iter=350, avg train loss=0.658, avg val loss=0.661, auc=0.607\n",
      "Iter=355, avg train loss=0.604, avg val loss=0.632, auc=0.615\n",
      "Iter=360, avg train loss=0.526, avg val loss=0.644, auc=0.623\n",
      "Iter=365, avg train loss=0.591, avg val loss=0.616, auc=0.633\n",
      "Iter=370, avg train loss=0.582, avg val loss=0.631, auc=0.647\n",
      "Iter=375, avg train loss=0.635, avg val loss=0.594, auc=0.649\n",
      "Iter=380, avg train loss=0.602, avg val loss=0.596, auc=0.651\n",
      "Best model saved.\n",
      "Iter=385, avg train loss=0.630, avg val loss=0.618, auc=0.630\n",
      "Iter=390, avg train loss=0.727, avg val loss=0.630, auc=0.642\n",
      "Iter=395, avg train loss=0.613, avg val loss=0.677, auc=0.644\n",
      "Iter=400, avg train loss=0.698, avg val loss=0.712, auc=0.631\n",
      "Iter=405, avg train loss=0.625, avg val loss=0.702, auc=0.614\n",
      "Iter=410, avg train loss=0.736, avg val loss=0.704, auc=0.617\n",
      "Iter=415, avg train loss=0.511, avg val loss=0.684, auc=0.613\n",
      "Iter=420, avg train loss=0.602, avg val loss=0.692, auc=0.628\n",
      "Iter=425, avg train loss=0.593, avg val loss=0.652, auc=0.619\n",
      "Iter=430, avg train loss=0.649, avg val loss=0.668, auc=0.618\n",
      "Iter=435, avg train loss=0.611, avg val loss=0.595, auc=0.635\n",
      "Iter=440, avg train loss=0.556, avg val loss=0.573, auc=0.637\n",
      "Iter=445, avg train loss=0.592, avg val loss=0.573, auc=0.633\n",
      "Iter=450, avg train loss=0.611, avg val loss=0.568, auc=0.629\n",
      "Iter=455, avg train loss=0.482, avg val loss=0.645, auc=0.619\n",
      "Iter=460, avg train loss=0.527, avg val loss=0.617, auc=0.638\n",
      "Iter=465, avg train loss=0.533, avg val loss=0.612, auc=0.625\n",
      "Iter=470, avg train loss=0.628, avg val loss=0.579, auc=0.633\n",
      "Iter=475, avg train loss=0.623, avg val loss=0.613, auc=0.643\n",
      "Iter=480, avg train loss=0.621, avg val loss=0.666, auc=0.629\n",
      "Iter=485, avg train loss=0.589, avg val loss=0.643, auc=0.621\n",
      "Iter=490, avg train loss=0.616, avg val loss=0.618, auc=0.630\n",
      "Iter=495, avg train loss=0.576, avg val loss=0.655, auc=0.623\n",
      "Iter=500, avg train loss=0.603, avg val loss=0.628, auc=0.629\n",
      "Iter=505, avg train loss=0.628, avg val loss=0.544, auc=0.650\n",
      "Iter=510, avg train loss=0.577, avg val loss=0.567, auc=0.638\n",
      "Iter=515, avg train loss=0.641, avg val loss=0.576, auc=0.637\n",
      "Iter=520, avg train loss=0.447, avg val loss=0.589, auc=0.652\n",
      "Best model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=525, avg train loss=0.467, avg val loss=0.582, auc=0.653\n",
      "Best model saved.\n",
      "Iter=530, avg train loss=0.590, avg val loss=0.583, auc=0.638\n",
      "Iter=535, avg train loss=0.513, avg val loss=0.596, auc=0.628\n",
      "Iter=540, avg train loss=0.616, avg val loss=0.682, auc=0.615\n",
      "Iter=545, avg train loss=0.479, avg val loss=0.714, auc=0.612\n",
      "Iter=550, avg train loss=0.569, avg val loss=0.663, auc=0.618\n",
      "Iter=555, avg train loss=0.480, avg val loss=0.617, auc=0.636\n",
      "Iter=560, avg train loss=0.637, avg val loss=0.651, auc=0.631\n",
      "Iter=565, avg train loss=0.467, avg val loss=0.617, auc=0.624\n",
      "Iter=570, avg train loss=0.576, avg val loss=0.633, auc=0.601\n",
      "Iter=575, avg train loss=0.573, avg val loss=0.689, auc=0.601\n",
      "Iter=580, avg train loss=0.553, avg val loss=0.666, auc=0.585\n",
      "Iter=585, avg train loss=0.573, avg val loss=0.667, auc=0.580\n",
      "Iter=590, avg train loss=0.530, avg val loss=0.655, auc=0.584\n",
      "Iter=595, avg train loss=0.500, avg val loss=0.615, auc=0.609\n",
      "Iter=600, avg train loss=0.562, avg val loss=0.569, auc=0.624\n",
      "Iter=605, avg train loss=0.473, avg val loss=0.548, auc=0.660\n",
      "Best model saved.\n",
      "Iter=610, avg train loss=0.549, avg val loss=0.566, auc=0.624\n",
      "Iter=615, avg train loss=0.638, avg val loss=0.585, auc=0.633\n",
      "Iter=620, avg train loss=0.543, avg val loss=0.664, auc=0.622\n",
      "Iter=625, avg train loss=0.647, avg val loss=0.711, auc=0.591\n",
      "Iter=630, avg train loss=0.599, avg val loss=0.613, auc=0.619\n",
      "Iter=635, avg train loss=0.461, avg val loss=0.623, auc=0.605\n",
      "Iter=640, avg train loss=0.557, avg val loss=0.661, auc=0.608\n",
      "Iter=645, avg train loss=0.496, avg val loss=0.588, auc=0.624\n",
      "Iter=650, avg train loss=0.421, avg val loss=0.565, auc=0.615\n",
      "Iter=655, avg train loss=0.678, avg val loss=0.572, auc=0.631\n",
      "Iter=660, avg train loss=0.598, avg val loss=0.583, auc=0.606\n",
      "Iter=665, avg train loss=0.553, avg val loss=0.608, auc=0.597\n",
      "Iter=670, avg train loss=0.617, avg val loss=0.630, auc=0.607\n",
      "Iter=675, avg train loss=0.432, avg val loss=0.669, auc=0.609\n",
      "Iter=680, avg train loss=0.458, avg val loss=0.639, auc=0.613\n",
      "Iter=685, avg train loss=0.487, avg val loss=0.643, auc=0.617\n",
      "Iter=690, avg train loss=0.460, avg val loss=0.635, auc=0.628\n",
      "Iter=695, avg train loss=0.372, avg val loss=0.608, auc=0.637\n",
      "Iter=700, avg train loss=0.609, avg val loss=0.609, auc=0.667\n",
      "Best model saved.\n",
      "Iter=705, avg train loss=0.489, avg val loss=0.627, auc=0.663\n",
      "Iter=710, avg train loss=0.552, avg val loss=0.691, auc=0.646\n",
      "Iter=715, avg train loss=0.498, avg val loss=0.594, auc=0.649\n",
      "Iter=720, avg train loss=0.514, avg val loss=0.687, auc=0.619\n",
      "Iter=725, avg train loss=0.497, avg val loss=0.623, auc=0.622\n",
      "Iter=730, avg train loss=0.434, avg val loss=0.645, auc=0.633\n",
      "Iter=735, avg train loss=0.438, avg val loss=0.654, auc=0.624\n",
      "Iter=740, avg train loss=0.548, avg val loss=0.621, auc=0.621\n",
      "Iter=745, avg train loss=0.518, avg val loss=0.597, auc=0.622\n",
      "Iter=750, avg train loss=0.541, avg val loss=0.657, auc=0.600\n",
      "Iter=755, avg train loss=0.507, avg val loss=0.680, auc=0.579\n",
      "Iter=760, avg train loss=0.630, avg val loss=0.637, auc=0.588\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.597, max-score-based AUC=0.549\n",
      "\n",
      "\n",
      "========== Fold 3 ==========\n",
      "Test AUC at start=0.506, max-score-based AUC=0.506\n",
      "Iter=5, avg train loss=0.955, avg val loss=0.606, auc=0.605\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=1.012, avg val loss=0.567, auc=0.578\n",
      "Iter=15, avg train loss=0.797, avg val loss=0.576, auc=0.536\n",
      "Iter=20, avg train loss=0.909, avg val loss=0.572, auc=0.542\n",
      "Iter=25, avg train loss=1.059, avg val loss=0.588, auc=0.536\n",
      "Iter=30, avg train loss=0.780, avg val loss=0.592, auc=0.550\n",
      "Iter=35, avg train loss=0.889, avg val loss=0.578, auc=0.576\n",
      "Iter=40, avg train loss=0.824, avg val loss=0.594, auc=0.566\n",
      "Iter=45, avg train loss=0.889, avg val loss=0.601, auc=0.560\n",
      "Iter=50, avg train loss=0.751, avg val loss=0.612, auc=0.551\n",
      "Iter=55, avg train loss=0.820, avg val loss=0.623, auc=0.555\n",
      "Iter=60, avg train loss=0.677, avg val loss=0.591, auc=0.562\n",
      "Iter=65, avg train loss=0.683, avg val loss=0.650, auc=0.525\n",
      "Iter=70, avg train loss=0.734, avg val loss=0.666, auc=0.514\n",
      "Iter=75, avg train loss=0.700, avg val loss=0.662, auc=0.504\n",
      "Iter=80, avg train loss=0.690, avg val loss=0.658, auc=0.503\n",
      "Iter=85, avg train loss=0.660, avg val loss=0.616, auc=0.506\n",
      "Iter=90, avg train loss=0.681, avg val loss=0.621, auc=0.508\n",
      "Iter=95, avg train loss=0.662, avg val loss=0.620, auc=0.499\n",
      "Iter=100, avg train loss=0.711, avg val loss=0.628, auc=0.498\n",
      "Iter=105, avg train loss=0.629, avg val loss=0.616, auc=0.518\n",
      "Iter=110, avg train loss=0.699, avg val loss=0.603, auc=0.530\n",
      "Iter=115, avg train loss=0.704, avg val loss=0.620, auc=0.496\n",
      "Iter=120, avg train loss=0.723, avg val loss=0.617, auc=0.519\n",
      "Iter=125, avg train loss=0.600, avg val loss=0.616, auc=0.525\n",
      "Iter=130, avg train loss=0.710, avg val loss=0.591, auc=0.539\n",
      "Iter=135, avg train loss=0.694, avg val loss=0.593, auc=0.571\n",
      "Iter=140, avg train loss=0.675, avg val loss=0.615, auc=0.566\n",
      "Iter=145, avg train loss=0.726, avg val loss=0.591, auc=0.598\n",
      "Iter=150, avg train loss=0.673, avg val loss=0.600, auc=0.603\n",
      "Iter=155, avg train loss=0.628, avg val loss=0.626, auc=0.589\n",
      "Iter=160, avg train loss=0.716, avg val loss=0.654, auc=0.580\n",
      "Iter=165, avg train loss=0.670, avg val loss=0.622, auc=0.628\n",
      "Best model saved.\n",
      "Iter=170, avg train loss=0.680, avg val loss=0.649, auc=0.601\n",
      "Iter=175, avg train loss=0.661, avg val loss=0.624, auc=0.615\n",
      "Iter=180, avg train loss=0.641, avg val loss=0.638, auc=0.600\n",
      "Iter=185, avg train loss=0.665, avg val loss=0.644, auc=0.591\n",
      "Iter=190, avg train loss=0.651, avg val loss=0.614, auc=0.615\n",
      "Iter=195, avg train loss=0.657, avg val loss=0.621, auc=0.625\n",
      "Iter=200, avg train loss=0.648, avg val loss=0.673, auc=0.587\n",
      "Iter=205, avg train loss=0.650, avg val loss=0.652, auc=0.608\n",
      "Iter=210, avg train loss=0.691, avg val loss=0.639, auc=0.625\n",
      "Iter=215, avg train loss=0.672, avg val loss=0.627, auc=0.614\n",
      "Iter=220, avg train loss=0.621, avg val loss=0.608, auc=0.620\n",
      "Iter=225, avg train loss=0.613, avg val loss=0.592, auc=0.636\n",
      "Best model saved.\n",
      "Iter=230, avg train loss=0.642, avg val loss=0.585, auc=0.645\n",
      "Best model saved.\n",
      "Iter=235, avg train loss=0.596, avg val loss=0.588, auc=0.654\n",
      "Best model saved.\n",
      "Iter=240, avg train loss=0.682, avg val loss=0.600, auc=0.665\n",
      "Best model saved.\n",
      "Iter=245, avg train loss=0.587, avg val loss=0.627, auc=0.649\n",
      "Iter=250, avg train loss=0.687, avg val loss=0.643, auc=0.630\n",
      "Iter=255, avg train loss=0.672, avg val loss=0.672, auc=0.612\n",
      "Iter=260, avg train loss=0.629, avg val loss=0.620, auc=0.632\n",
      "Iter=265, avg train loss=0.582, avg val loss=0.620, auc=0.635\n",
      "Iter=270, avg train loss=0.520, avg val loss=0.609, auc=0.654\n",
      "Iter=275, avg train loss=0.684, avg val loss=0.646, auc=0.646\n",
      "Iter=280, avg train loss=0.642, avg val loss=0.607, auc=0.658\n",
      "Iter=285, avg train loss=0.689, avg val loss=0.588, auc=0.622\n",
      "Iter=290, avg train loss=0.571, avg val loss=0.578, auc=0.611\n",
      "Iter=295, avg train loss=0.617, avg val loss=0.590, auc=0.598\n",
      "Iter=300, avg train loss=0.647, avg val loss=0.651, auc=0.539\n",
      "Iter=305, avg train loss=0.579, avg val loss=0.682, auc=0.509\n",
      "Iter=310, avg train loss=0.611, avg val loss=0.643, auc=0.520\n",
      "Iter=315, avg train loss=0.626, avg val loss=0.679, auc=0.487\n",
      "Iter=320, avg train loss=0.551, avg val loss=0.666, auc=0.512\n",
      "Iter=325, avg train loss=0.641, avg val loss=0.696, auc=0.489\n",
      "Iter=330, avg train loss=0.609, avg val loss=0.694, auc=0.474\n",
      "Iter=335, avg train loss=0.582, avg val loss=0.719, auc=0.460\n",
      "Iter=340, avg train loss=0.604, avg val loss=0.652, auc=0.507\n",
      "Iter=345, avg train loss=0.687, avg val loss=0.630, auc=0.541\n",
      "Iter=350, avg train loss=0.687, avg val loss=0.631, auc=0.546\n",
      "Iter=355, avg train loss=0.628, avg val loss=0.645, auc=0.557\n",
      "Iter=360, avg train loss=0.552, avg val loss=0.618, auc=0.577\n",
      "Iter=365, avg train loss=0.630, avg val loss=0.610, auc=0.589\n",
      "Iter=370, avg train loss=0.568, avg val loss=0.607, auc=0.590\n",
      "Iter=375, avg train loss=0.645, avg val loss=0.601, auc=0.583\n",
      "Iter=380, avg train loss=0.625, avg val loss=0.671, auc=0.547\n",
      "Iter=385, avg train loss=0.596, avg val loss=0.669, auc=0.520\n",
      "Iter=390, avg train loss=0.490, avg val loss=0.635, auc=0.519\n",
      "Iter=395, avg train loss=0.685, avg val loss=0.612, auc=0.537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=400, avg train loss=0.621, avg val loss=0.636, auc=0.526\n",
      "Iter=405, avg train loss=0.556, avg val loss=0.658, auc=0.504\n",
      "Iter=410, avg train loss=0.648, avg val loss=0.636, auc=0.521\n",
      "Iter=415, avg train loss=0.580, avg val loss=0.701, auc=0.482\n",
      "Iter=420, avg train loss=0.569, avg val loss=0.728, auc=0.514\n",
      "Iter=425, avg train loss=0.571, avg val loss=0.629, auc=0.558\n",
      "Iter=430, avg train loss=0.563, avg val loss=0.743, auc=0.497\n",
      "Iter=435, avg train loss=0.526, avg val loss=0.681, auc=0.528\n",
      "Iter=440, avg train loss=0.593, avg val loss=0.636, auc=0.536\n",
      "Iter=445, avg train loss=0.533, avg val loss=0.671, auc=0.509\n",
      "Iter=450, avg train loss=0.751, avg val loss=0.655, auc=0.500\n",
      "Iter=455, avg train loss=0.636, avg val loss=0.794, auc=0.463\n",
      "Iter=460, avg train loss=0.631, avg val loss=0.780, auc=0.460\n",
      "Iter=465, avg train loss=0.535, avg val loss=0.814, auc=0.492\n",
      "Iter=470, avg train loss=0.633, avg val loss=0.717, auc=0.498\n",
      "Iter=475, avg train loss=0.699, avg val loss=0.662, auc=0.538\n",
      "Iter=480, avg train loss=0.585, avg val loss=0.626, auc=0.547\n",
      "Iter=485, avg train loss=0.620, avg val loss=0.662, auc=0.558\n",
      "Iter=490, avg train loss=0.572, avg val loss=0.698, auc=0.564\n",
      "Iter=495, avg train loss=0.627, avg val loss=0.607, auc=0.572\n",
      "Iter=500, avg train loss=0.553, avg val loss=0.607, auc=0.579\n",
      "Iter=505, avg train loss=0.621, avg val loss=0.633, auc=0.565\n",
      "Iter=510, avg train loss=0.540, avg val loss=0.697, auc=0.552\n",
      "Iter=515, avg train loss=0.638, avg val loss=0.609, auc=0.583\n",
      "Iter=520, avg train loss=0.541, avg val loss=0.583, auc=0.602\n",
      "Iter=525, avg train loss=0.550, avg val loss=0.603, auc=0.609\n",
      "Iter=530, avg train loss=0.575, avg val loss=0.651, auc=0.609\n",
      "Iter=535, avg train loss=0.537, avg val loss=0.580, auc=0.619\n",
      "Iter=540, avg train loss=0.621, avg val loss=0.582, auc=0.613\n",
      "Iter=545, avg train loss=0.501, avg val loss=0.632, auc=0.607\n",
      "Iter=550, avg train loss=0.576, avg val loss=0.671, auc=0.589\n",
      "Iter=555, avg train loss=0.567, avg val loss=0.648, auc=0.584\n",
      "Iter=560, avg train loss=0.549, avg val loss=0.635, auc=0.566\n",
      "Iter=565, avg train loss=0.597, avg val loss=0.641, auc=0.554\n",
      "Iter=570, avg train loss=0.494, avg val loss=0.638, auc=0.547\n",
      "Iter=575, avg train loss=0.627, avg val loss=0.603, auc=0.569\n",
      "Iter=580, avg train loss=0.505, avg val loss=0.609, auc=0.580\n",
      "Iter=585, avg train loss=0.490, avg val loss=0.613, auc=0.580\n",
      "Iter=590, avg train loss=0.509, avg val loss=0.615, auc=0.589\n",
      "Iter=595, avg train loss=0.659, avg val loss=0.575, auc=0.622\n",
      "Iter=600, avg train loss=0.515, avg val loss=0.645, auc=0.573\n",
      "Iter=605, avg train loss=0.570, avg val loss=0.662, auc=0.557\n",
      "Iter=610, avg train loss=0.519, avg val loss=0.662, auc=0.558\n",
      "Iter=615, avg train loss=0.497, avg val loss=0.669, auc=0.570\n",
      "Iter=620, avg train loss=0.569, avg val loss=0.607, auc=0.590\n",
      "Iter=625, avg train loss=0.533, avg val loss=0.624, auc=0.551\n",
      "Iter=630, avg train loss=0.539, avg val loss=0.649, auc=0.532\n",
      "Iter=635, avg train loss=0.549, avg val loss=0.661, auc=0.524\n",
      "Iter=640, avg train loss=0.447, avg val loss=0.647, auc=0.549\n",
      "Iter=645, avg train loss=0.495, avg val loss=0.694, auc=0.542\n",
      "Iter=650, avg train loss=0.529, avg val loss=0.637, auc=0.571\n",
      "Iter=655, avg train loss=0.479, avg val loss=0.610, auc=0.587\n",
      "Iter=660, avg train loss=0.487, avg val loss=0.661, auc=0.581\n",
      "Iter=665, avg train loss=0.530, avg val loss=0.703, auc=0.587\n",
      "Iter=670, avg train loss=0.532, avg val loss=0.613, auc=0.596\n",
      "Iter=675, avg train loss=0.434, avg val loss=0.597, auc=0.602\n",
      "Iter=680, avg train loss=0.477, avg val loss=0.609, auc=0.590\n",
      "Iter=685, avg train loss=0.404, avg val loss=0.652, auc=0.565\n",
      "Iter=690, avg train loss=0.476, avg val loss=0.674, auc=0.541\n",
      "Iter=695, avg train loss=0.500, avg val loss=0.674, auc=0.545\n",
      "Iter=700, avg train loss=0.540, avg val loss=0.669, auc=0.561\n",
      "Iter=705, avg train loss=0.494, avg val loss=0.682, auc=0.557\n",
      "Iter=710, avg train loss=0.588, avg val loss=0.655, auc=0.564\n",
      "Iter=715, avg train loss=0.476, avg val loss=0.647, auc=0.559\n",
      "Iter=720, avg train loss=0.412, avg val loss=0.636, auc=0.553\n",
      "Iter=725, avg train loss=0.438, avg val loss=0.625, auc=0.579\n",
      "Iter=730, avg train loss=0.460, avg val loss=0.643, auc=0.557\n",
      "Iter=735, avg train loss=0.500, avg val loss=0.661, auc=0.539\n",
      "Iter=740, avg train loss=0.498, avg val loss=0.683, auc=0.557\n",
      "Iter=745, avg train loss=0.455, avg val loss=0.673, auc=0.566\n",
      "Iter=750, avg train loss=0.587, avg val loss=0.642, auc=0.589\n",
      "Iter=755, avg train loss=0.445, avg val loss=0.713, auc=0.577\n",
      "Iter=760, avg train loss=0.494, avg val loss=0.678, auc=0.573\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.653, max-score-based AUC=0.626\n",
      "\n",
      "\n",
      "========== Fold 4 ==========\n",
      "Test AUC at start=0.561, max-score-based AUC=0.502\n",
      "Iter=5, avg train loss=1.017, avg val loss=0.632, auc=0.479\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=1.151, avg val loss=0.603, auc=0.491\n",
      "Best model saved.\n",
      "Iter=15, avg train loss=0.798, avg val loss=0.597, auc=0.492\n",
      "Best model saved.\n",
      "Iter=20, avg train loss=0.878, avg val loss=0.590, auc=0.513\n",
      "Best model saved.\n",
      "Iter=25, avg train loss=0.781, avg val loss=0.596, auc=0.504\n",
      "Iter=30, avg train loss=0.820, avg val loss=0.579, auc=0.530\n",
      "Best model saved.\n",
      "Iter=35, avg train loss=0.798, avg val loss=0.581, auc=0.542\n",
      "Best model saved.\n",
      "Iter=40, avg train loss=0.748, avg val loss=0.580, auc=0.542\n",
      "Best model saved.\n",
      "Iter=45, avg train loss=0.996, avg val loss=0.607, auc=0.523\n",
      "Iter=50, avg train loss=0.620, avg val loss=0.605, auc=0.545\n",
      "Best model saved.\n",
      "Iter=55, avg train loss=0.753, avg val loss=0.608, auc=0.530\n",
      "Iter=60, avg train loss=0.675, avg val loss=0.639, auc=0.524\n",
      "Iter=65, avg train loss=0.584, avg val loss=0.655, auc=0.526\n",
      "Iter=70, avg train loss=0.642, avg val loss=0.639, auc=0.523\n",
      "Iter=75, avg train loss=0.752, avg val loss=0.662, auc=0.524\n",
      "Iter=80, avg train loss=0.720, avg val loss=0.666, auc=0.526\n",
      "Iter=85, avg train loss=0.580, avg val loss=0.634, auc=0.538\n",
      "Iter=90, avg train loss=0.637, avg val loss=0.661, auc=0.531\n",
      "Iter=95, avg train loss=0.765, avg val loss=0.665, auc=0.533\n",
      "Iter=100, avg train loss=0.695, avg val loss=0.672, auc=0.532\n",
      "Iter=105, avg train loss=0.653, avg val loss=0.673, auc=0.544\n",
      "Iter=110, avg train loss=0.673, avg val loss=0.669, auc=0.542\n",
      "Iter=115, avg train loss=0.668, avg val loss=0.657, auc=0.544\n",
      "Iter=120, avg train loss=0.646, avg val loss=0.668, auc=0.535\n",
      "Iter=125, avg train loss=0.654, avg val loss=0.636, auc=0.550\n",
      "Best model saved.\n",
      "Iter=130, avg train loss=0.564, avg val loss=0.651, auc=0.545\n",
      "Iter=135, avg train loss=0.675, avg val loss=0.640, auc=0.526\n",
      "Iter=140, avg train loss=0.680, avg val loss=0.628, auc=0.539\n",
      "Iter=145, avg train loss=0.732, avg val loss=0.637, auc=0.540\n",
      "Iter=150, avg train loss=0.676, avg val loss=0.672, auc=0.530\n",
      "Iter=155, avg train loss=0.617, avg val loss=0.681, auc=0.525\n",
      "Iter=160, avg train loss=0.648, avg val loss=0.650, auc=0.538\n",
      "Iter=165, avg train loss=0.597, avg val loss=0.650, auc=0.535\n",
      "Iter=170, avg train loss=0.692, avg val loss=0.662, auc=0.523\n",
      "Iter=175, avg train loss=0.561, avg val loss=0.663, auc=0.521\n",
      "Iter=180, avg train loss=0.634, avg val loss=0.653, auc=0.525\n",
      "Iter=185, avg train loss=0.671, avg val loss=0.655, auc=0.528\n",
      "Iter=190, avg train loss=0.597, avg val loss=0.655, auc=0.534\n",
      "Iter=195, avg train loss=0.670, avg val loss=0.674, auc=0.546\n",
      "Iter=200, avg train loss=0.650, avg val loss=0.654, auc=0.571\n",
      "Best model saved.\n",
      "Iter=205, avg train loss=0.629, avg val loss=0.669, auc=0.563\n",
      "Iter=210, avg train loss=0.621, avg val loss=0.653, auc=0.558\n",
      "Iter=215, avg train loss=0.574, avg val loss=0.656, auc=0.567\n",
      "Iter=220, avg train loss=0.689, avg val loss=0.641, auc=0.572\n",
      "Best model saved.\n",
      "Iter=225, avg train loss=0.604, avg val loss=0.675, auc=0.557\n",
      "Iter=230, avg train loss=0.716, avg val loss=0.673, auc=0.560\n",
      "Iter=235, avg train loss=0.623, avg val loss=0.675, auc=0.562\n",
      "Iter=240, avg train loss=0.627, avg val loss=0.695, auc=0.554\n",
      "Iter=245, avg train loss=0.636, avg val loss=0.668, auc=0.565\n",
      "Iter=250, avg train loss=0.582, avg val loss=0.668, auc=0.537\n",
      "Iter=255, avg train loss=0.667, avg val loss=0.676, auc=0.532\n",
      "Iter=260, avg train loss=0.710, avg val loss=0.683, auc=0.542\n",
      "Iter=265, avg train loss=0.578, avg val loss=0.699, auc=0.541\n",
      "Iter=270, avg train loss=0.631, avg val loss=0.694, auc=0.532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=275, avg train loss=0.642, avg val loss=0.685, auc=0.539\n",
      "Iter=280, avg train loss=0.556, avg val loss=0.688, auc=0.548\n",
      "Iter=285, avg train loss=0.565, avg val loss=0.675, auc=0.551\n",
      "Iter=290, avg train loss=0.619, avg val loss=0.677, auc=0.553\n",
      "Iter=295, avg train loss=0.637, avg val loss=0.701, auc=0.542\n",
      "Iter=300, avg train loss=0.524, avg val loss=0.713, auc=0.550\n",
      "Iter=305, avg train loss=0.664, avg val loss=0.710, auc=0.547\n",
      "Iter=310, avg train loss=0.558, avg val loss=0.696, auc=0.539\n",
      "Iter=315, avg train loss=0.555, avg val loss=0.786, auc=0.569\n",
      "Iter=320, avg train loss=0.503, avg val loss=0.713, auc=0.558\n",
      "Iter=325, avg train loss=0.559, avg val loss=0.724, auc=0.540\n",
      "Iter=330, avg train loss=0.562, avg val loss=0.728, auc=0.568\n",
      "Iter=335, avg train loss=0.476, avg val loss=0.698, auc=0.555\n",
      "Iter=340, avg train loss=0.642, avg val loss=0.694, auc=0.563\n",
      "Iter=345, avg train loss=0.626, avg val loss=0.715, auc=0.548\n",
      "Iter=350, avg train loss=0.582, avg val loss=0.722, auc=0.548\n",
      "Iter=355, avg train loss=0.639, avg val loss=0.702, auc=0.538\n",
      "Iter=360, avg train loss=0.599, avg val loss=0.719, auc=0.547\n",
      "Iter=365, avg train loss=0.591, avg val loss=0.729, auc=0.542\n",
      "Iter=370, avg train loss=0.597, avg val loss=0.745, auc=0.571\n",
      "Iter=375, avg train loss=0.627, avg val loss=0.747, auc=0.576\n",
      "Best model saved.\n",
      "Iter=380, avg train loss=0.535, avg val loss=0.741, auc=0.578\n",
      "Best model saved.\n",
      "Iter=385, avg train loss=0.532, avg val loss=0.709, auc=0.569\n",
      "Iter=390, avg train loss=0.508, avg val loss=0.772, auc=0.601\n",
      "Best model saved.\n",
      "Iter=395, avg train loss=0.560, avg val loss=0.766, auc=0.592\n",
      "Iter=400, avg train loss=0.547, avg val loss=0.739, auc=0.576\n",
      "Iter=405, avg train loss=0.587, avg val loss=0.698, auc=0.583\n",
      "Iter=410, avg train loss=0.535, avg val loss=0.715, auc=0.571\n",
      "Iter=415, avg train loss=0.638, avg val loss=0.683, auc=0.571\n",
      "Iter=420, avg train loss=0.526, avg val loss=0.685, auc=0.569\n",
      "Iter=425, avg train loss=0.459, avg val loss=0.676, auc=0.573\n",
      "Iter=430, avg train loss=0.520, avg val loss=0.668, auc=0.574\n",
      "Iter=435, avg train loss=0.655, avg val loss=0.658, auc=0.561\n",
      "Iter=440, avg train loss=0.540, avg val loss=0.673, auc=0.560\n",
      "Iter=445, avg train loss=0.592, avg val loss=0.709, auc=0.566\n",
      "Iter=450, avg train loss=0.529, avg val loss=0.738, auc=0.577\n",
      "Iter=455, avg train loss=0.512, avg val loss=0.681, auc=0.570\n",
      "Iter=460, avg train loss=0.556, avg val loss=0.703, auc=0.580\n",
      "Iter=465, avg train loss=0.567, avg val loss=0.716, auc=0.586\n",
      "Iter=470, avg train loss=0.567, avg val loss=0.757, auc=0.601\n",
      "Iter=475, avg train loss=0.600, avg val loss=0.722, auc=0.594\n",
      "Iter=480, avg train loss=0.539, avg val loss=0.774, auc=0.591\n",
      "Iter=485, avg train loss=0.641, avg val loss=0.688, auc=0.595\n",
      "Iter=490, avg train loss=0.580, avg val loss=0.638, auc=0.612\n",
      "Best model saved.\n",
      "Iter=495, avg train loss=0.529, avg val loss=0.666, auc=0.591\n",
      "Iter=500, avg train loss=0.487, avg val loss=0.733, auc=0.583\n",
      "Iter=505, avg train loss=0.498, avg val loss=0.694, auc=0.596\n",
      "Iter=510, avg train loss=0.578, avg val loss=0.754, auc=0.567\n",
      "Iter=515, avg train loss=0.509, avg val loss=0.703, auc=0.555\n",
      "Iter=520, avg train loss=0.493, avg val loss=0.733, auc=0.549\n",
      "Iter=525, avg train loss=0.496, avg val loss=0.693, auc=0.545\n",
      "Iter=530, avg train loss=0.518, avg val loss=0.695, auc=0.574\n",
      "Iter=535, avg train loss=0.494, avg val loss=0.709, auc=0.575\n",
      "Iter=540, avg train loss=0.585, avg val loss=0.664, auc=0.574\n",
      "Iter=545, avg train loss=0.471, avg val loss=0.677, auc=0.564\n",
      "Iter=550, avg train loss=0.585, avg val loss=0.669, auc=0.553\n",
      "Iter=555, avg train loss=0.596, avg val loss=0.674, auc=0.562\n",
      "Iter=560, avg train loss=0.459, avg val loss=0.643, auc=0.570\n",
      "Iter=565, avg train loss=0.530, avg val loss=0.673, auc=0.563\n",
      "Iter=570, avg train loss=0.523, avg val loss=0.779, auc=0.540\n",
      "Iter=575, avg train loss=0.611, avg val loss=0.777, auc=0.518\n",
      "Iter=580, avg train loss=0.503, avg val loss=0.765, auc=0.522\n",
      "Iter=585, avg train loss=0.528, avg val loss=0.792, auc=0.518\n",
      "Iter=590, avg train loss=0.532, avg val loss=0.725, auc=0.518\n",
      "Iter=595, avg train loss=0.466, avg val loss=0.745, auc=0.551\n",
      "Iter=600, avg train loss=0.433, avg val loss=0.750, auc=0.563\n",
      "Iter=605, avg train loss=0.559, avg val loss=0.708, auc=0.578\n",
      "Iter=610, avg train loss=0.489, avg val loss=0.728, auc=0.573\n",
      "Iter=615, avg train loss=0.412, avg val loss=0.745, auc=0.550\n",
      "Iter=620, avg train loss=0.549, avg val loss=0.743, auc=0.509\n",
      "Iter=625, avg train loss=0.523, avg val loss=0.767, auc=0.511\n",
      "Iter=630, avg train loss=0.393, avg val loss=0.745, auc=0.525\n",
      "Iter=635, avg train loss=0.540, avg val loss=0.754, auc=0.536\n",
      "Iter=640, avg train loss=0.429, avg val loss=0.713, auc=0.533\n",
      "Iter=645, avg train loss=0.431, avg val loss=0.720, auc=0.536\n",
      "Iter=650, avg train loss=0.403, avg val loss=0.781, auc=0.540\n",
      "Iter=655, avg train loss=0.594, avg val loss=0.729, auc=0.555\n",
      "Iter=660, avg train loss=0.464, avg val loss=0.743, auc=0.547\n",
      "Iter=665, avg train loss=0.480, avg val loss=0.704, auc=0.555\n",
      "Iter=670, avg train loss=0.527, avg val loss=0.709, auc=0.546\n",
      "Iter=675, avg train loss=0.392, avg val loss=0.729, auc=0.553\n",
      "Iter=680, avg train loss=0.617, avg val loss=0.703, auc=0.566\n",
      "Iter=685, avg train loss=0.362, avg val loss=0.725, auc=0.545\n",
      "Iter=690, avg train loss=0.558, avg val loss=0.706, auc=0.542\n",
      "Iter=695, avg train loss=0.462, avg val loss=0.718, auc=0.535\n",
      "Iter=700, avg train loss=0.334, avg val loss=0.769, auc=0.524\n",
      "Iter=705, avg train loss=0.486, avg val loss=0.714, auc=0.536\n",
      "Iter=710, avg train loss=0.373, avg val loss=0.751, auc=0.540\n",
      "Iter=715, avg train loss=0.432, avg val loss=0.773, auc=0.533\n",
      "Iter=720, avg train loss=0.473, avg val loss=0.732, auc=0.556\n",
      "Iter=725, avg train loss=0.427, avg val loss=0.738, auc=0.550\n",
      "Iter=730, avg train loss=0.316, avg val loss=0.731, auc=0.549\n",
      "Iter=735, avg train loss=0.449, avg val loss=0.731, auc=0.536\n",
      "Iter=740, avg train loss=0.432, avg val loss=0.724, auc=0.566\n",
      "Iter=745, avg train loss=0.403, avg val loss=0.730, auc=0.562\n",
      "Iter=750, avg train loss=0.415, avg val loss=0.777, auc=0.568\n",
      "Iter=755, avg train loss=0.497, avg val loss=0.798, auc=0.574\n",
      "Iter=760, avg train loss=0.458, avg val loss=0.710, auc=0.565\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.511, max-score-based AUC=0.484\n",
      "\n",
      "\n",
      "========== Fold 5 ==========\n",
      "Test AUC at start=0.559, max-score-based AUC=0.547\n",
      "Iter=5, avg train loss=0.962, avg val loss=0.608, auc=0.609\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.796, avg val loss=0.575, auc=0.580\n",
      "Iter=15, avg train loss=0.916, avg val loss=0.570, auc=0.543\n",
      "Iter=20, avg train loss=0.709, avg val loss=0.573, auc=0.544\n",
      "Iter=25, avg train loss=0.745, avg val loss=0.575, auc=0.543\n",
      "Iter=30, avg train loss=0.838, avg val loss=0.575, auc=0.550\n",
      "Iter=35, avg train loss=0.809, avg val loss=0.585, auc=0.550\n",
      "Iter=40, avg train loss=0.802, avg val loss=0.575, auc=0.552\n",
      "Iter=45, avg train loss=0.808, avg val loss=0.570, auc=0.567\n",
      "Iter=50, avg train loss=0.742, avg val loss=0.576, auc=0.583\n",
      "Iter=55, avg train loss=0.679, avg val loss=0.583, auc=0.578\n",
      "Iter=60, avg train loss=0.727, avg val loss=0.605, auc=0.563\n",
      "Iter=65, avg train loss=0.632, avg val loss=0.617, auc=0.575\n",
      "Iter=70, avg train loss=0.692, avg val loss=0.635, auc=0.551\n",
      "Iter=75, avg train loss=0.760, avg val loss=0.623, auc=0.562\n",
      "Iter=80, avg train loss=0.666, avg val loss=0.626, auc=0.554\n",
      "Iter=85, avg train loss=0.715, avg val loss=0.635, auc=0.550\n",
      "Iter=90, avg train loss=0.718, avg val loss=0.616, auc=0.557\n",
      "Iter=95, avg train loss=0.685, avg val loss=0.638, auc=0.561\n",
      "Iter=100, avg train loss=0.667, avg val loss=0.644, auc=0.550\n",
      "Iter=105, avg train loss=0.656, avg val loss=0.635, auc=0.543\n",
      "Iter=110, avg train loss=0.610, avg val loss=0.628, auc=0.534\n",
      "Iter=115, avg train loss=0.686, avg val loss=0.616, auc=0.526\n",
      "Iter=120, avg train loss=0.618, avg val loss=0.644, auc=0.546\n",
      "Iter=125, avg train loss=0.701, avg val loss=0.637, auc=0.527\n",
      "Iter=130, avg train loss=0.566, avg val loss=0.605, auc=0.550\n",
      "Iter=135, avg train loss=0.604, avg val loss=0.615, auc=0.527\n",
      "Iter=140, avg train loss=0.709, avg val loss=0.619, auc=0.520\n",
      "Iter=145, avg train loss=0.731, avg val loss=0.645, auc=0.517\n",
      "Iter=150, avg train loss=0.690, avg val loss=0.671, auc=0.534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=155, avg train loss=0.619, avg val loss=0.706, auc=0.515\n",
      "Iter=160, avg train loss=0.650, avg val loss=0.663, auc=0.515\n",
      "Iter=165, avg train loss=0.632, avg val loss=0.692, auc=0.517\n",
      "Iter=170, avg train loss=0.601, avg val loss=0.689, auc=0.537\n",
      "Iter=175, avg train loss=0.600, avg val loss=0.683, auc=0.527\n",
      "Iter=180, avg train loss=0.704, avg val loss=0.664, auc=0.530\n",
      "Iter=185, avg train loss=0.708, avg val loss=0.651, auc=0.526\n",
      "Iter=190, avg train loss=0.627, avg val loss=0.637, auc=0.535\n",
      "Iter=195, avg train loss=0.589, avg val loss=0.654, auc=0.531\n",
      "Iter=200, avg train loss=0.565, avg val loss=0.655, auc=0.524\n",
      "Iter=205, avg train loss=0.765, avg val loss=0.620, auc=0.542\n",
      "Iter=210, avg train loss=0.652, avg val loss=0.657, auc=0.538\n",
      "Iter=215, avg train loss=0.572, avg val loss=0.675, auc=0.524\n",
      "Iter=220, avg train loss=0.653, avg val loss=0.626, auc=0.540\n",
      "Iter=225, avg train loss=0.614, avg val loss=0.627, auc=0.543\n",
      "Iter=230, avg train loss=0.629, avg val loss=0.649, auc=0.537\n",
      "Iter=235, avg train loss=0.592, avg val loss=0.660, auc=0.539\n",
      "Iter=240, avg train loss=0.667, avg val loss=0.635, auc=0.551\n",
      "Iter=245, avg train loss=0.652, avg val loss=0.630, auc=0.557\n",
      "Iter=250, avg train loss=0.586, avg val loss=0.623, auc=0.571\n",
      "Iter=255, avg train loss=0.664, avg val loss=0.607, auc=0.587\n",
      "Iter=260, avg train loss=0.702, avg val loss=0.647, auc=0.589\n",
      "Iter=265, avg train loss=0.652, avg val loss=0.602, auc=0.605\n",
      "Iter=270, avg train loss=0.600, avg val loss=0.594, auc=0.593\n",
      "Iter=275, avg train loss=0.669, avg val loss=0.597, auc=0.610\n",
      "Best model saved.\n",
      "Iter=280, avg train loss=0.640, avg val loss=0.613, auc=0.596\n",
      "Iter=285, avg train loss=0.562, avg val loss=0.640, auc=0.590\n",
      "Iter=290, avg train loss=0.627, avg val loss=0.603, auc=0.588\n",
      "Iter=295, avg train loss=0.626, avg val loss=0.639, auc=0.578\n",
      "Iter=300, avg train loss=0.506, avg val loss=0.625, auc=0.575\n",
      "Iter=305, avg train loss=0.547, avg val loss=0.630, auc=0.567\n",
      "Iter=310, avg train loss=0.625, avg val loss=0.605, auc=0.578\n",
      "Iter=315, avg train loss=0.583, avg val loss=0.587, auc=0.596\n",
      "Iter=320, avg train loss=0.682, avg val loss=0.608, auc=0.575\n",
      "Iter=325, avg train loss=0.566, avg val loss=0.656, auc=0.578\n",
      "Iter=330, avg train loss=0.537, avg val loss=0.712, auc=0.587\n",
      "Iter=335, avg train loss=0.587, avg val loss=0.671, auc=0.582\n",
      "Iter=340, avg train loss=0.566, avg val loss=0.711, auc=0.582\n",
      "Iter=345, avg train loss=0.645, avg val loss=0.664, auc=0.584\n",
      "Iter=350, avg train loss=0.589, avg val loss=0.665, auc=0.595\n",
      "Iter=355, avg train loss=0.495, avg val loss=0.632, auc=0.619\n",
      "Best model saved.\n",
      "Iter=360, avg train loss=0.568, avg val loss=0.601, auc=0.626\n",
      "Best model saved.\n",
      "Iter=365, avg train loss=0.623, avg val loss=0.600, auc=0.624\n",
      "Iter=370, avg train loss=0.681, avg val loss=0.632, auc=0.608\n",
      "Iter=375, avg train loss=0.550, avg val loss=0.634, auc=0.625\n",
      "Iter=380, avg train loss=0.648, avg val loss=0.604, auc=0.630\n",
      "Best model saved.\n",
      "Iter=385, avg train loss=0.557, avg val loss=0.629, auc=0.621\n",
      "Iter=390, avg train loss=0.572, avg val loss=0.627, auc=0.610\n",
      "Iter=395, avg train loss=0.539, avg val loss=0.674, auc=0.584\n",
      "Iter=400, avg train loss=0.520, avg val loss=0.647, auc=0.616\n",
      "Iter=405, avg train loss=0.580, avg val loss=0.631, auc=0.614\n",
      "Iter=410, avg train loss=0.525, avg val loss=0.709, auc=0.620\n",
      "Iter=415, avg train loss=0.544, avg val loss=0.773, auc=0.609\n",
      "Iter=420, avg train loss=0.603, avg val loss=0.706, auc=0.611\n",
      "Iter=425, avg train loss=0.582, avg val loss=0.653, auc=0.607\n",
      "Iter=430, avg train loss=0.510, avg val loss=0.623, auc=0.617\n",
      "Iter=435, avg train loss=0.608, avg val loss=0.655, auc=0.611\n",
      "Iter=440, avg train loss=0.489, avg val loss=0.665, auc=0.597\n",
      "Iter=445, avg train loss=0.578, avg val loss=0.657, auc=0.590\n",
      "Iter=450, avg train loss=0.578, avg val loss=0.624, auc=0.579\n",
      "Iter=455, avg train loss=0.594, avg val loss=0.654, auc=0.579\n",
      "Iter=460, avg train loss=0.573, avg val loss=0.717, auc=0.588\n",
      "Iter=465, avg train loss=0.512, avg val loss=0.838, auc=0.560\n",
      "Iter=470, avg train loss=0.525, avg val loss=0.762, auc=0.537\n",
      "Iter=475, avg train loss=0.536, avg val loss=0.747, auc=0.519\n",
      "Iter=480, avg train loss=0.586, avg val loss=0.634, auc=0.547\n",
      "Iter=485, avg train loss=0.475, avg val loss=0.593, auc=0.568\n",
      "Iter=490, avg train loss=0.519, avg val loss=0.600, auc=0.571\n",
      "Iter=495, avg train loss=0.485, avg val loss=0.579, auc=0.585\n",
      "Iter=500, avg train loss=0.566, avg val loss=0.587, auc=0.593\n",
      "Iter=505, avg train loss=0.623, avg val loss=0.675, auc=0.575\n",
      "Iter=510, avg train loss=0.410, avg val loss=0.711, auc=0.562\n",
      "Iter=515, avg train loss=0.491, avg val loss=0.723, auc=0.569\n",
      "Iter=520, avg train loss=0.471, avg val loss=0.628, auc=0.593\n",
      "Iter=525, avg train loss=0.567, avg val loss=0.629, auc=0.558\n",
      "Iter=530, avg train loss=0.458, avg val loss=0.637, auc=0.541\n",
      "Iter=535, avg train loss=0.512, avg val loss=0.653, auc=0.569\n",
      "Iter=540, avg train loss=0.581, avg val loss=0.648, auc=0.579\n",
      "Iter=545, avg train loss=0.534, avg val loss=0.749, auc=0.543\n",
      "Iter=550, avg train loss=0.542, avg val loss=0.883, auc=0.505\n",
      "Iter=555, avg train loss=0.499, avg val loss=0.826, auc=0.536\n",
      "Iter=560, avg train loss=0.638, avg val loss=0.750, auc=0.557\n",
      "Iter=565, avg train loss=0.383, avg val loss=0.662, auc=0.567\n",
      "Iter=570, avg train loss=0.495, avg val loss=0.619, auc=0.558\n",
      "Iter=575, avg train loss=0.339, avg val loss=0.658, auc=0.543\n",
      "Iter=580, avg train loss=0.602, avg val loss=0.677, auc=0.541\n",
      "Iter=585, avg train loss=0.489, avg val loss=0.756, auc=0.534\n",
      "Iter=590, avg train loss=0.442, avg val loss=0.876, auc=0.517\n",
      "Iter=595, avg train loss=0.448, avg val loss=0.701, auc=0.551\n",
      "Iter=600, avg train loss=0.490, avg val loss=0.691, auc=0.549\n",
      "Iter=605, avg train loss=0.530, avg val loss=0.656, auc=0.581\n",
      "Iter=610, avg train loss=0.469, avg val loss=0.633, auc=0.568\n",
      "Iter=615, avg train loss=0.435, avg val loss=0.764, auc=0.540\n",
      "Iter=620, avg train loss=0.476, avg val loss=0.859, auc=0.530\n",
      "Iter=625, avg train loss=0.511, avg val loss=0.739, auc=0.551\n",
      "Iter=630, avg train loss=0.512, avg val loss=0.613, auc=0.566\n",
      "Iter=635, avg train loss=0.407, avg val loss=0.645, auc=0.571\n",
      "Iter=640, avg train loss=0.501, avg val loss=0.701, auc=0.572\n",
      "Iter=645, avg train loss=0.497, avg val loss=0.732, auc=0.575\n",
      "Iter=650, avg train loss=0.434, avg val loss=0.732, auc=0.554\n",
      "Iter=655, avg train loss=0.527, avg val loss=0.635, auc=0.568\n",
      "Iter=660, avg train loss=0.421, avg val loss=0.636, auc=0.575\n",
      "Iter=665, avg train loss=0.305, avg val loss=0.686, auc=0.582\n",
      "Iter=670, avg train loss=0.440, avg val loss=0.668, auc=0.583\n",
      "Iter=675, avg train loss=0.377, avg val loss=0.690, auc=0.580\n",
      "Iter=680, avg train loss=0.377, avg val loss=0.642, auc=0.592\n",
      "Iter=685, avg train loss=0.489, avg val loss=0.714, auc=0.618\n",
      "Iter=690, avg train loss=0.500, avg val loss=0.684, auc=0.600\n",
      "Iter=695, avg train loss=0.381, avg val loss=0.703, auc=0.582\n",
      "Iter=700, avg train loss=0.405, avg val loss=0.705, auc=0.600\n",
      "Iter=705, avg train loss=0.360, avg val loss=0.651, auc=0.604\n",
      "Iter=710, avg train loss=0.427, avg val loss=0.662, auc=0.618\n",
      "Iter=715, avg train loss=0.463, avg val loss=0.745, auc=0.594\n",
      "Iter=720, avg train loss=0.506, avg val loss=0.734, auc=0.579\n",
      "Iter=725, avg train loss=0.430, avg val loss=0.647, auc=0.594\n",
      "Iter=730, avg train loss=0.402, avg val loss=0.719, auc=0.597\n",
      "Iter=735, avg train loss=0.558, avg val loss=0.739, auc=0.588\n",
      "Iter=740, avg train loss=0.451, avg val loss=0.669, auc=0.595\n",
      "Iter=745, avg train loss=0.451, avg val loss=0.619, auc=0.574\n",
      "Iter=750, avg train loss=0.408, avg val loss=0.648, auc=0.573\n",
      "Iter=755, avg train loss=0.340, avg val loss=0.732, auc=0.538\n",
      "Iter=760, avg train loss=0.354, avg val loss=0.830, auc=0.528\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.638, max-score-based AUC=0.719\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "cpu_threads = 4\n",
    "batch_size = 4\n",
    "n_folds = 5\n",
    "epochs = 20\n",
    "subject_pool, exam_pool = [], []\n",
    "pred_pool, label_pool, machine_pool = [], [], []\n",
    "age_pool, race_pool, bmi_pool = [], [], []\n",
    "birads_pool, libra_pool = [], []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=12345)\n",
    "fold = 0\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t3), 1)), ys_t3):\n",
    "    fold += 1\n",
    "    # train-val-test idx.\n",
    "    train_y = ys_t3[train_ix_]\n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(\n",
    "        train_ix_, test_size=test_prop, \n",
    "        stratify=train_y, random_state=12345)\n",
    "    # subset.\n",
    "    train_dataset = Subset(risk_dataset_t3, train_ix)\n",
    "    val_dataset = Subset(risk_dataset_t3, val_ix)\n",
    "    test_dataset = Subset(risk_dataset_t3, test_ix)\n",
    "    train_y = ys_t3[train_ix]\n",
    "    val_y = ys_t3[val_ix]\n",
    "    test_y = ys_t3[test_ix]\n",
    "    # weighted sampler.\n",
    "    f0, f1 = np.bincount(train_y)\n",
    "    train_w = np.zeros_like(train_y, dtype='float')\n",
    "    train_w[train_y==0] = 1/f0\n",
    "    train_w[train_y==1] = 1/f1\n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        train_w, len(train_y)//batch_size*batch_size, \n",
    "        replacement=True)\n",
    "    # data loaders.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "        collate_fn=mammo_collate)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    \n",
    "    image_only_parameters_rcc = shared_parameters.copy()\n",
    "    image_only_parameters_rcc[\"view\"] = \"R-CC\"\n",
    "    image_only_parameters_rcc[\"use_heatmaps\"] = False\n",
    "    image_only_parameters_rcc[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "    \n",
    "    image_only_parameters_rmlo = shared_parameters.copy()\n",
    "    image_only_parameters_rmlo[\"view\"] = \"R-MLO\"\n",
    "    image_only_parameters_rmlo[\"use_heatmaps\"] = False\n",
    "    image_only_parameters_rmlo[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "    \n",
    "    image_only_parameters_lmlo = shared_parameters.copy()\n",
    "    image_only_parameters_lmlo[\"view\"] = \"L-MLO\"\n",
    "    image_only_parameters_lmlo[\"use_heatmaps\"] = False\n",
    "    image_only_parameters_lmlo[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "    \n",
    "    # Reset model before training.\n",
    "    model, device_ = load_model(image_only_parameters_lmlo)\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # train & test.\n",
    "    best_name_ = 'best_model_{}_t3p_lmlo.pt'.format(fold)\n",
    "    print('='*10, 'Fold', fold, '='*10)\n",
    "    _, start_auc = val_loss(model, test_loader, device, return_auc=True)\n",
    "    start_auc_m = test_max_auc(model, test_loader, device)\n",
    "    \n",
    "    print('Test AUC at start={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        start_auc, start_auc_m))\n",
    "    \n",
    "    train(model, train_loader, val_loader, best_name_, device, \n",
    "          epochs=epochs, lr=1e-5, check_iters=5, log_name='finetune_t3p_rcc.txt')\n",
    "    \n",
    "    print('Predicting on the test set...', end='')\n",
    "    subject_list, exam_list, \\\n",
    "    pred_list, label_list, machine_list, \\\n",
    "    age_list, race_list, bmi_list, \\\n",
    "    birads_list, libra_list = do_test(model, test_loader, device)\n",
    "    subject_pool.extend(subject_list)\n",
    "    exam_pool.extend(exam_list)\n",
    "    pred_pool.extend(pred_list)\n",
    "    label_pool.extend(label_list)\n",
    "    machine_pool.extend(machine_list)\n",
    "    age_pool.extend(age_list)\n",
    "    race_pool.extend(race_list)\n",
    "    bmi_pool.extend(bmi_list)\n",
    "    birads_pool.extend(birads_list)\n",
    "    libra_pool.extend(libra_list) \n",
    "    # test AUC.\n",
    "    _, fold_auc = val_loss(model, test_loader, device, return_auc=True)\n",
    "    fold_auc_m = test_max_auc(model, test_loader, device)\n",
    "    print('Done')\n",
    "    print('Test AUC after training={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        fold_auc, fold_auc_m))\n",
    "    if fold < n_folds:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t3_lmlo = np.concatenate(subject_pool)\n",
    "all_exam_t3_lmlo = np.concatenate(exam_pool)\n",
    "all_preds_t3_lmlo = torch.cat(pred_pool)\n",
    "all_labels_t3_lmlo = torch.cat(label_pool)\n",
    "all_preds_t3_lmlo = all_preds_t3_lmlo.cpu().numpy()\n",
    "all_labels_t3_lmlo = all_labels_t3_lmlo.numpy()\n",
    "all_probs_max_t3_lmlo = all_preds_t3_lmlo.max(1)\n",
    "all_machines_t3_lmlo = np.concatenate(machine_pool)\n",
    "all_ages_t3_lmlo = np.concatenate(age_pool)\n",
    "all_races_t3_lmlo = np.concatenate(race_pool)\n",
    "all_bmis_t3_lmlo = np.concatenate(bmi_pool)\n",
    "all_birads_t3_lmlo = np.concatenate(birads_pool)\n",
    "all_libras_t3_lmlo = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['009112', '050212', '062061']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subj_t3_lmlo = [ '{:06d}'.format(s) for s in all_subj_t3_lmlo]\n",
    "all_subj_t3_lmlo[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>009112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>050212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>29.071513</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>062061</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>29.880615</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>020549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>26.879516</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>045170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject  exam  is_ge  age race        bmi birads\n",
       "0  009112     1      1   48    6  29.679908      b\n",
       "1  050212     1      1   42    8  29.071513      b\n",
       "2  062061     1      1   41    8  29.880615      c\n",
       "3  020549     1      1   62    8  26.879516      c\n",
       "4  045170     1      1   64    1  23.643988      b"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_t3_lmlo = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t3_lmlo, 'exam': all_exam_t3_lmlo, 'is_ge': all_machines_t3_lmlo, \n",
    "     'age': all_ages_t3_lmlo, 'race': all_races_t3_lmlo, 'bmi': all_bmis_t3_lmlo, \n",
    "     'birads': all_birads_t3_lmlo,})\n",
    "df_t3_lmlo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = np.concatenate([all_preds_t3_lmlo, all_labels_t3_lmlo[:, np.newaxis]], axis=1)\n",
    "d_[:3]\n",
    "df_t3_lmlo = pd.concat([df_t3_lmlo, pd.DataFrame(d_)], axis=1)\n",
    "df_t3_lmlo.head()\n",
    "df_t3_lmlo = df_t3_lmlo.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t3_lmlo = df_t3_lmlo.astype({'is_case': 'int8'})\n",
    "df_t3_lmlo.head()\n",
    "df_t3_lmlo.to_csv('../time_set/finetuned_pred_score_4view_T3p_nyu_single_view_train_lmlo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "      <th>ips-cc</th>\n",
       "      <th>ips-mlo</th>\n",
       "      <th>contra-cc</th>\n",
       "      <th>contra-mlo</th>\n",
       "      <th>is_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>009112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "      <td>0.304521</td>\n",
       "      <td>0.232105</td>\n",
       "      <td>0.274591</td>\n",
       "      <td>0.258612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>050212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>29.071513</td>\n",
       "      <td>b</td>\n",
       "      <td>0.412983</td>\n",
       "      <td>0.372199</td>\n",
       "      <td>0.339143</td>\n",
       "      <td>0.416420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>062061</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>29.880615</td>\n",
       "      <td>c</td>\n",
       "      <td>0.273903</td>\n",
       "      <td>0.369669</td>\n",
       "      <td>0.223389</td>\n",
       "      <td>0.372314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>020549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>26.879516</td>\n",
       "      <td>c</td>\n",
       "      <td>0.389335</td>\n",
       "      <td>0.456041</td>\n",
       "      <td>0.368480</td>\n",
       "      <td>0.516701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>045170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "      <td>0.598203</td>\n",
       "      <td>0.596657</td>\n",
       "      <td>0.314678</td>\n",
       "      <td>0.534118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>104915</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>30.625144</td>\n",
       "      <td>b</td>\n",
       "      <td>0.250022</td>\n",
       "      <td>0.205919</td>\n",
       "      <td>0.429894</td>\n",
       "      <td>0.361700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>780928</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>21.456605</td>\n",
       "      <td>c</td>\n",
       "      <td>0.248418</td>\n",
       "      <td>0.305946</td>\n",
       "      <td>0.220482</td>\n",
       "      <td>0.278784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>195698</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>34.784098</td>\n",
       "      <td>b</td>\n",
       "      <td>0.118622</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.212740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>230544</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>24.735432</td>\n",
       "      <td>c</td>\n",
       "      <td>0.543312</td>\n",
       "      <td>0.649607</td>\n",
       "      <td>0.273528</td>\n",
       "      <td>0.481383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>233277</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>30.196769</td>\n",
       "      <td>b</td>\n",
       "      <td>0.140187</td>\n",
       "      <td>0.180222</td>\n",
       "      <td>0.107538</td>\n",
       "      <td>0.148982</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject  exam  is_ge  age race        bmi birads    ips-cc   ips-mlo  \\\n",
       "0    009112     1      1   48    6  29.679908      b  0.304521  0.232105   \n",
       "1    050212     1      1   42    8  29.071513      b  0.412983  0.372199   \n",
       "2    062061     1      1   41    8  29.880615      c  0.273903  0.369669   \n",
       "3    020549     1      1   62    8  26.879516      c  0.389335  0.456041   \n",
       "4    045170     1      1   64    1  23.643988      b  0.598203  0.596657   \n",
       "..      ...   ...    ...  ...  ...        ...    ...       ...       ...   \n",
       "251  104915     2      1   47    8  30.625144      b  0.250022  0.205919   \n",
       "252  780928     3      1   66    4  21.456605      c  0.248418  0.305946   \n",
       "253  195698     3      1   73    8  34.784098      b  0.118622  0.149065   \n",
       "254  230544     6      1   58    8  24.735432      c  0.543312  0.649607   \n",
       "255  233277     5      1   58    8  30.196769      b  0.140187  0.180222   \n",
       "\n",
       "     contra-cc  contra-mlo  is_case  \n",
       "0     0.274591    0.258612        0  \n",
       "1     0.339143    0.416420        0  \n",
       "2     0.223389    0.372314        0  \n",
       "3     0.368480    0.516701        0  \n",
       "4     0.314678    0.534118        0  \n",
       "..         ...         ...      ...  \n",
       "251   0.429894    0.361700        0  \n",
       "252   0.220482    0.278784        1  \n",
       "253   0.096667    0.212740        0  \n",
       "254   0.273528    0.481383        0  \n",
       "255   0.107538    0.148982        0  \n",
       "\n",
       "[256 rows x 12 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t3_lmlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_t3_lcc = pd.read_csv('../time_set/finetuned_pred_score_4view_T3p_nyu_single_view_train_lcc.csv').iloc[:, [7,8,9,10]].values\n",
    "all_preds_t3_rcc = pd.read_csv('../time_set/finetuned_pred_score_4view_T3p_nyu_single_view_train_rcc.csv').iloc[:, [7,8,9,10]].values\n",
    "all_preds_t3_rmlo = pd.read_csv('../time_set/finetuned_pred_score_4view_T3p_nyu_single_view_train_rmlo.csv').iloc[:, [7,8,9,10]].values\n",
    "all_preds_t3_lmlo = pd.read_csv('../time_set/finetuned_pred_score_4view_T3p_nyu_single_view_train_lmlo.csv').iloc[:, [7,8,9,10]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_preds_t3 = (all_preds_t3_lmlo + all_preds_t3_lcc + all_preds_t3_rcc + all_preds_t3_rmlo) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmlo [[0.3045212  0.23210508 0.2745908  0.25861162]\n",
      " [0.4129827  0.37219867 0.33914253 0.41641963]\n",
      " [0.27390337 0.36966935 0.22338854 0.37231356]\n",
      " ...\n",
      " [0.11862215 0.14906469 0.09666682 0.21273951]\n",
      " [0.5433124  0.6496068  0.27352816 0.48138282]\n",
      " [0.14018653 0.18022181 0.10753763 0.14898185]]\n",
      "lcc [[0.20317103 0.16945077 0.31250197 0.5297075 ]\n",
      " [0.3365171  0.5484205  0.28681237 0.65435016]\n",
      " [0.14965224 0.084284   0.06922546 0.2652455 ]\n",
      " ...\n",
      " [0.32699254 0.34328938 0.33800042 0.33058527]\n",
      " [0.56082565 0.5238925  0.7311519  0.65048933]\n",
      " [0.25710955 0.15362966 0.214254   0.14270675]]\n",
      "rcc [[0.15563513 0.17080535 0.19516897 0.24901165]\n",
      " [0.254623   0.24362631 0.32191047 0.52902853]\n",
      " [0.15186371 0.04103323 0.07087473 0.22386558]\n",
      " ...\n",
      " [0.3782608  0.36807624 0.39721835 0.34895813]\n",
      " [0.42556557 0.43215024 0.5005677  0.49874818]\n",
      " [0.31781304 0.17652683 0.28034237 0.19249876]]\n",
      "rmlo [[0.50077754 0.251973   0.41222876 0.254368  ]\n",
      " [0.5467737  0.5569067  0.47044453 0.6245646 ]\n",
      " [0.45830807 0.65846    0.32467332 0.49347627]\n",
      " ...\n",
      " [0.03117913 0.06711233 0.0431823  0.16702427]\n",
      " [0.36409804 0.44508877 0.23205672 0.4107275 ]\n",
      " [0.12518586 0.09207974 0.07004864 0.11012685]]\n"
     ]
    }
   ],
   "source": [
    "print('lmlo', all_preds_t3_lmlo)\n",
    "print('lcc', all_preds_t3_lcc)\n",
    "print('rcc', all_preds_t3_rcc)\n",
    "print('rmlo', all_preds_t3_rmlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=256\n",
      "4view max AUC=0.636\n",
      "4view mean AUC=0.623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t3_lmlo)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t3_rmlo, avg_preds_t3.max(axis=1))))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t3_rmlo, avg_preds_t3.mean(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=256\n",
      "4view max AUC=0.636\n",
      "4view mean AUC=0.623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t3_lmlo)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t3_rmlo, avg_preds_t3.max(axis=1))))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t3_rmlo, avg_preds_t3.mean(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=256\n",
      "4view max AUC=0.594\n",
      "4view mean AUC=0.601\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t3_lmlo)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t3_lmlo, all_probs_max_t3_lmlo)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t3_lmlo, all_preds_t3_lmlo.mean(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t3_rmlo = np.concatenate(subject_pool)\n",
    "all_exam_t3_rmlo = np.concatenate(exam_pool)\n",
    "all_preds_t3_rmlo = torch.cat(pred_pool)\n",
    "all_labels_t3_rmlo = torch.cat(label_pool)\n",
    "all_preds_t3_rmlo = all_preds_t3_rmlo.cpu().numpy()\n",
    "all_labels_t3_rmlo = all_labels_t3_rmlo.numpy()\n",
    "all_probs_max_t3_rmlo = all_preds_t3_rmlo.max(1)\n",
    "all_machines_t3_rmlo = np.concatenate(machine_pool)\n",
    "all_ages_t3_rmlo = np.concatenate(age_pool)\n",
    "all_races_t3_rmlo = np.concatenate(race_pool)\n",
    "all_bmis_t3_rmlo = np.concatenate(bmi_pool)\n",
    "all_birads_t3_rmlo = np.concatenate(birads_pool)\n",
    "all_libras_t3_rmlo = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "      <th>ips-cc</th>\n",
       "      <th>ips-mlo</th>\n",
       "      <th>contra-cc</th>\n",
       "      <th>contra-mlo</th>\n",
       "      <th>is_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "      <td>0.500778</td>\n",
       "      <td>0.251973</td>\n",
       "      <td>0.412229</td>\n",
       "      <td>0.254368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>29.071513</td>\n",
       "      <td>b</td>\n",
       "      <td>0.546774</td>\n",
       "      <td>0.556907</td>\n",
       "      <td>0.470445</td>\n",
       "      <td>0.624565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62061</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>29.880615</td>\n",
       "      <td>c</td>\n",
       "      <td>0.458308</td>\n",
       "      <td>0.658460</td>\n",
       "      <td>0.324673</td>\n",
       "      <td>0.493476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>26.879516</td>\n",
       "      <td>c</td>\n",
       "      <td>0.553072</td>\n",
       "      <td>0.641456</td>\n",
       "      <td>0.532164</td>\n",
       "      <td>0.601776</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "      <td>0.747165</td>\n",
       "      <td>0.763433</td>\n",
       "      <td>0.287816</td>\n",
       "      <td>0.793160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>104915</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>30.625144</td>\n",
       "      <td>b</td>\n",
       "      <td>0.383301</td>\n",
       "      <td>0.205719</td>\n",
       "      <td>0.394153</td>\n",
       "      <td>0.358733</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>780928</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>21.456605</td>\n",
       "      <td>c</td>\n",
       "      <td>0.203912</td>\n",
       "      <td>0.160643</td>\n",
       "      <td>0.217358</td>\n",
       "      <td>0.192662</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>195698</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>34.784098</td>\n",
       "      <td>b</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.067112</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>0.167024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>230544</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>24.735432</td>\n",
       "      <td>c</td>\n",
       "      <td>0.364098</td>\n",
       "      <td>0.445089</td>\n",
       "      <td>0.232057</td>\n",
       "      <td>0.410728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>233277</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>30.196769</td>\n",
       "      <td>b</td>\n",
       "      <td>0.125186</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.070049</td>\n",
       "      <td>0.110127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject  exam  is_ge  age race        bmi birads    ips-cc   ips-mlo  \\\n",
       "0       9112     1      1   48    6  29.679908      b  0.500778  0.251973   \n",
       "1      50212     1      1   42    8  29.071513      b  0.546774  0.556907   \n",
       "2      62061     1      1   41    8  29.880615      c  0.458308  0.658460   \n",
       "3      20549     1      1   62    8  26.879516      c  0.553072  0.641456   \n",
       "4      45170     1      1   64    1  23.643988      b  0.747165  0.763433   \n",
       "..       ...   ...    ...  ...  ...        ...    ...       ...       ...   \n",
       "251   104915     2      1   47    8  30.625144      b  0.383301  0.205719   \n",
       "252   780928     3      1   66    4  21.456605      c  0.203912  0.160643   \n",
       "253   195698     3      1   73    8  34.784098      b  0.031179  0.067112   \n",
       "254   230544     6      1   58    8  24.735432      c  0.364098  0.445089   \n",
       "255   233277     5      1   58    8  30.196769      b  0.125186  0.092080   \n",
       "\n",
       "     contra-cc  contra-mlo  is_case  \n",
       "0     0.412229    0.254368        0  \n",
       "1     0.470445    0.624565        0  \n",
       "2     0.324673    0.493476        0  \n",
       "3     0.532164    0.601776        0  \n",
       "4     0.287816    0.793160        0  \n",
       "..         ...         ...      ...  \n",
       "251   0.394153    0.358733        0  \n",
       "252   0.217358    0.192662        1  \n",
       "253   0.043182    0.167024        0  \n",
       "254   0.232057    0.410728        0  \n",
       "255   0.070049    0.110127        0  \n",
       "\n",
       "[256 rows x 12 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t3_rmlo = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t3_rmlo, 'exam': all_exam_t3_rmlo, 'is_ge': all_machines_t3_rmlo, \n",
    "     'age': all_ages_t3_rmlo, 'race': all_races_t3_rmlo, 'bmi': all_bmis_t3_rmlo, \n",
    "     'birads': all_birads_t3_rmlo,})\n",
    "df_t3_rmlo.head()\n",
    "d_ = np.concatenate([all_preds_t3_rmlo, all_labels_t3_rmlo[:, np.newaxis]], axis=1)\n",
    "d_[:3]\n",
    "df_t3_rmlo = pd.concat([df_t3_rmlo, pd.DataFrame(d_)], axis=1)\n",
    "df_t3_rmlo.head()\n",
    "df_t3_rmlo = df_t3_rmlo.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t3_rmlo = df_t3_rmlo.astype({'is_case': 'int8'})\n",
    "df_t3_rmlo.head()\n",
    "df_t3_rmlo.to_csv('../time_set/finetuned_pred_score_4view_T3p_nyu_single_view_train_rmlo.csv', index=False)\n",
    "df_t3_rmlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=256\n",
      "4view max AUC=0.621\n",
      "4view mean AUC=0.610\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t3_rmlo)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t3_rmlo, all_probs_max_t3_rmlo)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t3_rmlo, all_preds_t3_rmlo.mean(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t3_rcc = np.concatenate(subject_pool)\n",
    "all_exam_t3_rcc = np.concatenate(exam_pool)\n",
    "all_preds_t3_rcc = torch.cat(pred_pool)\n",
    "all_labels_t3_rcc = torch.cat(label_pool)\n",
    "all_preds_t3_rcc = all_preds_t3_rcc.cpu().numpy()\n",
    "all_labels_t3_rcc = all_labels_t3_rcc.numpy()\n",
    "all_probs_max_t3_rcc = all_preds_t3_rcc.max(1)\n",
    "all_machines_t3_rcc = np.concatenate(machine_pool)\n",
    "all_ages_t3_rcc = np.concatenate(age_pool)\n",
    "all_races_t3_rcc = np.concatenate(race_pool)\n",
    "all_bmis_t3_rcc = np.concatenate(bmi_pool)\n",
    "all_birads_t3_rcc = np.concatenate(birads_pool)\n",
    "all_libras_t3_rcc = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject  exam  is_ge  age race        bmi birads\n",
      "0     9112     1      1   48    6  29.679908      b\n",
      "1    50212     1      1   42    8  29.071513      b\n",
      "2    62061     1      1   41    8  29.880615      c\n",
      "3    20549     1      1   62    8  26.879516      c\n",
      "4    45170     1      1   64    1  23.643988      b\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "      <th>ips-cc</th>\n",
       "      <th>ips-mlo</th>\n",
       "      <th>contra-cc</th>\n",
       "      <th>contra-mlo</th>\n",
       "      <th>is_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "      <td>0.155635</td>\n",
       "      <td>0.170805</td>\n",
       "      <td>0.195169</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>29.071513</td>\n",
       "      <td>b</td>\n",
       "      <td>0.254623</td>\n",
       "      <td>0.243626</td>\n",
       "      <td>0.321910</td>\n",
       "      <td>0.529029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62061</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>29.880615</td>\n",
       "      <td>c</td>\n",
       "      <td>0.151864</td>\n",
       "      <td>0.041033</td>\n",
       "      <td>0.070875</td>\n",
       "      <td>0.223866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>26.879516</td>\n",
       "      <td>c</td>\n",
       "      <td>0.550666</td>\n",
       "      <td>0.762339</td>\n",
       "      <td>0.488756</td>\n",
       "      <td>0.786969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "      <td>0.470914</td>\n",
       "      <td>0.622266</td>\n",
       "      <td>0.262973</td>\n",
       "      <td>0.359517</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>104915</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>30.625144</td>\n",
       "      <td>b</td>\n",
       "      <td>0.261728</td>\n",
       "      <td>0.139807</td>\n",
       "      <td>0.327221</td>\n",
       "      <td>0.294291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>780928</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>21.456605</td>\n",
       "      <td>c</td>\n",
       "      <td>0.319006</td>\n",
       "      <td>0.343166</td>\n",
       "      <td>0.260633</td>\n",
       "      <td>0.203384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>195698</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>34.784098</td>\n",
       "      <td>b</td>\n",
       "      <td>0.378261</td>\n",
       "      <td>0.368076</td>\n",
       "      <td>0.397218</td>\n",
       "      <td>0.348958</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>230544</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>24.735432</td>\n",
       "      <td>c</td>\n",
       "      <td>0.425566</td>\n",
       "      <td>0.432150</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>0.498748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>233277</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>30.196769</td>\n",
       "      <td>b</td>\n",
       "      <td>0.317813</td>\n",
       "      <td>0.176527</td>\n",
       "      <td>0.280342</td>\n",
       "      <td>0.192499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject  exam  is_ge  age race        bmi birads    ips-cc   ips-mlo  \\\n",
       "0       9112     1      1   48    6  29.679908      b  0.155635  0.170805   \n",
       "1      50212     1      1   42    8  29.071513      b  0.254623  0.243626   \n",
       "2      62061     1      1   41    8  29.880615      c  0.151864  0.041033   \n",
       "3      20549     1      1   62    8  26.879516      c  0.550666  0.762339   \n",
       "4      45170     1      1   64    1  23.643988      b  0.470914  0.622266   \n",
       "..       ...   ...    ...  ...  ...        ...    ...       ...       ...   \n",
       "251   104915     2      1   47    8  30.625144      b  0.261728  0.139807   \n",
       "252   780928     3      1   66    4  21.456605      c  0.319006  0.343166   \n",
       "253   195698     3      1   73    8  34.784098      b  0.378261  0.368076   \n",
       "254   230544     6      1   58    8  24.735432      c  0.425566  0.432150   \n",
       "255   233277     5      1   58    8  30.196769      b  0.317813  0.176527   \n",
       "\n",
       "     contra-cc  contra-mlo  is_case  \n",
       "0     0.195169    0.249012        0  \n",
       "1     0.321910    0.529029        0  \n",
       "2     0.070875    0.223866        0  \n",
       "3     0.488756    0.786969        0  \n",
       "4     0.262973    0.359517        0  \n",
       "..         ...         ...      ...  \n",
       "251   0.327221    0.294291        0  \n",
       "252   0.260633    0.203384        1  \n",
       "253   0.397218    0.348958        0  \n",
       "254   0.500568    0.498748        0  \n",
       "255   0.280342    0.192499        0  \n",
       "\n",
       "[256 rows x 12 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t3_rcc = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t3_rcc, 'exam': all_exam_t3_rcc, 'is_ge': all_machines_t3_rcc, \n",
    "     'age': all_ages_t3_rcc, 'race': all_races_t3_rcc, 'bmi': all_bmis_t3_rcc, \n",
    "     'birads': all_birads_t3_rcc,})\n",
    "print(df_t3_rcc.head())\n",
    "d_ = np.concatenate([all_preds_t3_rcc, all_labels_t3_rcc[:, np.newaxis]], axis=1)\n",
    "d_[:3]\n",
    "df_t3_rcc = pd.concat([df_t3_rcc, pd.DataFrame(d_)], axis=1)\n",
    "df_t3_rcc.head()\n",
    "df_t3_rcc = df_t3_rcc.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t3_rcc = df_t3_rcc.astype({'is_case': 'int8'})\n",
    "df_t3_rcc.head()\n",
    "df_t3_rcc.to_csv('../time_set/finetuned_pred_score_4view_T3p_nyu_single_view_train_rcc.csv', index=False)\n",
    "df_t3_rcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=256\n",
      "4view max AUC=0.575\n",
      "4view mean AUC=0.597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t3_rcc)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t3_rcc, all_probs_max_t3_rcc)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t3_rcc, all_preds_t3_rcc.mean(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t3 = np.concatenate(subject_pool)\n",
    "all_exam_t3 = np.concatenate(exam_pool)\n",
    "all_preds_t3 = torch.cat(pred_pool)\n",
    "all_labels_t3 = torch.cat(label_pool)\n",
    "all_preds_t3 = all_preds_t3.cpu().numpy()\n",
    "all_labels_t3 = all_labels_t3.numpy()\n",
    "all_probs_max_t3 = all_preds_t3.max(1)\n",
    "all_machines_t3 = np.concatenate(machine_pool)\n",
    "all_ages_t3 = np.concatenate(age_pool)\n",
    "all_races_t3 = np.concatenate(race_pool)\n",
    "all_bmis_t3 = np.concatenate(bmi_pool)\n",
    "all_birads_t3 = np.concatenate(birads_pool)\n",
    "all_libras_t3 = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=256\n",
      "4view max AUC=0.596\n",
      "4view mean AUC=0.596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t3)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t3, all_probs_max_t3)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t3, all_preds_t3.mean(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['009112', '050212', '062061']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subj_t3 = [ '{:06d}'.format(s) for s in all_subj_t3]\n",
    "all_subj_t3[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>009112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>050212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>29.071513</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>062061</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>29.880615</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>020549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>26.879516</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>045170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject  exam  is_ge  age race        bmi birads\n",
       "0  009112     1      1   48    6  29.679908      b\n",
       "1  050212     1      1   42    8  29.071513      b\n",
       "2  062061     1      1   41    8  29.880615      c\n",
       "3  020549     1      1   62    8  26.879516      c\n",
       "4  045170     1      1   64    1  23.643988      b"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_t3 = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t3, 'exam': all_exam_t3, 'is_ge': all_machines_t3, \n",
    "     'age': all_ages_t3, 'race': all_races_t3, 'bmi': all_bmis_t3, \n",
    "     'birads': all_birads_t3,})\n",
    "df_t3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20317103, 0.16945077, 0.31250197, 0.5297075 , 0.        ],\n",
       "       [0.3365171 , 0.5484205 , 0.28681237, 0.65435016, 0.        ],\n",
       "       [0.14965224, 0.084284  , 0.06922546, 0.2652455 , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ = np.concatenate([all_preds_t3, all_labels_t3[:, np.newaxis]], axis=1)\n",
    "d_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>009112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "      <td>0.203171</td>\n",
       "      <td>0.169451</td>\n",
       "      <td>0.312502</td>\n",
       "      <td>0.529707</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>050212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>29.071513</td>\n",
       "      <td>b</td>\n",
       "      <td>0.336517</td>\n",
       "      <td>0.548420</td>\n",
       "      <td>0.286812</td>\n",
       "      <td>0.654350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>062061</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>29.880615</td>\n",
       "      <td>c</td>\n",
       "      <td>0.149652</td>\n",
       "      <td>0.084284</td>\n",
       "      <td>0.069225</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>020549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>26.879516</td>\n",
       "      <td>c</td>\n",
       "      <td>0.592148</td>\n",
       "      <td>0.875797</td>\n",
       "      <td>0.532543</td>\n",
       "      <td>0.898938</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>045170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "      <td>0.720334</td>\n",
       "      <td>0.892669</td>\n",
       "      <td>0.320591</td>\n",
       "      <td>0.756474</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject  exam  is_ge  age race        bmi birads         0         1  \\\n",
       "0  009112     1      1   48    6  29.679908      b  0.203171  0.169451   \n",
       "1  050212     1      1   42    8  29.071513      b  0.336517  0.548420   \n",
       "2  062061     1      1   41    8  29.880615      c  0.149652  0.084284   \n",
       "3  020549     1      1   62    8  26.879516      c  0.592148  0.875797   \n",
       "4  045170     1      1   64    1  23.643988      b  0.720334  0.892669   \n",
       "\n",
       "          2         3    4  \n",
       "0  0.312502  0.529707  0.0  \n",
       "1  0.286812  0.654350  0.0  \n",
       "2  0.069225  0.265245  0.0  \n",
       "3  0.532543  0.898938  0.0  \n",
       "4  0.320591  0.756474  0.0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t3 = pd.concat([df_t3, pd.DataFrame(d_)], axis=1)\n",
    "df_t3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "      <th>ips-cc</th>\n",
       "      <th>ips-mlo</th>\n",
       "      <th>contra-cc</th>\n",
       "      <th>contra-mlo</th>\n",
       "      <th>is_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>009112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "      <td>0.203171</td>\n",
       "      <td>0.169451</td>\n",
       "      <td>0.312502</td>\n",
       "      <td>0.529707</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>050212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>29.071513</td>\n",
       "      <td>b</td>\n",
       "      <td>0.336517</td>\n",
       "      <td>0.548420</td>\n",
       "      <td>0.286812</td>\n",
       "      <td>0.654350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>062061</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>29.880615</td>\n",
       "      <td>c</td>\n",
       "      <td>0.149652</td>\n",
       "      <td>0.084284</td>\n",
       "      <td>0.069225</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>020549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>26.879516</td>\n",
       "      <td>c</td>\n",
       "      <td>0.592148</td>\n",
       "      <td>0.875797</td>\n",
       "      <td>0.532543</td>\n",
       "      <td>0.898938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>045170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "      <td>0.720334</td>\n",
       "      <td>0.892669</td>\n",
       "      <td>0.320591</td>\n",
       "      <td>0.756474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject  exam  is_ge  age race        bmi birads    ips-cc   ips-mlo  \\\n",
       "0  009112     1      1   48    6  29.679908      b  0.203171  0.169451   \n",
       "1  050212     1      1   42    8  29.071513      b  0.336517  0.548420   \n",
       "2  062061     1      1   41    8  29.880615      c  0.149652  0.084284   \n",
       "3  020549     1      1   62    8  26.879516      c  0.592148  0.875797   \n",
       "4  045170     1      1   64    1  23.643988      b  0.720334  0.892669   \n",
       "\n",
       "   contra-cc  contra-mlo  is_case  \n",
       "0   0.312502    0.529707        0  \n",
       "1   0.286812    0.654350        0  \n",
       "2   0.069225    0.265245        0  \n",
       "3   0.532543    0.898938        0  \n",
       "4   0.320591    0.756474        0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t3 = df_t3.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t3 = df_t3.astype({'is_case': 'int8'})\n",
    "df_t3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t3.to_csv('../time_set/finetuned_pred_score_4view_T3p_nyu_single_view_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5961100260416667"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(all_labels_t3, all_preds_t3.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5958658854166666"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(all_labels_t3, all_preds_t3.max(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1+ Years Predicton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_dataset_t1p = torch.load('../time_set/risk_pred_GEHolo_4view_matchedSepCase_T1+.pt')\n",
    "len(risk_dataset_t1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    <mit_risk_data.FaceRight object at 0x7f137bab4310>\n",
       "    <mit_risk_data.NormalizePix object at 0x7f137bab4bd0>\n",
       "    <mit_risk_data.ToTensor3D object at 0x7f137bab4f10>\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_dataset_t1p.transform = trans\n",
    "risk_dataset_t1p.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from: ys_t1p.pkl\n"
     ]
    }
   ],
   "source": [
    "ys_t1p = load_pkl('ys_t1p.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Fold 1 ==========\n",
      "Test AUC at start=0.511, max-score-based AUC=0.515\n",
      "Iter=5, avg train loss=1.056, avg val loss=0.629, auc=0.389\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=1.088, avg val loss=0.627, auc=0.402\n",
      "Best model saved.\n",
      "Iter=15, avg train loss=1.117, avg val loss=0.623, auc=0.406\n",
      "Best model saved.\n",
      "Iter=20, avg train loss=0.789, avg val loss=0.611, auc=0.440\n",
      "Best model saved.\n",
      "Iter=25, avg train loss=0.801, avg val loss=0.604, auc=0.430\n",
      "Iter=30, avg train loss=0.650, avg val loss=0.605, auc=0.445\n",
      "Best model saved.\n",
      "Iter=35, avg train loss=0.675, avg val loss=0.600, auc=0.475\n",
      "Best model saved.\n",
      "Iter=40, avg train loss=0.693, avg val loss=0.597, auc=0.499\n",
      "Best model saved.\n",
      "Iter=45, avg train loss=0.681, avg val loss=0.606, auc=0.477\n",
      "Iter=50, avg train loss=0.794, avg val loss=0.597, auc=0.509\n",
      "Best model saved.\n",
      "Iter=55, avg train loss=0.693, avg val loss=0.605, auc=0.504\n",
      "Iter=60, avg train loss=0.723, avg val loss=0.603, auc=0.504\n",
      "Iter=65, avg train loss=0.728, avg val loss=0.619, auc=0.493\n",
      "Iter=70, avg train loss=0.719, avg val loss=0.638, auc=0.481\n",
      "Iter=75, avg train loss=0.670, avg val loss=0.628, auc=0.477\n",
      "Iter=80, avg train loss=0.743, avg val loss=0.627, auc=0.454\n",
      "Iter=85, avg train loss=0.645, avg val loss=0.629, auc=0.465\n",
      "Iter=90, avg train loss=0.627, avg val loss=0.629, auc=0.489\n",
      "Iter=95, avg train loss=0.705, avg val loss=0.633, auc=0.491\n",
      "Iter=100, avg train loss=0.651, avg val loss=0.632, auc=0.486\n",
      "Iter=105, avg train loss=0.647, avg val loss=0.633, auc=0.480\n",
      "Iter=110, avg train loss=0.663, avg val loss=0.649, auc=0.481\n",
      "Iter=115, avg train loss=0.608, avg val loss=0.642, auc=0.479\n",
      "Iter=120, avg train loss=0.674, avg val loss=0.628, auc=0.488\n",
      "Iter=125, avg train loss=0.639, avg val loss=0.653, auc=0.465\n",
      "Iter=130, avg train loss=0.676, avg val loss=0.644, auc=0.474\n",
      "Iter=135, avg train loss=0.623, avg val loss=0.630, auc=0.473\n",
      "Iter=140, avg train loss=0.578, avg val loss=0.642, auc=0.484\n",
      "Iter=145, avg train loss=0.649, avg val loss=0.645, auc=0.488\n",
      "Iter=150, avg train loss=0.703, avg val loss=0.654, auc=0.462\n",
      "Iter=155, avg train loss=0.679, avg val loss=0.661, auc=0.443\n",
      "Iter=160, avg train loss=0.606, avg val loss=0.656, auc=0.443\n",
      "Iter=165, avg train loss=0.663, avg val loss=0.636, auc=0.454\n",
      "Iter=170, avg train loss=0.652, avg val loss=0.664, auc=0.450\n",
      "Iter=175, avg train loss=0.540, avg val loss=0.658, auc=0.450\n",
      "Iter=180, avg train loss=0.639, avg val loss=0.666, auc=0.460\n",
      "Iter=185, avg train loss=0.651, avg val loss=0.688, auc=0.450\n",
      "Iter=190, avg train loss=0.699, avg val loss=0.699, auc=0.444\n",
      "Iter=195, avg train loss=0.680, avg val loss=0.733, auc=0.438\n",
      "Iter=200, avg train loss=0.670, avg val loss=0.687, auc=0.458\n",
      "Iter=205, avg train loss=0.597, avg val loss=0.669, auc=0.461\n",
      "Iter=210, avg train loss=0.531, avg val loss=0.676, auc=0.465\n",
      "Iter=215, avg train loss=0.660, avg val loss=0.668, auc=0.458\n",
      "Iter=220, avg train loss=0.634, avg val loss=0.714, auc=0.455\n",
      "Iter=225, avg train loss=0.674, avg val loss=0.661, auc=0.475\n",
      "Iter=230, avg train loss=0.723, avg val loss=0.644, auc=0.491\n",
      "Iter=235, avg train loss=0.599, avg val loss=0.668, auc=0.490\n",
      "Iter=240, avg train loss=0.629, avg val loss=0.679, auc=0.490\n",
      "Iter=245, avg train loss=0.686, avg val loss=0.687, auc=0.495\n",
      "Iter=250, avg train loss=0.603, avg val loss=0.731, auc=0.489\n",
      "Iter=255, avg train loss=0.634, avg val loss=0.707, auc=0.487\n",
      "Iter=260, avg train loss=0.634, avg val loss=0.697, auc=0.467\n",
      "Iter=265, avg train loss=0.602, avg val loss=0.660, auc=0.482\n",
      "Iter=270, avg train loss=0.605, avg val loss=0.680, auc=0.502\n",
      "Iter=275, avg train loss=0.583, avg val loss=0.658, auc=0.507\n",
      "Iter=280, avg train loss=0.770, avg val loss=0.670, auc=0.514\n",
      "Best model saved.\n",
      "Iter=285, avg train loss=0.524, avg val loss=0.663, auc=0.519\n",
      "Best model saved.\n",
      "Iter=290, avg train loss=0.569, avg val loss=0.666, auc=0.518\n",
      "Iter=295, avg train loss=0.549, avg val loss=0.654, auc=0.525\n",
      "Best model saved.\n",
      "Iter=300, avg train loss=0.622, avg val loss=0.651, auc=0.526\n",
      "Best model saved.\n",
      "Iter=305, avg train loss=0.716, avg val loss=0.628, auc=0.521\n",
      "Iter=310, avg train loss=0.574, avg val loss=0.653, auc=0.515\n",
      "Iter=315, avg train loss=0.676, avg val loss=0.654, auc=0.532\n",
      "Best model saved.\n",
      "Iter=320, avg train loss=0.541, avg val loss=0.697, auc=0.519\n",
      "Iter=325, avg train loss=0.529, avg val loss=0.683, auc=0.501\n",
      "Iter=330, avg train loss=0.629, avg val loss=0.698, auc=0.509\n",
      "Iter=335, avg train loss=0.613, avg val loss=0.751, auc=0.496\n",
      "Iter=340, avg train loss=0.663, avg val loss=0.691, auc=0.508\n",
      "Iter=345, avg train loss=0.629, avg val loss=0.738, auc=0.484\n",
      "Iter=350, avg train loss=0.581, avg val loss=0.710, auc=0.487\n",
      "Iter=355, avg train loss=0.623, avg val loss=0.722, auc=0.489\n",
      "Iter=360, avg train loss=0.611, avg val loss=0.713, auc=0.479\n",
      "Iter=365, avg train loss=0.602, avg val loss=0.792, auc=0.492\n",
      "Iter=370, avg train loss=0.609, avg val loss=0.754, auc=0.491\n",
      "Iter=375, avg train loss=0.575, avg val loss=0.687, auc=0.504\n",
      "Iter=380, avg train loss=0.625, avg val loss=0.632, auc=0.507\n",
      "Iter=385, avg train loss=0.490, avg val loss=0.648, auc=0.518\n",
      "Iter=390, avg train loss=0.594, avg val loss=0.642, auc=0.515\n",
      "Iter=395, avg train loss=0.562, avg val loss=0.684, auc=0.501\n",
      "Iter=400, avg train loss=0.601, avg val loss=0.731, auc=0.502\n",
      "Iter=405, avg train loss=0.563, avg val loss=0.741, auc=0.492\n",
      "Iter=410, avg train loss=0.527, avg val loss=0.673, auc=0.486\n",
      "Iter=415, avg train loss=0.661, avg val loss=0.698, auc=0.485\n",
      "Iter=420, avg train loss=0.539, avg val loss=0.766, auc=0.478\n",
      "Iter=425, avg train loss=0.584, avg val loss=0.736, auc=0.486\n",
      "Iter=430, avg train loss=0.729, avg val loss=0.730, auc=0.497\n",
      "Iter=435, avg train loss=0.563, avg val loss=0.738, auc=0.506\n",
      "Iter=440, avg train loss=0.580, avg val loss=0.734, auc=0.488\n",
      "Iter=445, avg train loss=0.613, avg val loss=0.736, auc=0.492\n",
      "Iter=450, avg train loss=0.573, avg val loss=0.712, auc=0.485\n",
      "Iter=455, avg train loss=0.621, avg val loss=0.691, auc=0.464\n",
      "Iter=460, avg train loss=0.537, avg val loss=0.646, auc=0.480\n",
      "Iter=465, avg train loss=0.649, avg val loss=0.715, auc=0.479\n",
      "Iter=470, avg train loss=0.594, avg val loss=0.645, auc=0.503\n",
      "Iter=475, avg train loss=0.588, avg val loss=0.674, auc=0.499\n",
      "Iter=480, avg train loss=0.534, avg val loss=0.783, auc=0.489\n",
      "Iter=485, avg train loss=0.503, avg val loss=0.692, auc=0.520\n",
      "Iter=490, avg train loss=0.596, avg val loss=0.729, auc=0.529\n",
      "Iter=495, avg train loss=0.629, avg val loss=0.740, auc=0.536\n",
      "Best model saved.\n",
      "Iter=500, avg train loss=0.492, avg val loss=0.713, auc=0.527\n",
      "Iter=505, avg train loss=0.556, avg val loss=0.716, auc=0.534\n",
      "Iter=510, avg train loss=0.585, avg val loss=0.656, auc=0.518\n",
      "Iter=515, avg train loss=0.478, avg val loss=0.645, auc=0.517\n",
      "Iter=520, avg train loss=0.645, avg val loss=0.677, auc=0.502\n",
      "Iter=525, avg train loss=0.558, avg val loss=0.659, auc=0.476\n",
      "Iter=530, avg train loss=0.569, avg val loss=0.663, auc=0.483\n",
      "Iter=535, avg train loss=0.653, avg val loss=0.668, auc=0.490\n",
      "Iter=540, avg train loss=0.432, avg val loss=0.703, auc=0.519\n",
      "Iter=545, avg train loss=0.527, avg val loss=0.688, auc=0.546\n",
      "Best model saved.\n",
      "Iter=550, avg train loss=0.573, avg val loss=0.732, auc=0.551\n",
      "Best model saved.\n",
      "Iter=555, avg train loss=0.559, avg val loss=0.745, auc=0.524\n",
      "Iter=560, avg train loss=0.658, avg val loss=0.765, auc=0.505\n",
      "Iter=565, avg train loss=0.553, avg val loss=0.762, auc=0.497\n",
      "Iter=570, avg train loss=0.435, avg val loss=0.649, auc=0.510\n",
      "Iter=575, avg train loss=0.467, avg val loss=0.671, auc=0.502\n",
      "Iter=580, avg train loss=0.558, avg val loss=0.678, auc=0.498\n",
      "Iter=585, avg train loss=0.538, avg val loss=0.738, auc=0.508\n",
      "Iter=590, avg train loss=0.651, avg val loss=0.788, auc=0.518\n",
      "Iter=595, avg train loss=0.497, avg val loss=0.854, auc=0.517\n",
      "Iter=600, avg train loss=0.622, avg val loss=0.824, auc=0.526\n",
      "Iter=605, avg train loss=0.536, avg val loss=0.705, auc=0.519\n",
      "Iter=610, avg train loss=0.652, avg val loss=0.654, auc=0.531\n",
      "Iter=615, avg train loss=0.513, avg val loss=0.618, auc=0.564\n",
      "Best model saved.\n",
      "Iter=620, avg train loss=0.576, avg val loss=0.610, auc=0.557\n",
      "Iter=625, avg train loss=0.604, avg val loss=0.627, auc=0.515\n",
      "Iter=630, avg train loss=0.564, avg val loss=0.634, auc=0.515\n",
      "Iter=635, avg train loss=0.627, avg val loss=0.670, auc=0.514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=640, avg train loss=0.503, avg val loss=0.701, auc=0.495\n",
      "Iter=645, avg train loss=0.505, avg val loss=0.754, auc=0.513\n",
      "Iter=650, avg train loss=0.626, avg val loss=0.680, auc=0.524\n",
      "Iter=655, avg train loss=0.471, avg val loss=0.635, auc=0.545\n",
      "Iter=660, avg train loss=0.688, avg val loss=0.676, auc=0.519\n",
      "Iter=665, avg train loss=0.529, avg val loss=0.736, auc=0.517\n",
      "Iter=670, avg train loss=0.529, avg val loss=0.702, auc=0.529\n",
      "Iter=675, avg train loss=0.418, avg val loss=0.718, auc=0.511\n",
      "Iter=680, avg train loss=0.493, avg val loss=0.639, auc=0.539\n",
      "Iter=685, avg train loss=0.537, avg val loss=0.626, auc=0.535\n",
      "Iter=690, avg train loss=0.403, avg val loss=0.663, auc=0.550\n",
      "Iter=695, avg train loss=0.566, avg val loss=0.665, auc=0.541\n",
      "Iter=700, avg train loss=0.535, avg val loss=0.686, auc=0.546\n",
      "Iter=705, avg train loss=0.482, avg val loss=0.656, auc=0.548\n",
      "Iter=710, avg train loss=0.504, avg val loss=0.666, auc=0.513\n",
      "Iter=715, avg train loss=0.418, avg val loss=0.676, auc=0.518\n",
      "Iter=720, avg train loss=0.493, avg val loss=0.679, auc=0.526\n",
      "Iter=725, avg train loss=0.436, avg val loss=0.779, auc=0.513\n",
      "Iter=730, avg train loss=0.400, avg val loss=0.692, auc=0.538\n",
      "Iter=735, avg train loss=0.545, avg val loss=0.659, auc=0.523\n",
      "Iter=740, avg train loss=0.481, avg val loss=0.658, auc=0.524\n",
      "Iter=745, avg train loss=0.544, avg val loss=0.668, auc=0.535\n",
      "Iter=750, avg train loss=0.468, avg val loss=0.785, auc=0.523\n",
      "Iter=755, avg train loss=0.615, avg val loss=0.721, auc=0.510\n",
      "Iter=760, avg train loss=0.487, avg val loss=0.842, auc=0.497\n",
      "Iter=765, avg train loss=0.522, avg val loss=0.827, auc=0.498\n",
      "Iter=770, avg train loss=0.474, avg val loss=0.742, auc=0.512\n",
      "Iter=775, avg train loss=0.450, avg val loss=0.652, auc=0.518\n",
      "Iter=780, avg train loss=0.479, avg val loss=0.675, auc=0.539\n",
      "Iter=785, avg train loss=0.488, avg val loss=0.625, auc=0.582\n",
      "Best model saved.\n",
      "Iter=790, avg train loss=0.519, avg val loss=0.642, auc=0.569\n",
      "Iter=795, avg train loss=0.472, avg val loss=0.624, auc=0.561\n",
      "Iter=800, avg train loss=0.481, avg val loss=0.668, auc=0.558\n",
      "Iter=805, avg train loss=0.516, avg val loss=0.644, auc=0.570\n",
      "Iter=810, avg train loss=0.422, avg val loss=0.641, auc=0.556\n",
      "Iter=815, avg train loss=0.332, avg val loss=0.627, auc=0.577\n",
      "Iter=820, avg train loss=0.561, avg val loss=0.653, auc=0.561\n",
      "Iter=825, avg train loss=0.491, avg val loss=0.722, auc=0.541\n",
      "Iter=830, avg train loss=0.434, avg val loss=0.706, auc=0.529\n",
      "Iter=835, avg train loss=0.561, avg val loss=0.685, auc=0.540\n",
      "Iter=840, avg train loss=0.445, avg val loss=0.634, auc=0.516\n",
      "Iter=845, avg train loss=0.568, avg val loss=0.640, auc=0.517\n",
      "Iter=850, avg train loss=0.298, avg val loss=0.659, auc=0.505\n",
      "Iter=855, avg train loss=0.531, avg val loss=0.706, auc=0.522\n",
      "Iter=860, avg train loss=0.479, avg val loss=0.774, auc=0.542\n",
      "Iter=865, avg train loss=0.455, avg val loss=0.694, auc=0.552\n",
      "Iter=870, avg train loss=0.543, avg val loss=0.668, auc=0.552\n",
      "Iter=875, avg train loss=0.449, avg val loss=0.636, auc=0.553\n",
      "Iter=880, avg train loss=0.409, avg val loss=0.626, auc=0.551\n",
      "Iter=885, avg train loss=0.467, avg val loss=0.639, auc=0.558\n",
      "Iter=890, avg train loss=0.384, avg val loss=0.919, auc=0.525\n",
      "Iter=895, avg train loss=0.512, avg val loss=0.742, auc=0.548\n",
      "Iter=900, avg train loss=0.346, avg val loss=0.649, auc=0.553\n",
      "Iter=905, avg train loss=0.444, avg val loss=0.639, auc=0.573\n",
      "Iter=910, avg train loss=0.327, avg val loss=0.633, auc=0.579\n",
      "Iter=915, avg train loss=0.512, avg val loss=0.619, auc=0.562\n",
      "Iter=920, avg train loss=0.468, avg val loss=0.680, auc=0.571\n",
      "Iter=925, avg train loss=0.478, avg val loss=0.780, auc=0.560\n",
      "Iter=930, avg train loss=0.516, avg val loss=0.626, auc=0.581\n",
      "Iter=935, avg train loss=0.375, avg val loss=0.611, auc=0.543\n",
      "Iter=940, avg train loss=0.429, avg val loss=0.662, auc=0.503\n",
      "Iter=945, avg train loss=0.414, avg val loss=0.677, auc=0.538\n",
      "Iter=950, avg train loss=0.350, avg val loss=0.725, auc=0.532\n",
      "Iter=955, avg train loss=0.441, avg val loss=0.670, auc=0.552\n",
      "Iter=960, avg train loss=0.384, avg val loss=0.644, auc=0.541\n",
      "Iter=965, avg train loss=0.353, avg val loss=0.642, auc=0.540\n",
      "Iter=970, avg train loss=0.445, avg val loss=0.675, auc=0.512\n",
      "Iter=975, avg train loss=0.417, avg val loss=0.680, auc=0.516\n",
      "Iter=980, avg train loss=0.420, avg val loss=0.806, auc=0.520\n",
      "Iter=985, avg train loss=0.355, avg val loss=0.764, auc=0.491\n",
      "Iter=990, avg train loss=0.326, avg val loss=0.720, auc=0.523\n",
      "Iter=995, avg train loss=0.395, avg val loss=0.648, auc=0.531\n",
      "Iter=1000, avg train loss=0.426, avg val loss=0.768, auc=0.536\n",
      "Iter=1005, avg train loss=0.326, avg val loss=0.836, auc=0.529\n",
      "Iter=1010, avg train loss=0.369, avg val loss=0.741, auc=0.496\n",
      "Iter=1015, avg train loss=0.515, avg val loss=0.689, auc=0.497\n",
      "Iter=1020, avg train loss=0.397, avg val loss=0.746, auc=0.519\n",
      "Iter=1025, avg train loss=0.303, avg val loss=0.754, auc=0.528\n",
      "Iter=1030, avg train loss=0.371, avg val loss=0.683, auc=0.505\n",
      "Iter=1035, avg train loss=0.396, avg val loss=0.659, auc=0.514\n",
      "Iter=1040, avg train loss=0.519, avg val loss=0.744, auc=0.508\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.642, max-score-based AUC=0.618\n",
      "\n",
      "\n",
      "========== Fold 2 ==========\n",
      "Test AUC at start=0.454, max-score-based AUC=0.384\n",
      "Iter=5, avg train loss=1.191, avg val loss=0.585, auc=0.548\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.647, avg val loss=0.577, auc=0.572\n",
      "Best model saved.\n",
      "Iter=15, avg train loss=0.841, avg val loss=0.586, auc=0.554\n",
      "Iter=20, avg train loss=0.973, avg val loss=0.594, auc=0.543\n",
      "Iter=25, avg train loss=0.761, avg val loss=0.589, auc=0.546\n",
      "Iter=30, avg train loss=0.869, avg val loss=0.587, auc=0.547\n",
      "Iter=35, avg train loss=0.779, avg val loss=0.592, auc=0.549\n",
      "Iter=40, avg train loss=0.877, avg val loss=0.583, auc=0.549\n",
      "Iter=45, avg train loss=0.709, avg val loss=0.588, auc=0.539\n",
      "Iter=50, avg train loss=0.699, avg val loss=0.590, auc=0.542\n",
      "Iter=55, avg train loss=0.757, avg val loss=0.610, auc=0.528\n",
      "Iter=60, avg train loss=0.744, avg val loss=0.603, auc=0.533\n",
      "Iter=65, avg train loss=0.775, avg val loss=0.613, auc=0.544\n",
      "Iter=70, avg train loss=0.762, avg val loss=0.599, auc=0.542\n",
      "Iter=75, avg train loss=0.715, avg val loss=0.612, auc=0.548\n",
      "Iter=80, avg train loss=0.634, avg val loss=0.607, auc=0.534\n",
      "Iter=85, avg train loss=0.701, avg val loss=0.604, auc=0.540\n",
      "Iter=90, avg train loss=0.670, avg val loss=0.619, auc=0.536\n",
      "Iter=95, avg train loss=0.672, avg val loss=0.621, auc=0.548\n",
      "Iter=100, avg train loss=0.682, avg val loss=0.638, auc=0.558\n",
      "Iter=105, avg train loss=0.695, avg val loss=0.630, auc=0.558\n",
      "Iter=110, avg train loss=0.749, avg val loss=0.634, auc=0.555\n",
      "Iter=115, avg train loss=0.667, avg val loss=0.661, auc=0.555\n",
      "Iter=120, avg train loss=0.688, avg val loss=0.685, auc=0.567\n",
      "Iter=125, avg train loss=0.708, avg val loss=0.686, auc=0.566\n",
      "Iter=130, avg train loss=0.692, avg val loss=0.678, auc=0.559\n",
      "Iter=135, avg train loss=0.680, avg val loss=0.654, auc=0.565\n",
      "Iter=140, avg train loss=0.708, avg val loss=0.690, auc=0.569\n",
      "Iter=145, avg train loss=0.675, avg val loss=0.651, auc=0.567\n",
      "Iter=150, avg train loss=0.702, avg val loss=0.693, auc=0.584\n",
      "Best model saved.\n",
      "Iter=155, avg train loss=0.694, avg val loss=0.709, auc=0.571\n",
      "Iter=160, avg train loss=0.645, avg val loss=0.686, auc=0.594\n",
      "Best model saved.\n",
      "Iter=165, avg train loss=0.687, avg val loss=0.658, auc=0.586\n",
      "Iter=170, avg train loss=0.608, avg val loss=0.651, auc=0.581\n",
      "Iter=175, avg train loss=0.654, avg val loss=0.657, auc=0.592\n",
      "Iter=180, avg train loss=0.694, avg val loss=0.670, auc=0.594\n",
      "Best model saved.\n",
      "Iter=185, avg train loss=0.667, avg val loss=0.699, auc=0.596\n",
      "Best model saved.\n",
      "Iter=190, avg train loss=0.633, avg val loss=0.671, auc=0.584\n",
      "Iter=195, avg train loss=0.682, avg val loss=0.724, auc=0.577\n",
      "Iter=200, avg train loss=0.680, avg val loss=0.697, auc=0.579\n",
      "Iter=205, avg train loss=0.659, avg val loss=0.688, auc=0.574\n",
      "Iter=210, avg train loss=0.660, avg val loss=0.714, auc=0.572\n",
      "Iter=215, avg train loss=0.664, avg val loss=0.735, auc=0.566\n",
      "Iter=220, avg train loss=0.661, avg val loss=0.682, auc=0.568\n",
      "Iter=225, avg train loss=0.708, avg val loss=0.696, auc=0.578\n",
      "Iter=230, avg train loss=0.658, avg val loss=0.743, auc=0.586\n",
      "Iter=235, avg train loss=0.664, avg val loss=0.696, auc=0.588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=240, avg train loss=0.656, avg val loss=0.672, auc=0.606\n",
      "Best model saved.\n",
      "Iter=245, avg train loss=0.678, avg val loss=0.728, auc=0.600\n",
      "Iter=250, avg train loss=0.691, avg val loss=0.712, auc=0.601\n",
      "Iter=255, avg train loss=0.648, avg val loss=0.731, auc=0.620\n",
      "Best model saved.\n",
      "Iter=260, avg train loss=0.679, avg val loss=0.694, auc=0.624\n",
      "Best model saved.\n",
      "Iter=265, avg train loss=0.661, avg val loss=0.680, auc=0.629\n",
      "Best model saved.\n",
      "Iter=270, avg train loss=0.675, avg val loss=0.667, auc=0.621\n",
      "Iter=275, avg train loss=0.631, avg val loss=0.656, auc=0.621\n",
      "Iter=280, avg train loss=0.675, avg val loss=0.718, auc=0.633\n",
      "Best model saved.\n",
      "Iter=285, avg train loss=0.636, avg val loss=0.686, auc=0.630\n",
      "Iter=290, avg train loss=0.689, avg val loss=0.632, auc=0.623\n",
      "Iter=295, avg train loss=0.673, avg val loss=0.641, auc=0.615\n",
      "Iter=300, avg train loss=0.726, avg val loss=0.634, auc=0.600\n",
      "Iter=305, avg train loss=0.686, avg val loss=0.650, auc=0.593\n",
      "Iter=310, avg train loss=0.629, avg val loss=0.645, auc=0.594\n",
      "Iter=315, avg train loss=0.682, avg val loss=0.689, auc=0.601\n",
      "Iter=320, avg train loss=0.645, avg val loss=0.631, auc=0.584\n",
      "Iter=325, avg train loss=0.623, avg val loss=0.669, auc=0.587\n",
      "Iter=330, avg train loss=0.618, avg val loss=0.692, auc=0.583\n",
      "Iter=335, avg train loss=0.669, avg val loss=0.700, auc=0.575\n",
      "Iter=340, avg train loss=0.624, avg val loss=0.659, auc=0.579\n",
      "Iter=345, avg train loss=0.611, avg val loss=0.666, auc=0.573\n",
      "Iter=350, avg train loss=0.597, avg val loss=0.643, auc=0.583\n",
      "Iter=355, avg train loss=0.723, avg val loss=0.638, auc=0.588\n",
      "Iter=360, avg train loss=0.668, avg val loss=0.637, auc=0.601\n",
      "Iter=365, avg train loss=0.689, avg val loss=0.634, auc=0.597\n",
      "Iter=370, avg train loss=0.643, avg val loss=0.653, auc=0.610\n",
      "Iter=375, avg train loss=0.615, avg val loss=0.693, auc=0.624\n",
      "Iter=380, avg train loss=0.656, avg val loss=0.608, auc=0.612\n",
      "Iter=385, avg train loss=0.566, avg val loss=0.644, auc=0.593\n",
      "Iter=390, avg train loss=0.685, avg val loss=0.621, auc=0.579\n",
      "Iter=395, avg train loss=0.639, avg val loss=0.626, auc=0.579\n",
      "Iter=400, avg train loss=0.645, avg val loss=0.628, auc=0.567\n",
      "Iter=405, avg train loss=0.593, avg val loss=0.629, auc=0.556\n",
      "Iter=410, avg train loss=0.664, avg val loss=0.664, auc=0.553\n",
      "Iter=415, avg train loss=0.626, avg val loss=0.679, auc=0.565\n",
      "Iter=420, avg train loss=0.607, avg val loss=0.766, auc=0.568\n",
      "Iter=425, avg train loss=0.597, avg val loss=0.726, auc=0.578\n",
      "Iter=430, avg train loss=0.628, avg val loss=0.719, auc=0.585\n",
      "Iter=435, avg train loss=0.618, avg val loss=0.701, auc=0.598\n",
      "Iter=440, avg train loss=0.656, avg val loss=0.731, auc=0.605\n",
      "Iter=445, avg train loss=0.634, avg val loss=0.736, auc=0.612\n",
      "Iter=450, avg train loss=0.606, avg val loss=0.712, auc=0.615\n",
      "Iter=455, avg train loss=0.553, avg val loss=0.676, auc=0.616\n",
      "Iter=460, avg train loss=0.606, avg val loss=0.684, auc=0.611\n",
      "Iter=465, avg train loss=0.603, avg val loss=0.654, auc=0.606\n",
      "Iter=470, avg train loss=0.607, avg val loss=0.647, auc=0.596\n",
      "Iter=475, avg train loss=0.615, avg val loss=0.710, auc=0.605\n",
      "Iter=480, avg train loss=0.557, avg val loss=0.641, auc=0.591\n",
      "Iter=485, avg train loss=0.540, avg val loss=0.629, auc=0.579\n",
      "Iter=490, avg train loss=0.628, avg val loss=0.656, auc=0.574\n",
      "Iter=495, avg train loss=0.595, avg val loss=0.667, auc=0.562\n",
      "Iter=500, avg train loss=0.631, avg val loss=0.659, auc=0.556\n",
      "Iter=505, avg train loss=0.696, avg val loss=0.694, auc=0.536\n",
      "Iter=510, avg train loss=0.591, avg val loss=0.710, auc=0.532\n",
      "Iter=515, avg train loss=0.629, avg val loss=0.753, auc=0.542\n",
      "Iter=520, avg train loss=0.638, avg val loss=0.808, auc=0.581\n",
      "Iter=525, avg train loss=0.658, avg val loss=0.783, auc=0.587\n",
      "Iter=530, avg train loss=0.560, avg val loss=0.730, auc=0.600\n",
      "Iter=535, avg train loss=0.543, avg val loss=0.711, auc=0.606\n",
      "Iter=540, avg train loss=0.482, avg val loss=0.636, auc=0.603\n",
      "Iter=545, avg train loss=0.654, avg val loss=0.620, auc=0.578\n",
      "Iter=550, avg train loss=0.560, avg val loss=0.607, auc=0.584\n",
      "Iter=555, avg train loss=0.504, avg val loss=0.622, auc=0.598\n",
      "Iter=560, avg train loss=0.445, avg val loss=0.625, auc=0.587\n",
      "Iter=565, avg train loss=0.597, avg val loss=0.640, auc=0.594\n",
      "Iter=570, avg train loss=0.475, avg val loss=0.668, auc=0.606\n",
      "Iter=575, avg train loss=0.643, avg val loss=0.656, auc=0.597\n",
      "Iter=580, avg train loss=0.692, avg val loss=0.650, auc=0.612\n",
      "Iter=585, avg train loss=0.517, avg val loss=0.737, auc=0.612\n",
      "Iter=590, avg train loss=0.551, avg val loss=0.696, auc=0.610\n",
      "Iter=595, avg train loss=0.578, avg val loss=0.636, auc=0.595\n",
      "Iter=600, avg train loss=0.561, avg val loss=0.620, auc=0.588\n",
      "Iter=605, avg train loss=0.564, avg val loss=0.647, auc=0.566\n",
      "Iter=610, avg train loss=0.646, avg val loss=0.676, auc=0.544\n",
      "Iter=615, avg train loss=0.563, avg val loss=0.757, auc=0.547\n",
      "Iter=620, avg train loss=0.638, avg val loss=0.744, auc=0.557\n",
      "Iter=625, avg train loss=0.549, avg val loss=0.791, auc=0.550\n",
      "Iter=630, avg train loss=0.655, avg val loss=0.743, auc=0.552\n",
      "Iter=635, avg train loss=0.600, avg val loss=0.718, auc=0.578\n",
      "Iter=640, avg train loss=0.511, avg val loss=0.743, auc=0.584\n",
      "Iter=645, avg train loss=0.570, avg val loss=0.762, auc=0.594\n",
      "Iter=650, avg train loss=0.615, avg val loss=0.782, auc=0.596\n",
      "Iter=655, avg train loss=0.535, avg val loss=0.717, auc=0.565\n",
      "Iter=660, avg train loss=0.581, avg val loss=0.718, auc=0.555\n",
      "Iter=665, avg train loss=0.538, avg val loss=0.770, auc=0.565\n",
      "Iter=670, avg train loss=0.577, avg val loss=0.736, auc=0.570\n",
      "Iter=675, avg train loss=0.584, avg val loss=0.721, auc=0.567\n",
      "Iter=680, avg train loss=0.581, avg val loss=0.678, auc=0.580\n",
      "Iter=685, avg train loss=0.557, avg val loss=0.644, auc=0.573\n",
      "Iter=690, avg train loss=0.547, avg val loss=0.685, auc=0.575\n",
      "Iter=695, avg train loss=0.415, avg val loss=0.665, auc=0.577\n",
      "Iter=700, avg train loss=0.666, avg val loss=0.672, auc=0.580\n",
      "Iter=705, avg train loss=0.597, avg val loss=0.672, auc=0.588\n",
      "Iter=710, avg train loss=0.559, avg val loss=0.663, auc=0.613\n",
      "Iter=715, avg train loss=0.540, avg val loss=0.668, auc=0.560\n",
      "Iter=720, avg train loss=0.618, avg val loss=0.674, auc=0.524\n",
      "Iter=725, avg train loss=0.432, avg val loss=0.672, auc=0.523\n",
      "Iter=730, avg train loss=0.580, avg val loss=0.678, auc=0.516\n",
      "Iter=735, avg train loss=0.578, avg val loss=0.748, auc=0.520\n",
      "Iter=740, avg train loss=0.443, avg val loss=0.765, auc=0.544\n",
      "Iter=745, avg train loss=0.482, avg val loss=0.730, auc=0.556\n",
      "Iter=750, avg train loss=0.495, avg val loss=0.856, auc=0.579\n",
      "Iter=755, avg train loss=0.522, avg val loss=0.753, auc=0.565\n",
      "Iter=760, avg train loss=0.557, avg val loss=0.781, auc=0.566\n",
      "Iter=765, avg train loss=0.575, avg val loss=0.754, auc=0.572\n",
      "Iter=770, avg train loss=0.587, avg val loss=0.813, auc=0.600\n",
      "Iter=775, avg train loss=0.503, avg val loss=0.715, auc=0.600\n",
      "Iter=780, avg train loss=0.646, avg val loss=0.756, auc=0.606\n",
      "Iter=785, avg train loss=0.582, avg val loss=0.677, auc=0.596\n",
      "Iter=790, avg train loss=0.512, avg val loss=0.666, auc=0.624\n",
      "Iter=795, avg train loss=0.509, avg val loss=0.672, auc=0.633\n",
      "Iter=800, avg train loss=0.480, avg val loss=0.648, auc=0.615\n",
      "Iter=805, avg train loss=0.449, avg val loss=0.630, auc=0.619\n",
      "Iter=810, avg train loss=0.500, avg val loss=0.619, auc=0.583\n",
      "Iter=815, avg train loss=0.577, avg val loss=0.606, auc=0.577\n",
      "Iter=820, avg train loss=0.474, avg val loss=0.637, auc=0.588\n",
      "Iter=825, avg train loss=0.639, avg val loss=0.747, auc=0.595\n",
      "Iter=830, avg train loss=0.677, avg val loss=0.763, auc=0.612\n",
      "Iter=835, avg train loss=0.448, avg val loss=0.836, auc=0.595\n",
      "Iter=840, avg train loss=0.656, avg val loss=0.862, auc=0.606\n",
      "Iter=845, avg train loss=0.482, avg val loss=0.822, auc=0.630\n",
      "Iter=850, avg train loss=0.488, avg val loss=0.787, auc=0.646\n",
      "Best model saved.\n",
      "Iter=855, avg train loss=0.518, avg val loss=0.730, auc=0.637\n",
      "Iter=860, avg train loss=0.539, avg val loss=0.688, auc=0.636\n",
      "Iter=865, avg train loss=0.596, avg val loss=0.636, auc=0.621\n",
      "Iter=870, avg train loss=0.423, avg val loss=0.631, auc=0.631\n",
      "Iter=875, avg train loss=0.490, avg val loss=0.627, auc=0.627\n",
      "Iter=880, avg train loss=0.495, avg val loss=0.626, auc=0.648\n",
      "Best model saved.\n",
      "Iter=885, avg train loss=0.498, avg val loss=0.628, auc=0.623\n",
      "Iter=890, avg train loss=0.507, avg val loss=0.650, auc=0.604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=895, avg train loss=0.442, avg val loss=0.623, auc=0.600\n",
      "Iter=900, avg train loss=0.467, avg val loss=0.632, auc=0.603\n",
      "Iter=905, avg train loss=0.436, avg val loss=0.703, auc=0.623\n",
      "Iter=910, avg train loss=0.494, avg val loss=0.708, auc=0.635\n",
      "Iter=915, avg train loss=0.514, avg val loss=0.957, auc=0.656\n",
      "Best model saved.\n",
      "Iter=920, avg train loss=0.382, avg val loss=0.778, auc=0.651\n",
      "Iter=925, avg train loss=0.475, avg val loss=0.718, auc=0.640\n",
      "Iter=930, avg train loss=0.417, avg val loss=0.714, auc=0.638\n",
      "Iter=935, avg train loss=0.458, avg val loss=0.667, auc=0.629\n",
      "Iter=940, avg train loss=0.443, avg val loss=0.675, auc=0.652\n",
      "Iter=945, avg train loss=0.460, avg val loss=0.701, auc=0.647\n",
      "Iter=950, avg train loss=0.505, avg val loss=0.625, auc=0.627\n",
      "Iter=955, avg train loss=0.434, avg val loss=0.666, auc=0.597\n",
      "Iter=960, avg train loss=0.601, avg val loss=0.640, auc=0.584\n",
      "Iter=965, avg train loss=0.513, avg val loss=0.634, auc=0.570\n",
      "Iter=970, avg train loss=0.492, avg val loss=0.639, auc=0.571\n",
      "Iter=975, avg train loss=0.444, avg val loss=0.672, auc=0.570\n",
      "Iter=980, avg train loss=0.524, avg val loss=0.717, auc=0.591\n",
      "Iter=985, avg train loss=0.547, avg val loss=0.849, auc=0.633\n",
      "Iter=990, avg train loss=0.427, avg val loss=0.845, auc=0.634\n",
      "Iter=995, avg train loss=0.369, avg val loss=0.676, auc=0.623\n",
      "Iter=1000, avg train loss=0.462, avg val loss=0.639, auc=0.588\n",
      "Iter=1005, avg train loss=0.508, avg val loss=0.641, auc=0.563\n",
      "Iter=1010, avg train loss=0.446, avg val loss=0.656, auc=0.579\n",
      "Iter=1015, avg train loss=0.373, avg val loss=0.837, auc=0.601\n",
      "Iter=1020, avg train loss=0.398, avg val loss=0.756, auc=0.616\n",
      "Iter=1025, avg train loss=0.416, avg val loss=0.771, auc=0.623\n",
      "Iter=1030, avg train loss=0.537, avg val loss=0.642, auc=0.596\n",
      "Iter=1035, avg train loss=0.353, avg val loss=0.657, auc=0.572\n",
      "Iter=1040, avg train loss=0.494, avg val loss=0.727, auc=0.565\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.627, max-score-based AUC=0.623\n",
      "\n",
      "\n",
      "========== Fold 3 ==========\n",
      "Test AUC at start=0.477, max-score-based AUC=0.421\n",
      "Iter=5, avg train loss=1.031, avg val loss=0.564, auc=0.620\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.893, avg val loss=0.565, auc=0.584\n",
      "Iter=15, avg train loss=1.233, avg val loss=0.575, auc=0.559\n",
      "Iter=20, avg train loss=0.889, avg val loss=0.583, auc=0.537\n",
      "Iter=25, avg train loss=0.720, avg val loss=0.583, auc=0.541\n",
      "Iter=30, avg train loss=0.892, avg val loss=0.583, auc=0.553\n",
      "Iter=35, avg train loss=0.820, avg val loss=0.577, auc=0.556\n",
      "Iter=40, avg train loss=0.679, avg val loss=0.587, auc=0.548\n",
      "Iter=45, avg train loss=0.699, avg val loss=0.582, auc=0.573\n",
      "Iter=50, avg train loss=0.828, avg val loss=0.597, auc=0.576\n",
      "Iter=55, avg train loss=0.841, avg val loss=0.601, auc=0.574\n",
      "Iter=60, avg train loss=0.674, avg val loss=0.607, auc=0.567\n",
      "Iter=65, avg train loss=0.717, avg val loss=0.614, auc=0.580\n",
      "Iter=70, avg train loss=0.655, avg val loss=0.625, auc=0.586\n",
      "Iter=75, avg train loss=0.644, avg val loss=0.642, auc=0.589\n",
      "Iter=80, avg train loss=0.677, avg val loss=0.608, auc=0.616\n",
      "Iter=85, avg train loss=0.645, avg val loss=0.599, auc=0.625\n",
      "Best model saved.\n",
      "Iter=90, avg train loss=0.705, avg val loss=0.637, auc=0.605\n",
      "Iter=95, avg train loss=0.633, avg val loss=0.615, auc=0.625\n",
      "Iter=100, avg train loss=0.600, avg val loss=0.597, auc=0.635\n",
      "Best model saved.\n",
      "Iter=105, avg train loss=0.701, avg val loss=0.583, auc=0.651\n",
      "Best model saved.\n",
      "Iter=110, avg train loss=0.687, avg val loss=0.603, auc=0.643\n",
      "Iter=115, avg train loss=0.748, avg val loss=0.600, auc=0.645\n",
      "Iter=120, avg train loss=0.753, avg val loss=0.615, auc=0.643\n",
      "Iter=125, avg train loss=0.690, avg val loss=0.626, auc=0.642\n",
      "Iter=130, avg train loss=0.767, avg val loss=0.599, auc=0.650\n",
      "Iter=135, avg train loss=0.646, avg val loss=0.619, auc=0.650\n",
      "Iter=140, avg train loss=0.655, avg val loss=0.589, auc=0.656\n",
      "Best model saved.\n",
      "Iter=145, avg train loss=0.656, avg val loss=0.581, auc=0.660\n",
      "Best model saved.\n",
      "Iter=150, avg train loss=0.658, avg val loss=0.594, auc=0.660\n",
      "Iter=155, avg train loss=0.616, avg val loss=0.602, auc=0.657\n",
      "Iter=160, avg train loss=0.712, avg val loss=0.584, auc=0.667\n",
      "Best model saved.\n",
      "Iter=165, avg train loss=0.713, avg val loss=0.577, auc=0.671\n",
      "Best model saved.\n",
      "Iter=170, avg train loss=0.760, avg val loss=0.588, auc=0.665\n",
      "Iter=175, avg train loss=0.622, avg val loss=0.618, auc=0.657\n",
      "Iter=180, avg train loss=0.618, avg val loss=0.614, auc=0.662\n",
      "Iter=185, avg train loss=0.648, avg val loss=0.601, auc=0.671\n",
      "Iter=190, avg train loss=0.636, avg val loss=0.603, auc=0.673\n",
      "Best model saved.\n",
      "Iter=195, avg train loss=0.635, avg val loss=0.602, auc=0.676\n",
      "Best model saved.\n",
      "Iter=200, avg train loss=0.668, avg val loss=0.615, auc=0.671\n",
      "Iter=205, avg train loss=0.674, avg val loss=0.619, auc=0.669\n",
      "Iter=210, avg train loss=0.641, avg val loss=0.616, auc=0.662\n",
      "Iter=215, avg train loss=0.677, avg val loss=0.604, auc=0.679\n",
      "Best model saved.\n",
      "Iter=220, avg train loss=0.663, avg val loss=0.568, auc=0.685\n",
      "Best model saved.\n",
      "Iter=225, avg train loss=0.685, avg val loss=0.571, auc=0.682\n",
      "Iter=230, avg train loss=0.697, avg val loss=0.602, auc=0.660\n",
      "Iter=235, avg train loss=0.720, avg val loss=0.621, auc=0.658\n",
      "Iter=240, avg train loss=0.701, avg val loss=0.626, auc=0.654\n",
      "Iter=245, avg train loss=0.608, avg val loss=0.618, auc=0.652\n",
      "Iter=250, avg train loss=0.654, avg val loss=0.625, auc=0.662\n",
      "Iter=255, avg train loss=0.602, avg val loss=0.635, auc=0.660\n",
      "Iter=260, avg train loss=0.679, avg val loss=0.590, auc=0.671\n",
      "Iter=265, avg train loss=0.656, avg val loss=0.571, auc=0.677\n",
      "Iter=270, avg train loss=0.665, avg val loss=0.569, auc=0.685\n",
      "Iter=275, avg train loss=0.660, avg val loss=0.565, auc=0.682\n",
      "Iter=280, avg train loss=0.656, avg val loss=0.590, auc=0.672\n",
      "Iter=285, avg train loss=0.576, avg val loss=0.582, auc=0.669\n",
      "Iter=290, avg train loss=0.653, avg val loss=0.551, auc=0.679\n",
      "Iter=295, avg train loss=0.683, avg val loss=0.558, auc=0.679\n",
      "Iter=300, avg train loss=0.633, avg val loss=0.586, auc=0.674\n",
      "Iter=305, avg train loss=0.615, avg val loss=0.588, auc=0.662\n",
      "Iter=310, avg train loss=0.652, avg val loss=0.598, auc=0.672\n",
      "Iter=315, avg train loss=0.736, avg val loss=0.605, auc=0.662\n",
      "Iter=320, avg train loss=0.590, avg val loss=0.617, auc=0.664\n",
      "Iter=325, avg train loss=0.587, avg val loss=0.620, auc=0.658\n",
      "Iter=330, avg train loss=0.650, avg val loss=0.577, auc=0.660\n",
      "Iter=335, avg train loss=0.689, avg val loss=0.605, auc=0.654\n",
      "Iter=340, avg train loss=0.581, avg val loss=0.636, auc=0.648\n",
      "Iter=345, avg train loss=0.592, avg val loss=0.617, auc=0.648\n",
      "Iter=350, avg train loss=0.621, avg val loss=0.608, auc=0.655\n",
      "Iter=355, avg train loss=0.629, avg val loss=0.606, auc=0.653\n",
      "Iter=360, avg train loss=0.702, avg val loss=0.630, auc=0.656\n",
      "Iter=365, avg train loss=0.693, avg val loss=0.625, auc=0.655\n",
      "Iter=370, avg train loss=0.647, avg val loss=0.648, auc=0.649\n",
      "Iter=375, avg train loss=0.638, avg val loss=0.649, auc=0.648\n",
      "Iter=380, avg train loss=0.664, avg val loss=0.614, auc=0.652\n",
      "Iter=385, avg train loss=0.606, avg val loss=0.575, auc=0.666\n",
      "Iter=390, avg train loss=0.632, avg val loss=0.579, auc=0.664\n",
      "Iter=395, avg train loss=0.634, avg val loss=0.578, auc=0.675\n",
      "Iter=400, avg train loss=0.585, avg val loss=0.559, auc=0.675\n",
      "Iter=405, avg train loss=0.633, avg val loss=0.549, auc=0.684\n",
      "Iter=410, avg train loss=0.580, avg val loss=0.545, auc=0.679\n",
      "Iter=415, avg train loss=0.714, avg val loss=0.539, auc=0.688\n",
      "Best model saved.\n",
      "Iter=420, avg train loss=0.623, avg val loss=0.539, auc=0.703\n",
      "Best model saved.\n",
      "Iter=425, avg train loss=0.681, avg val loss=0.570, auc=0.703\n",
      "Best model saved.\n",
      "Iter=430, avg train loss=0.613, avg val loss=0.560, auc=0.700\n",
      "Iter=435, avg train loss=0.672, avg val loss=0.554, auc=0.687\n",
      "Iter=440, avg train loss=0.681, avg val loss=0.581, auc=0.677\n",
      "Iter=445, avg train loss=0.674, avg val loss=0.586, auc=0.685\n",
      "Iter=450, avg train loss=0.590, avg val loss=0.596, auc=0.681\n",
      "Iter=455, avg train loss=0.637, avg val loss=0.597, auc=0.681\n",
      "Iter=460, avg train loss=0.579, avg val loss=0.607, auc=0.676\n",
      "Iter=465, avg train loss=0.617, avg val loss=0.626, auc=0.676\n",
      "Iter=470, avg train loss=0.639, avg val loss=0.647, auc=0.679\n",
      "Iter=475, avg train loss=0.659, avg val loss=0.663, auc=0.678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=480, avg train loss=0.720, avg val loss=0.625, auc=0.684\n",
      "Iter=485, avg train loss=0.638, avg val loss=0.603, auc=0.688\n",
      "Iter=490, avg train loss=0.609, avg val loss=0.568, auc=0.693\n",
      "Iter=495, avg train loss=0.648, avg val loss=0.533, auc=0.694\n",
      "Iter=500, avg train loss=0.586, avg val loss=0.532, auc=0.687\n",
      "Iter=505, avg train loss=0.631, avg val loss=0.527, auc=0.692\n",
      "Iter=510, avg train loss=0.675, avg val loss=0.553, auc=0.680\n",
      "Iter=515, avg train loss=0.648, avg val loss=0.613, auc=0.666\n",
      "Iter=520, avg train loss=0.669, avg val loss=0.632, auc=0.661\n",
      "Iter=525, avg train loss=0.635, avg val loss=0.725, auc=0.634\n",
      "Iter=530, avg train loss=0.646, avg val loss=0.712, auc=0.639\n",
      "Iter=535, avg train loss=0.623, avg val loss=0.664, auc=0.657\n",
      "Iter=540, avg train loss=0.612, avg val loss=0.591, auc=0.665\n",
      "Iter=545, avg train loss=0.500, avg val loss=0.597, auc=0.669\n",
      "Iter=550, avg train loss=0.589, avg val loss=0.590, auc=0.669\n",
      "Iter=555, avg train loss=0.621, avg val loss=0.620, auc=0.660\n",
      "Iter=560, avg train loss=0.560, avg val loss=0.626, auc=0.668\n",
      "Iter=565, avg train loss=0.614, avg val loss=0.585, auc=0.679\n",
      "Iter=570, avg train loss=0.644, avg val loss=0.596, auc=0.673\n",
      "Iter=575, avg train loss=0.588, avg val loss=0.602, auc=0.666\n",
      "Iter=580, avg train loss=0.621, avg val loss=0.601, auc=0.680\n",
      "Iter=585, avg train loss=0.522, avg val loss=0.568, auc=0.686\n",
      "Iter=590, avg train loss=0.451, avg val loss=0.533, auc=0.682\n",
      "Iter=595, avg train loss=0.667, avg val loss=0.538, auc=0.683\n",
      "Iter=600, avg train loss=0.577, avg val loss=0.564, auc=0.687\n",
      "Iter=605, avg train loss=0.606, avg val loss=0.579, auc=0.668\n",
      "Iter=610, avg train loss=0.689, avg val loss=0.588, auc=0.670\n",
      "Iter=615, avg train loss=0.543, avg val loss=0.580, auc=0.666\n",
      "Iter=620, avg train loss=0.576, avg val loss=0.564, auc=0.655\n",
      "Iter=625, avg train loss=0.641, avg val loss=0.583, auc=0.662\n",
      "Iter=630, avg train loss=0.595, avg val loss=0.647, auc=0.667\n",
      "Iter=635, avg train loss=0.520, avg val loss=0.602, auc=0.668\n",
      "Iter=640, avg train loss=0.560, avg val loss=0.590, auc=0.668\n",
      "Iter=645, avg train loss=0.595, avg val loss=0.569, auc=0.672\n",
      "Iter=650, avg train loss=0.607, avg val loss=0.584, auc=0.676\n",
      "Iter=655, avg train loss=0.504, avg val loss=0.590, auc=0.683\n",
      "Iter=660, avg train loss=0.624, avg val loss=0.623, auc=0.684\n",
      "Iter=665, avg train loss=0.527, avg val loss=0.699, auc=0.691\n",
      "Iter=670, avg train loss=0.587, avg val loss=0.681, auc=0.694\n",
      "Iter=675, avg train loss=0.593, avg val loss=0.633, auc=0.693\n",
      "Iter=680, avg train loss=0.535, avg val loss=0.637, auc=0.688\n",
      "Iter=685, avg train loss=0.565, avg val loss=0.524, auc=0.699\n",
      "Iter=690, avg train loss=0.417, avg val loss=0.538, auc=0.693\n",
      "Iter=695, avg train loss=0.614, avg val loss=0.556, auc=0.687\n",
      "Iter=700, avg train loss=0.488, avg val loss=0.572, auc=0.691\n",
      "Iter=705, avg train loss=0.557, avg val loss=0.644, auc=0.673\n",
      "Iter=710, avg train loss=0.636, avg val loss=0.602, auc=0.672\n",
      "Iter=715, avg train loss=0.520, avg val loss=0.635, auc=0.666\n",
      "Iter=720, avg train loss=0.550, avg val loss=0.585, auc=0.662\n",
      "Iter=725, avg train loss=0.536, avg val loss=0.597, auc=0.653\n",
      "Iter=730, avg train loss=0.584, avg val loss=0.598, auc=0.649\n",
      "Iter=735, avg train loss=0.512, avg val loss=0.593, auc=0.643\n",
      "Iter=740, avg train loss=0.570, avg val loss=0.612, auc=0.641\n",
      "Iter=745, avg train loss=0.518, avg val loss=0.628, auc=0.648\n",
      "Iter=750, avg train loss=0.531, avg val loss=0.710, auc=0.665\n",
      "Iter=755, avg train loss=0.449, avg val loss=0.625, auc=0.675\n",
      "Iter=760, avg train loss=0.638, avg val loss=0.672, auc=0.670\n",
      "Iter=765, avg train loss=0.573, avg val loss=0.658, auc=0.665\n",
      "Iter=770, avg train loss=0.558, avg val loss=0.554, auc=0.667\n",
      "Iter=775, avg train loss=0.496, avg val loss=0.557, auc=0.675\n",
      "Iter=780, avg train loss=0.592, avg val loss=0.606, auc=0.682\n",
      "Iter=785, avg train loss=0.577, avg val loss=0.638, auc=0.697\n",
      "Iter=790, avg train loss=0.582, avg val loss=0.636, auc=0.689\n",
      "Iter=795, avg train loss=0.524, avg val loss=0.559, auc=0.686\n",
      "Iter=800, avg train loss=0.500, avg val loss=0.543, auc=0.674\n",
      "Iter=805, avg train loss=0.501, avg val loss=0.546, auc=0.665\n",
      "Iter=810, avg train loss=0.556, avg val loss=0.560, auc=0.665\n",
      "Iter=815, avg train loss=0.492, avg val loss=0.703, auc=0.664\n",
      "Iter=820, avg train loss=0.470, avg val loss=0.789, auc=0.679\n",
      "Iter=825, avg train loss=0.483, avg val loss=0.628, auc=0.686\n",
      "Iter=830, avg train loss=0.479, avg val loss=0.557, auc=0.685\n",
      "Iter=835, avg train loss=0.505, avg val loss=0.596, auc=0.654\n",
      "Iter=840, avg train loss=0.538, avg val loss=0.578, auc=0.649\n",
      "Iter=845, avg train loss=0.429, avg val loss=0.687, auc=0.624\n",
      "Iter=850, avg train loss=0.491, avg val loss=0.733, auc=0.647\n",
      "Iter=855, avg train loss=0.608, avg val loss=0.747, auc=0.671\n",
      "Iter=860, avg train loss=0.542, avg val loss=0.572, auc=0.699\n",
      "Iter=865, avg train loss=0.484, avg val loss=0.538, auc=0.684\n",
      "Iter=870, avg train loss=0.617, avg val loss=0.574, auc=0.668\n",
      "Iter=875, avg train loss=0.703, avg val loss=0.577, auc=0.657\n",
      "Iter=880, avg train loss=0.505, avg val loss=0.584, auc=0.619\n",
      "Iter=885, avg train loss=0.464, avg val loss=0.652, auc=0.586\n",
      "Iter=890, avg train loss=0.465, avg val loss=0.796, auc=0.602\n",
      "Iter=895, avg train loss=0.590, avg val loss=0.814, auc=0.624\n",
      "Iter=900, avg train loss=0.470, avg val loss=0.669, auc=0.640\n",
      "Iter=905, avg train loss=0.582, avg val loss=0.808, auc=0.643\n",
      "Iter=910, avg train loss=0.639, avg val loss=0.817, auc=0.677\n",
      "Iter=915, avg train loss=0.543, avg val loss=0.589, auc=0.675\n",
      "Iter=920, avg train loss=0.429, avg val loss=0.564, auc=0.653\n",
      "Iter=925, avg train loss=0.389, avg val loss=0.579, auc=0.667\n",
      "Iter=930, avg train loss=0.572, avg val loss=0.604, auc=0.660\n",
      "Iter=935, avg train loss=0.436, avg val loss=0.649, auc=0.659\n",
      "Iter=940, avg train loss=0.529, avg val loss=0.600, auc=0.630\n",
      "Iter=945, avg train loss=0.441, avg val loss=0.572, auc=0.644\n",
      "Iter=950, avg train loss=0.445, avg val loss=0.578, auc=0.616\n",
      "Iter=955, avg train loss=0.513, avg val loss=0.584, auc=0.624\n",
      "Iter=960, avg train loss=0.429, avg val loss=0.703, auc=0.643\n",
      "Iter=965, avg train loss=0.413, avg val loss=0.614, auc=0.646\n",
      "Iter=970, avg train loss=0.423, avg val loss=0.654, auc=0.641\n",
      "Iter=975, avg train loss=0.440, avg val loss=0.708, auc=0.641\n",
      "Iter=980, avg train loss=0.493, avg val loss=0.663, auc=0.648\n",
      "Iter=985, avg train loss=0.427, avg val loss=0.587, auc=0.654\n",
      "Iter=990, avg train loss=0.575, avg val loss=0.565, auc=0.645\n",
      "Iter=995, avg train loss=0.454, avg val loss=0.587, auc=0.632\n",
      "Iter=1000, avg train loss=0.357, avg val loss=0.668, auc=0.650\n",
      "Iter=1005, avg train loss=0.471, avg val loss=0.695, auc=0.640\n",
      "Iter=1010, avg train loss=0.504, avg val loss=0.632, auc=0.654\n",
      "Iter=1015, avg train loss=0.364, avg val loss=0.619, auc=0.636\n",
      "Iter=1020, avg train loss=0.333, avg val loss=0.607, auc=0.627\n",
      "Iter=1025, avg train loss=0.326, avg val loss=0.581, auc=0.630\n",
      "Iter=1030, avg train loss=0.529, avg val loss=0.609, auc=0.616\n",
      "Iter=1035, avg train loss=0.479, avg val loss=0.733, auc=0.607\n",
      "Iter=1040, avg train loss=0.494, avg val loss=0.670, auc=0.609\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.655, max-score-based AUC=0.653\n",
      "\n",
      "\n",
      "========== Fold 4 ==========\n",
      "Test AUC at start=0.519, max-score-based AUC=0.543\n",
      "Iter=5, avg train loss=1.230, avg val loss=0.564, auc=0.572\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.951, avg val loss=0.552, auc=0.607\n",
      "Best model saved.\n",
      "Iter=15, avg train loss=0.748, avg val loss=0.554, auc=0.618\n",
      "Best model saved.\n",
      "Iter=20, avg train loss=0.868, avg val loss=0.554, auc=0.615\n",
      "Iter=25, avg train loss=0.817, avg val loss=0.548, auc=0.630\n",
      "Best model saved.\n",
      "Iter=30, avg train loss=0.632, avg val loss=0.559, auc=0.599\n",
      "Iter=35, avg train loss=0.806, avg val loss=0.549, auc=0.625\n",
      "Iter=40, avg train loss=0.858, avg val loss=0.551, auc=0.626\n",
      "Iter=45, avg train loss=0.811, avg val loss=0.558, auc=0.626\n",
      "Iter=50, avg train loss=0.717, avg val loss=0.561, auc=0.618\n",
      "Iter=55, avg train loss=0.726, avg val loss=0.561, auc=0.612\n",
      "Iter=60, avg train loss=0.657, avg val loss=0.576, auc=0.604\n",
      "Iter=65, avg train loss=0.604, avg val loss=0.575, auc=0.603\n",
      "Iter=70, avg train loss=0.736, avg val loss=0.558, auc=0.604\n",
      "Iter=75, avg train loss=0.663, avg val loss=0.565, auc=0.603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=80, avg train loss=0.720, avg val loss=0.567, auc=0.607\n",
      "Iter=85, avg train loss=0.725, avg val loss=0.560, auc=0.603\n",
      "Iter=90, avg train loss=0.768, avg val loss=0.565, auc=0.626\n",
      "Iter=95, avg train loss=0.731, avg val loss=0.575, auc=0.615\n",
      "Iter=100, avg train loss=0.670, avg val loss=0.576, auc=0.631\n",
      "Best model saved.\n",
      "Iter=105, avg train loss=0.654, avg val loss=0.576, auc=0.614\n",
      "Iter=110, avg train loss=0.632, avg val loss=0.578, auc=0.613\n",
      "Iter=115, avg train loss=0.730, avg val loss=0.570, auc=0.606\n",
      "Iter=120, avg train loss=0.660, avg val loss=0.573, auc=0.607\n",
      "Iter=125, avg train loss=0.680, avg val loss=0.572, auc=0.606\n",
      "Iter=130, avg train loss=0.713, avg val loss=0.594, auc=0.616\n",
      "Iter=135, avg train loss=0.675, avg val loss=0.600, auc=0.622\n",
      "Iter=140, avg train loss=0.714, avg val loss=0.600, auc=0.613\n",
      "Iter=145, avg train loss=0.677, avg val loss=0.601, auc=0.616\n",
      "Iter=150, avg train loss=0.646, avg val loss=0.606, auc=0.603\n",
      "Iter=155, avg train loss=0.624, avg val loss=0.605, auc=0.597\n",
      "Iter=160, avg train loss=0.653, avg val loss=0.608, auc=0.596\n",
      "Iter=165, avg train loss=0.663, avg val loss=0.593, auc=0.597\n",
      "Iter=170, avg train loss=0.741, avg val loss=0.590, auc=0.602\n",
      "Iter=175, avg train loss=0.633, avg val loss=0.585, auc=0.612\n",
      "Iter=180, avg train loss=0.643, avg val loss=0.595, auc=0.622\n",
      "Iter=185, avg train loss=0.669, avg val loss=0.588, auc=0.624\n",
      "Iter=190, avg train loss=0.673, avg val loss=0.612, auc=0.638\n",
      "Best model saved.\n",
      "Iter=195, avg train loss=0.626, avg val loss=0.604, auc=0.632\n",
      "Iter=200, avg train loss=0.632, avg val loss=0.614, auc=0.634\n",
      "Iter=205, avg train loss=0.596, avg val loss=0.568, auc=0.623\n",
      "Iter=210, avg train loss=0.636, avg val loss=0.574, auc=0.615\n",
      "Iter=215, avg train loss=0.651, avg val loss=0.572, auc=0.608\n",
      "Iter=220, avg train loss=0.812, avg val loss=0.578, auc=0.598\n",
      "Iter=225, avg train loss=0.619, avg val loss=0.603, auc=0.610\n",
      "Iter=230, avg train loss=0.694, avg val loss=0.606, auc=0.603\n",
      "Iter=235, avg train loss=0.648, avg val loss=0.599, auc=0.601\n",
      "Iter=240, avg train loss=0.621, avg val loss=0.620, auc=0.600\n",
      "Iter=245, avg train loss=0.601, avg val loss=0.638, auc=0.605\n",
      "Iter=250, avg train loss=0.599, avg val loss=0.597, auc=0.593\n",
      "Iter=255, avg train loss=0.702, avg val loss=0.593, auc=0.599\n",
      "Iter=260, avg train loss=0.564, avg val loss=0.595, auc=0.593\n",
      "Iter=265, avg train loss=0.689, avg val loss=0.587, auc=0.601\n",
      "Iter=270, avg train loss=0.708, avg val loss=0.583, auc=0.606\n",
      "Iter=275, avg train loss=0.674, avg val loss=0.579, auc=0.603\n",
      "Iter=280, avg train loss=0.568, avg val loss=0.583, auc=0.601\n",
      "Iter=285, avg train loss=0.617, avg val loss=0.583, auc=0.603\n",
      "Iter=290, avg train loss=0.585, avg val loss=0.596, auc=0.601\n",
      "Iter=295, avg train loss=0.620, avg val loss=0.604, auc=0.605\n",
      "Iter=300, avg train loss=0.623, avg val loss=0.596, auc=0.610\n",
      "Iter=305, avg train loss=0.606, avg val loss=0.613, auc=0.616\n",
      "Iter=310, avg train loss=0.598, avg val loss=0.599, auc=0.620\n",
      "Iter=315, avg train loss=0.567, avg val loss=0.651, auc=0.651\n",
      "Best model saved.\n",
      "Iter=320, avg train loss=0.610, avg val loss=0.606, auc=0.632\n",
      "Iter=325, avg train loss=0.645, avg val loss=0.566, auc=0.614\n",
      "Iter=330, avg train loss=0.691, avg val loss=0.573, auc=0.628\n",
      "Iter=335, avg train loss=0.592, avg val loss=0.603, auc=0.639\n",
      "Iter=340, avg train loss=0.558, avg val loss=0.617, auc=0.645\n",
      "Iter=345, avg train loss=0.583, avg val loss=0.593, auc=0.628\n",
      "Iter=350, avg train loss=0.581, avg val loss=0.581, auc=0.633\n",
      "Iter=355, avg train loss=0.553, avg val loss=0.563, auc=0.626\n",
      "Iter=360, avg train loss=0.653, avg val loss=0.564, auc=0.621\n",
      "Iter=365, avg train loss=0.537, avg val loss=0.581, auc=0.624\n",
      "Iter=370, avg train loss=0.624, avg val loss=0.576, auc=0.630\n",
      "Iter=375, avg train loss=0.717, avg val loss=0.610, auc=0.637\n",
      "Iter=380, avg train loss=0.591, avg val loss=0.608, auc=0.655\n",
      "Best model saved.\n",
      "Iter=385, avg train loss=0.637, avg val loss=0.578, auc=0.636\n",
      "Iter=390, avg train loss=0.627, avg val loss=0.572, auc=0.637\n",
      "Iter=395, avg train loss=0.576, avg val loss=0.561, auc=0.631\n",
      "Iter=400, avg train loss=0.578, avg val loss=0.563, auc=0.637\n",
      "Iter=405, avg train loss=0.535, avg val loss=0.571, auc=0.618\n",
      "Iter=410, avg train loss=0.694, avg val loss=0.573, auc=0.625\n",
      "Iter=415, avg train loss=0.592, avg val loss=0.590, auc=0.630\n",
      "Iter=420, avg train loss=0.627, avg val loss=0.581, auc=0.616\n",
      "Iter=425, avg train loss=0.565, avg val loss=0.580, auc=0.628\n",
      "Iter=430, avg train loss=0.613, avg val loss=0.599, auc=0.634\n",
      "Iter=435, avg train loss=0.521, avg val loss=0.583, auc=0.625\n",
      "Iter=440, avg train loss=0.633, avg val loss=0.593, auc=0.617\n",
      "Iter=445, avg train loss=0.462, avg val loss=0.584, auc=0.604\n",
      "Iter=450, avg train loss=0.573, avg val loss=0.591, auc=0.600\n",
      "Iter=455, avg train loss=0.657, avg val loss=0.598, auc=0.615\n",
      "Iter=460, avg train loss=0.567, avg val loss=0.614, auc=0.604\n",
      "Iter=465, avg train loss=0.574, avg val loss=0.593, auc=0.612\n",
      "Iter=470, avg train loss=0.505, avg val loss=0.609, auc=0.602\n",
      "Iter=475, avg train loss=0.581, avg val loss=0.604, auc=0.604\n",
      "Iter=480, avg train loss=0.507, avg val loss=0.646, auc=0.605\n",
      "Iter=485, avg train loss=0.545, avg val loss=0.661, auc=0.603\n",
      "Iter=490, avg train loss=0.612, avg val loss=0.640, auc=0.578\n",
      "Iter=495, avg train loss=0.624, avg val loss=0.607, auc=0.583\n",
      "Iter=500, avg train loss=0.580, avg val loss=0.603, auc=0.580\n",
      "Iter=505, avg train loss=0.496, avg val loss=0.587, auc=0.593\n",
      "Iter=510, avg train loss=0.526, avg val loss=0.595, auc=0.588\n",
      "Iter=515, avg train loss=0.544, avg val loss=0.604, auc=0.581\n",
      "Iter=520, avg train loss=0.527, avg val loss=0.595, auc=0.581\n",
      "Iter=525, avg train loss=0.549, avg val loss=0.604, auc=0.572\n",
      "Iter=530, avg train loss=0.617, avg val loss=0.629, auc=0.573\n",
      "Iter=535, avg train loss=0.551, avg val loss=0.623, auc=0.588\n",
      "Iter=540, avg train loss=0.527, avg val loss=0.624, auc=0.580\n",
      "Iter=545, avg train loss=0.636, avg val loss=0.604, auc=0.574\n",
      "Iter=550, avg train loss=0.615, avg val loss=0.620, auc=0.555\n",
      "Iter=555, avg train loss=0.534, avg val loss=0.621, auc=0.558\n",
      "Iter=560, avg train loss=0.579, avg val loss=0.634, auc=0.568\n",
      "Iter=565, avg train loss=0.570, avg val loss=0.614, auc=0.574\n",
      "Iter=570, avg train loss=0.544, avg val loss=0.612, auc=0.584\n",
      "Iter=575, avg train loss=0.564, avg val loss=0.622, auc=0.595\n",
      "Iter=580, avg train loss=0.508, avg val loss=0.596, auc=0.602\n",
      "Iter=585, avg train loss=0.568, avg val loss=0.610, auc=0.612\n",
      "Iter=590, avg train loss=0.516, avg val loss=0.600, auc=0.606\n",
      "Iter=595, avg train loss=0.554, avg val loss=0.621, auc=0.595\n",
      "Iter=600, avg train loss=0.475, avg val loss=0.583, auc=0.595\n",
      "Iter=605, avg train loss=0.456, avg val loss=0.586, auc=0.586\n",
      "Iter=610, avg train loss=0.684, avg val loss=0.605, auc=0.576\n",
      "Iter=615, avg train loss=0.542, avg val loss=0.622, auc=0.571\n",
      "Iter=620, avg train loss=0.533, avg val loss=0.639, auc=0.574\n",
      "Iter=625, avg train loss=0.517, avg val loss=0.665, auc=0.572\n",
      "Iter=630, avg train loss=0.434, avg val loss=0.688, auc=0.584\n",
      "Iter=635, avg train loss=0.522, avg val loss=0.711, auc=0.577\n",
      "Iter=640, avg train loss=0.478, avg val loss=0.701, auc=0.568\n",
      "Iter=645, avg train loss=0.528, avg val loss=0.634, auc=0.569\n",
      "Iter=650, avg train loss=0.460, avg val loss=0.647, auc=0.570\n",
      "Iter=655, avg train loss=0.454, avg val loss=0.632, auc=0.573\n",
      "Iter=660, avg train loss=0.590, avg val loss=0.658, auc=0.577\n",
      "Iter=665, avg train loss=0.414, avg val loss=0.674, auc=0.585\n",
      "Iter=670, avg train loss=0.507, avg val loss=0.634, auc=0.594\n",
      "Iter=675, avg train loss=0.447, avg val loss=0.654, auc=0.585\n",
      "Iter=680, avg train loss=0.560, avg val loss=0.650, auc=0.563\n",
      "Iter=685, avg train loss=0.403, avg val loss=0.614, auc=0.570\n",
      "Iter=690, avg train loss=0.514, avg val loss=0.617, auc=0.573\n",
      "Iter=695, avg train loss=0.499, avg val loss=0.626, auc=0.575\n",
      "Iter=700, avg train loss=0.541, avg val loss=0.648, auc=0.583\n",
      "Iter=705, avg train loss=0.450, avg val loss=0.692, auc=0.589\n",
      "Iter=710, avg train loss=0.373, avg val loss=0.624, auc=0.600\n",
      "Iter=715, avg train loss=0.505, avg val loss=0.608, auc=0.589\n",
      "Iter=720, avg train loss=0.526, avg val loss=0.657, auc=0.588\n",
      "Iter=725, avg train loss=0.481, avg val loss=0.636, auc=0.593\n",
      "Iter=730, avg train loss=0.540, avg val loss=0.613, auc=0.597\n",
      "Iter=735, avg train loss=0.556, avg val loss=0.609, auc=0.610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=740, avg train loss=0.470, avg val loss=0.660, auc=0.606\n",
      "Iter=745, avg train loss=0.484, avg val loss=0.730, auc=0.601\n",
      "Iter=750, avg train loss=0.463, avg val loss=0.715, auc=0.607\n",
      "Iter=755, avg train loss=0.452, avg val loss=0.630, auc=0.604\n",
      "Iter=760, avg train loss=0.552, avg val loss=0.607, auc=0.591\n",
      "Iter=765, avg train loss=0.451, avg val loss=0.596, auc=0.586\n",
      "Iter=770, avg train loss=0.512, avg val loss=0.593, auc=0.593\n",
      "Iter=775, avg train loss=0.504, avg val loss=0.624, auc=0.596\n",
      "Iter=780, avg train loss=0.457, avg val loss=0.715, auc=0.594\n",
      "Iter=785, avg train loss=0.561, avg val loss=0.843, auc=0.588\n",
      "Iter=790, avg train loss=0.418, avg val loss=0.690, auc=0.594\n",
      "Iter=795, avg train loss=0.486, avg val loss=0.628, auc=0.588\n",
      "Iter=800, avg train loss=0.494, avg val loss=0.624, auc=0.586\n",
      "Iter=805, avg train loss=0.417, avg val loss=0.659, auc=0.572\n",
      "Iter=810, avg train loss=0.474, avg val loss=0.631, auc=0.583\n",
      "Iter=815, avg train loss=0.448, avg val loss=0.651, auc=0.594\n",
      "Iter=820, avg train loss=0.367, avg val loss=0.693, auc=0.588\n",
      "Iter=825, avg train loss=0.446, avg val loss=0.717, auc=0.571\n",
      "Iter=830, avg train loss=0.413, avg val loss=0.678, auc=0.560\n",
      "Iter=835, avg train loss=0.439, avg val loss=0.633, auc=0.564\n",
      "Iter=840, avg train loss=0.387, avg val loss=0.629, auc=0.572\n",
      "Iter=845, avg train loss=0.527, avg val loss=0.648, auc=0.574\n",
      "Iter=850, avg train loss=0.484, avg val loss=0.647, auc=0.576\n",
      "Iter=855, avg train loss=0.468, avg val loss=0.753, auc=0.553\n",
      "Iter=860, avg train loss=0.471, avg val loss=0.702, auc=0.556\n",
      "Iter=865, avg train loss=0.423, avg val loss=0.658, auc=0.566\n",
      "Iter=870, avg train loss=0.374, avg val loss=0.639, auc=0.579\n",
      "Iter=875, avg train loss=0.339, avg val loss=0.658, auc=0.588\n",
      "Iter=880, avg train loss=0.421, avg val loss=0.678, auc=0.582\n",
      "Iter=885, avg train loss=0.510, avg val loss=0.655, auc=0.578\n",
      "Iter=890, avg train loss=0.408, avg val loss=0.683, auc=0.569\n",
      "Iter=895, avg train loss=0.399, avg val loss=0.683, auc=0.569\n",
      "Iter=900, avg train loss=0.425, avg val loss=0.683, auc=0.572\n",
      "Iter=905, avg train loss=0.390, avg val loss=0.634, auc=0.578\n",
      "Iter=910, avg train loss=0.521, avg val loss=0.615, auc=0.593\n",
      "Iter=915, avg train loss=0.441, avg val loss=0.614, auc=0.586\n",
      "Iter=920, avg train loss=0.328, avg val loss=0.663, auc=0.588\n",
      "Iter=925, avg train loss=0.477, avg val loss=0.640, auc=0.594\n",
      "Iter=930, avg train loss=0.386, avg val loss=0.619, auc=0.592\n",
      "Iter=935, avg train loss=0.407, avg val loss=0.624, auc=0.604\n",
      "Iter=940, avg train loss=0.428, avg val loss=0.634, auc=0.594\n",
      "Iter=945, avg train loss=0.319, avg val loss=0.652, auc=0.594\n",
      "Iter=950, avg train loss=0.337, avg val loss=0.675, auc=0.596\n",
      "Iter=955, avg train loss=0.402, avg val loss=0.667, auc=0.600\n",
      "Iter=960, avg train loss=0.386, avg val loss=0.655, auc=0.608\n",
      "Iter=965, avg train loss=0.309, avg val loss=0.675, auc=0.598\n",
      "Iter=970, avg train loss=0.315, avg val loss=0.644, auc=0.592\n",
      "Iter=975, avg train loss=0.314, avg val loss=0.647, auc=0.583\n",
      "Iter=980, avg train loss=0.398, avg val loss=0.640, auc=0.586\n",
      "Iter=985, avg train loss=0.303, avg val loss=0.723, auc=0.583\n",
      "Iter=990, avg train loss=0.374, avg val loss=0.674, auc=0.588\n",
      "Iter=995, avg train loss=0.402, avg val loss=0.725, auc=0.586\n",
      "Iter=1000, avg train loss=0.332, avg val loss=0.834, auc=0.589\n",
      "Iter=1005, avg train loss=0.367, avg val loss=0.668, auc=0.587\n",
      "Iter=1010, avg train loss=0.385, avg val loss=0.711, auc=0.582\n",
      "Iter=1015, avg train loss=0.286, avg val loss=0.724, auc=0.592\n",
      "Iter=1020, avg train loss=0.281, avg val loss=0.754, auc=0.590\n",
      "Iter=1025, avg train loss=0.410, avg val loss=0.691, auc=0.591\n",
      "Iter=1030, avg train loss=0.340, avg val loss=0.642, auc=0.591\n",
      "Iter=1035, avg train loss=0.327, avg val loss=0.661, auc=0.578\n",
      "Iter=1040, avg train loss=0.321, avg val loss=0.674, auc=0.579\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.535, max-score-based AUC=0.588\n",
      "\n",
      "\n",
      "========== Fold 5 ==========\n",
      "Test AUC at start=0.493, max-score-based AUC=0.603\n",
      "Iter=5, avg train loss=1.380, avg val loss=0.615, auc=0.450\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.777, avg val loss=0.621, auc=0.456\n",
      "Best model saved.\n",
      "Iter=15, avg train loss=0.791, avg val loss=0.620, auc=0.459\n",
      "Best model saved.\n",
      "Iter=20, avg train loss=0.897, avg val loss=0.612, auc=0.470\n",
      "Best model saved.\n",
      "Iter=25, avg train loss=0.714, avg val loss=0.608, auc=0.472\n",
      "Best model saved.\n",
      "Iter=30, avg train loss=0.696, avg val loss=0.603, auc=0.495\n",
      "Best model saved.\n",
      "Iter=35, avg train loss=1.000, avg val loss=0.612, auc=0.487\n",
      "Iter=40, avg train loss=0.814, avg val loss=0.622, auc=0.466\n",
      "Iter=45, avg train loss=0.703, avg val loss=0.616, auc=0.483\n",
      "Iter=50, avg train loss=0.689, avg val loss=0.658, auc=0.488\n",
      "Iter=55, avg train loss=0.715, avg val loss=0.629, auc=0.466\n",
      "Iter=60, avg train loss=0.779, avg val loss=0.657, auc=0.462\n",
      "Iter=65, avg train loss=0.763, avg val loss=0.658, auc=0.487\n",
      "Iter=70, avg train loss=0.657, avg val loss=0.670, auc=0.478\n",
      "Iter=75, avg train loss=0.697, avg val loss=0.694, auc=0.477\n",
      "Iter=80, avg train loss=0.712, avg val loss=0.680, auc=0.486\n",
      "Iter=85, avg train loss=0.701, avg val loss=0.669, auc=0.487\n",
      "Iter=90, avg train loss=0.734, avg val loss=0.661, auc=0.501\n",
      "Best model saved.\n",
      "Iter=95, avg train loss=0.694, avg val loss=0.649, auc=0.512\n",
      "Best model saved.\n",
      "Iter=100, avg train loss=0.696, avg val loss=0.676, auc=0.499\n",
      "Iter=105, avg train loss=0.670, avg val loss=0.632, auc=0.522\n",
      "Best model saved.\n",
      "Iter=110, avg train loss=0.656, avg val loss=0.638, auc=0.529\n",
      "Best model saved.\n",
      "Iter=115, avg train loss=0.693, avg val loss=0.635, auc=0.535\n",
      "Best model saved.\n",
      "Iter=120, avg train loss=0.689, avg val loss=0.620, auc=0.538\n",
      "Best model saved.\n",
      "Iter=125, avg train loss=0.671, avg val loss=0.615, auc=0.541\n",
      "Best model saved.\n",
      "Iter=130, avg train loss=0.731, avg val loss=0.638, auc=0.540\n",
      "Iter=135, avg train loss=0.658, avg val loss=0.646, auc=0.532\n",
      "Iter=140, avg train loss=0.673, avg val loss=0.649, auc=0.525\n",
      "Iter=145, avg train loss=0.677, avg val loss=0.622, auc=0.540\n",
      "Iter=150, avg train loss=0.603, avg val loss=0.652, auc=0.542\n",
      "Best model saved.\n",
      "Iter=155, avg train loss=0.655, avg val loss=0.623, auc=0.542\n",
      "Best model saved.\n",
      "Iter=160, avg train loss=0.550, avg val loss=0.604, auc=0.545\n",
      "Best model saved.\n",
      "Iter=165, avg train loss=0.701, avg val loss=0.615, auc=0.550\n",
      "Best model saved.\n",
      "Iter=170, avg train loss=0.714, avg val loss=0.650, auc=0.550\n",
      "Iter=175, avg train loss=0.606, avg val loss=0.630, auc=0.557\n",
      "Best model saved.\n",
      "Iter=180, avg train loss=0.728, avg val loss=0.651, auc=0.570\n",
      "Best model saved.\n",
      "Iter=185, avg train loss=0.567, avg val loss=0.652, auc=0.579\n",
      "Best model saved.\n",
      "Iter=190, avg train loss=0.650, avg val loss=0.634, auc=0.577\n",
      "Iter=195, avg train loss=0.679, avg val loss=0.625, auc=0.570\n",
      "Iter=200, avg train loss=0.654, avg val loss=0.621, auc=0.572\n",
      "Iter=205, avg train loss=0.656, avg val loss=0.633, auc=0.574\n",
      "Iter=210, avg train loss=0.693, avg val loss=0.632, auc=0.569\n",
      "Iter=215, avg train loss=0.647, avg val loss=0.631, auc=0.575\n",
      "Iter=220, avg train loss=0.645, avg val loss=0.641, auc=0.576\n",
      "Iter=225, avg train loss=0.592, avg val loss=0.610, auc=0.577\n",
      "Iter=230, avg train loss=0.766, avg val loss=0.609, auc=0.578\n",
      "Iter=235, avg train loss=0.615, avg val loss=0.609, auc=0.577\n",
      "Iter=240, avg train loss=0.713, avg val loss=0.611, auc=0.580\n",
      "Best model saved.\n",
      "Iter=245, avg train loss=0.652, avg val loss=0.626, auc=0.581\n",
      "Best model saved.\n",
      "Iter=250, avg train loss=0.678, avg val loss=0.654, auc=0.580\n",
      "Iter=255, avg train loss=0.660, avg val loss=0.653, auc=0.556\n",
      "Iter=260, avg train loss=0.627, avg val loss=0.627, auc=0.576\n",
      "Iter=265, avg train loss=0.685, avg val loss=0.675, auc=0.556\n",
      "Iter=270, avg train loss=0.606, avg val loss=0.638, auc=0.570\n",
      "Iter=275, avg train loss=0.687, avg val loss=0.626, auc=0.562\n",
      "Iter=280, avg train loss=0.640, avg val loss=0.637, auc=0.570\n",
      "Iter=285, avg train loss=0.686, avg val loss=0.685, auc=0.558\n",
      "Iter=290, avg train loss=0.571, avg val loss=0.729, auc=0.555\n",
      "Iter=295, avg train loss=0.654, avg val loss=0.703, auc=0.563\n",
      "Iter=300, avg train loss=0.673, avg val loss=0.716, auc=0.538\n",
      "Iter=305, avg train loss=0.687, avg val loss=0.703, auc=0.541\n",
      "Iter=310, avg train loss=0.564, avg val loss=0.683, auc=0.561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=315, avg train loss=0.550, avg val loss=0.654, auc=0.557\n",
      "Iter=320, avg train loss=0.657, avg val loss=0.673, auc=0.549\n",
      "Iter=325, avg train loss=0.562, avg val loss=0.653, auc=0.558\n",
      "Iter=330, avg train loss=0.558, avg val loss=0.638, auc=0.548\n",
      "Iter=335, avg train loss=0.582, avg val loss=0.667, auc=0.551\n",
      "Iter=340, avg train loss=0.617, avg val loss=0.657, auc=0.548\n",
      "Iter=345, avg train loss=0.548, avg val loss=0.642, auc=0.553\n",
      "Iter=350, avg train loss=0.571, avg val loss=0.634, auc=0.556\n",
      "Iter=355, avg train loss=0.688, avg val loss=0.655, auc=0.551\n",
      "Iter=360, avg train loss=0.444, avg val loss=0.655, auc=0.560\n",
      "Iter=365, avg train loss=0.723, avg val loss=0.653, auc=0.565\n",
      "Iter=370, avg train loss=0.632, avg val loss=0.684, auc=0.551\n",
      "Iter=375, avg train loss=0.587, avg val loss=0.666, auc=0.532\n",
      "Iter=380, avg train loss=0.554, avg val loss=0.677, auc=0.514\n",
      "Iter=385, avg train loss=0.648, avg val loss=0.629, auc=0.502\n",
      "Iter=390, avg train loss=0.599, avg val loss=0.627, auc=0.484\n",
      "Iter=395, avg train loss=0.689, avg val loss=0.635, auc=0.491\n",
      "Iter=400, avg train loss=0.716, avg val loss=0.689, auc=0.516\n",
      "Iter=405, avg train loss=0.626, avg val loss=0.740, auc=0.531\n",
      "Iter=410, avg train loss=0.594, avg val loss=0.741, auc=0.546\n",
      "Iter=415, avg train loss=0.605, avg val loss=0.717, auc=0.563\n",
      "Iter=420, avg train loss=0.626, avg val loss=0.690, auc=0.583\n",
      "Best model saved.\n",
      "Iter=425, avg train loss=0.719, avg val loss=0.702, auc=0.620\n",
      "Best model saved.\n",
      "Iter=430, avg train loss=0.659, avg val loss=0.628, auc=0.615\n",
      "Iter=435, avg train loss=0.543, avg val loss=0.599, auc=0.619\n",
      "Iter=440, avg train loss=0.666, avg val loss=0.619, auc=0.588\n",
      "Iter=445, avg train loss=0.513, avg val loss=0.627, auc=0.571\n",
      "Iter=450, avg train loss=0.605, avg val loss=0.628, auc=0.561\n",
      "Iter=455, avg train loss=0.595, avg val loss=0.618, auc=0.552\n",
      "Iter=460, avg train loss=0.651, avg val loss=0.624, auc=0.547\n",
      "Iter=465, avg train loss=0.519, avg val loss=0.639, auc=0.588\n",
      "Iter=470, avg train loss=0.639, avg val loss=0.633, auc=0.588\n",
      "Iter=475, avg train loss=0.608, avg val loss=0.639, auc=0.567\n",
      "Iter=480, avg train loss=0.649, avg val loss=0.648, auc=0.551\n",
      "Iter=485, avg train loss=0.472, avg val loss=0.634, auc=0.539\n",
      "Iter=490, avg train loss=0.556, avg val loss=0.641, auc=0.529\n",
      "Iter=495, avg train loss=0.593, avg val loss=0.651, auc=0.526\n",
      "Iter=500, avg train loss=0.622, avg val loss=0.653, auc=0.553\n",
      "Iter=505, avg train loss=0.572, avg val loss=0.691, auc=0.569\n",
      "Iter=510, avg train loss=0.595, avg val loss=0.663, auc=0.584\n",
      "Iter=515, avg train loss=0.553, avg val loss=0.710, auc=0.579\n",
      "Iter=520, avg train loss=0.522, avg val loss=0.742, auc=0.593\n",
      "Iter=525, avg train loss=0.592, avg val loss=0.689, auc=0.583\n",
      "Iter=530, avg train loss=0.600, avg val loss=0.711, auc=0.558\n",
      "Iter=535, avg train loss=0.509, avg val loss=0.780, auc=0.553\n",
      "Iter=540, avg train loss=0.604, avg val loss=0.667, auc=0.525\n",
      "Iter=545, avg train loss=0.499, avg val loss=0.632, auc=0.525\n",
      "Iter=550, avg train loss=0.578, avg val loss=0.633, auc=0.518\n",
      "Iter=555, avg train loss=0.567, avg val loss=0.662, auc=0.519\n",
      "Iter=560, avg train loss=0.433, avg val loss=0.729, auc=0.510\n",
      "Iter=565, avg train loss=0.645, avg val loss=0.792, auc=0.519\n",
      "Iter=570, avg train loss=0.540, avg val loss=0.744, auc=0.530\n",
      "Iter=575, avg train loss=0.496, avg val loss=0.788, auc=0.523\n",
      "Iter=580, avg train loss=0.542, avg val loss=0.751, auc=0.503\n",
      "Iter=585, avg train loss=0.486, avg val loss=0.733, auc=0.501\n",
      "Iter=590, avg train loss=0.534, avg val loss=0.775, auc=0.493\n",
      "Iter=595, avg train loss=0.608, avg val loss=0.760, auc=0.494\n",
      "Iter=600, avg train loss=0.436, avg val loss=0.780, auc=0.512\n",
      "Iter=605, avg train loss=0.589, avg val loss=0.842, auc=0.539\n",
      "Iter=610, avg train loss=0.514, avg val loss=0.738, auc=0.526\n",
      "Iter=615, avg train loss=0.555, avg val loss=0.779, auc=0.520\n",
      "Iter=620, avg train loss=0.454, avg val loss=0.834, auc=0.547\n",
      "Iter=625, avg train loss=0.554, avg val loss=0.883, auc=0.556\n",
      "Iter=630, avg train loss=0.612, avg val loss=0.776, auc=0.531\n",
      "Iter=635, avg train loss=0.524, avg val loss=0.693, auc=0.514\n",
      "Iter=640, avg train loss=0.572, avg val loss=0.698, auc=0.509\n",
      "Iter=645, avg train loss=0.566, avg val loss=0.705, auc=0.504\n",
      "Iter=650, avg train loss=0.610, avg val loss=0.771, auc=0.523\n",
      "Iter=655, avg train loss=0.553, avg val loss=0.834, auc=0.544\n",
      "Iter=660, avg train loss=0.491, avg val loss=0.762, auc=0.555\n",
      "Iter=665, avg train loss=0.507, avg val loss=0.744, auc=0.572\n",
      "Iter=670, avg train loss=0.637, avg val loss=0.753, auc=0.566\n",
      "Iter=675, avg train loss=0.422, avg val loss=0.656, auc=0.579\n",
      "Iter=680, avg train loss=0.602, avg val loss=0.735, auc=0.580\n",
      "Iter=685, avg train loss=0.500, avg val loss=0.700, auc=0.580\n",
      "Iter=690, avg train loss=0.553, avg val loss=0.732, auc=0.587\n",
      "Iter=695, avg train loss=0.553, avg val loss=0.735, auc=0.569\n",
      "Iter=700, avg train loss=0.569, avg val loss=0.729, auc=0.552\n",
      "Iter=705, avg train loss=0.476, avg val loss=0.678, auc=0.546\n",
      "Iter=710, avg train loss=0.461, avg val loss=0.699, auc=0.560\n",
      "Iter=715, avg train loss=0.622, avg val loss=0.693, auc=0.553\n",
      "Iter=720, avg train loss=0.467, avg val loss=0.678, auc=0.528\n",
      "Iter=725, avg train loss=0.603, avg val loss=0.645, auc=0.564\n",
      "Iter=730, avg train loss=0.615, avg val loss=0.657, auc=0.557\n",
      "Iter=735, avg train loss=0.549, avg val loss=0.672, auc=0.570\n",
      "Iter=740, avg train loss=0.483, avg val loss=0.686, auc=0.545\n",
      "Iter=745, avg train loss=0.398, avg val loss=0.681, auc=0.553\n",
      "Iter=750, avg train loss=0.481, avg val loss=0.660, auc=0.562\n",
      "Iter=755, avg train loss=0.419, avg val loss=0.701, auc=0.564\n",
      "Iter=760, avg train loss=0.481, avg val loss=0.712, auc=0.573\n",
      "Iter=765, avg train loss=0.578, avg val loss=0.734, auc=0.570\n",
      "Iter=770, avg train loss=0.455, avg val loss=0.776, auc=0.584\n",
      "Iter=775, avg train loss=0.429, avg val loss=0.799, auc=0.586\n",
      "Iter=780, avg train loss=0.547, avg val loss=0.788, auc=0.580\n",
      "Iter=785, avg train loss=0.474, avg val loss=0.709, auc=0.573\n",
      "Iter=790, avg train loss=0.502, avg val loss=0.707, auc=0.534\n",
      "Iter=795, avg train loss=0.381, avg val loss=0.733, auc=0.567\n",
      "Iter=800, avg train loss=0.589, avg val loss=0.734, auc=0.559\n",
      "Iter=805, avg train loss=0.529, avg val loss=0.833, auc=0.530\n",
      "Iter=810, avg train loss=0.505, avg val loss=0.768, auc=0.537\n",
      "Iter=815, avg train loss=0.425, avg val loss=0.700, auc=0.529\n",
      "Iter=820, avg train loss=0.424, avg val loss=0.668, auc=0.503\n",
      "Iter=825, avg train loss=0.425, avg val loss=0.760, auc=0.491\n",
      "Iter=830, avg train loss=0.510, avg val loss=0.830, auc=0.514\n",
      "Iter=835, avg train loss=0.411, avg val loss=0.865, auc=0.505\n",
      "Iter=840, avg train loss=0.539, avg val loss=0.977, auc=0.541\n",
      "Iter=845, avg train loss=0.496, avg val loss=0.898, auc=0.549\n",
      "Iter=850, avg train loss=0.423, avg val loss=0.822, auc=0.560\n",
      "Iter=855, avg train loss=0.379, avg val loss=0.807, auc=0.554\n",
      "Iter=860, avg train loss=0.562, avg val loss=0.735, auc=0.565\n",
      "Iter=865, avg train loss=0.452, avg val loss=0.759, auc=0.562\n",
      "Iter=870, avg train loss=0.562, avg val loss=0.761, auc=0.561\n",
      "Iter=875, avg train loss=0.362, avg val loss=0.710, auc=0.551\n",
      "Iter=880, avg train loss=0.477, avg val loss=0.682, auc=0.538\n",
      "Iter=885, avg train loss=0.433, avg val loss=0.673, auc=0.539\n",
      "Iter=890, avg train loss=0.448, avg val loss=0.717, auc=0.559\n",
      "Iter=895, avg train loss=0.503, avg val loss=0.736, auc=0.542\n",
      "Iter=900, avg train loss=0.458, avg val loss=0.724, auc=0.539\n",
      "Iter=905, avg train loss=0.487, avg val loss=0.720, auc=0.526\n",
      "Iter=910, avg train loss=0.403, avg val loss=0.704, auc=0.524\n",
      "Iter=915, avg train loss=0.446, avg val loss=0.728, auc=0.514\n",
      "Iter=920, avg train loss=0.473, avg val loss=0.763, auc=0.518\n",
      "Iter=925, avg train loss=0.404, avg val loss=0.721, auc=0.537\n",
      "Iter=930, avg train loss=0.393, avg val loss=0.717, auc=0.541\n",
      "Iter=935, avg train loss=0.458, avg val loss=0.761, auc=0.542\n",
      "Iter=940, avg train loss=0.403, avg val loss=0.848, auc=0.546\n",
      "Iter=945, avg train loss=0.404, avg val loss=0.819, auc=0.539\n",
      "Iter=950, avg train loss=0.425, avg val loss=0.907, auc=0.544\n",
      "Iter=955, avg train loss=0.519, avg val loss=0.792, auc=0.536\n",
      "Iter=960, avg train loss=0.579, avg val loss=0.740, auc=0.529\n",
      "Iter=965, avg train loss=0.538, avg val loss=0.683, auc=0.537\n",
      "Iter=970, avg train loss=0.355, avg val loss=0.742, auc=0.552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=975, avg train loss=0.402, avg val loss=0.934, auc=0.561\n",
      "Iter=980, avg train loss=0.359, avg val loss=0.915, auc=0.568\n",
      "Iter=985, avg train loss=0.383, avg val loss=0.728, auc=0.537\n",
      "Iter=990, avg train loss=0.470, avg val loss=0.728, auc=0.518\n",
      "Iter=995, avg train loss=0.422, avg val loss=0.790, auc=0.529\n",
      "Iter=1000, avg train loss=0.420, avg val loss=0.954, auc=0.536\n",
      "Iter=1005, avg train loss=0.557, avg val loss=0.926, auc=0.547\n",
      "Iter=1010, avg train loss=0.382, avg val loss=0.870, auc=0.547\n",
      "Iter=1015, avg train loss=0.426, avg val loss=0.717, auc=0.558\n",
      "Iter=1020, avg train loss=0.324, avg val loss=0.701, auc=0.562\n",
      "Iter=1025, avg train loss=0.489, avg val loss=0.687, auc=0.558\n",
      "Iter=1030, avg train loss=0.483, avg val loss=0.655, auc=0.562\n",
      "Iter=1035, avg train loss=0.331, avg val loss=0.681, auc=0.575\n",
      "Iter=1040, avg train loss=0.442, avg val loss=0.746, auc=0.592\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.626, max-score-based AUC=0.663\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "cpu_threads = 4\n",
    "batch_size = 4\n",
    "n_folds = 5\n",
    "epochs = 20\n",
    "subject_pool, exam_pool = [], []\n",
    "pred_pool, label_pool, machine_pool = [], [], []\n",
    "age_pool, race_pool, bmi_pool = [], [], []\n",
    "birads_pool, libra_pool = [], []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=12345)\n",
    "fold = 0\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t1p), 1)), ys_t1p):\n",
    "    fold += 1\n",
    "    # train-val-test idx.\n",
    "    train_y = ys_t1p[train_ix_]\n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(\n",
    "        train_ix_, test_size=test_prop, \n",
    "        stratify=train_y, random_state=12345)\n",
    "    # subset.\n",
    "    train_dataset = Subset(risk_dataset_t1p, train_ix)\n",
    "    val_dataset = Subset(risk_dataset_t1p, val_ix)\n",
    "    test_dataset = Subset(risk_dataset_t1p, test_ix)\n",
    "    train_y = ys_t1p[train_ix]\n",
    "    val_y = ys_t1p[val_ix]\n",
    "    test_y = ys_t1p[test_ix]\n",
    "    # weighted sampler.\n",
    "    f0, f1 = np.bincount(train_y)\n",
    "    train_w = np.zeros_like(train_y, dtype='float')\n",
    "    train_w[train_y==0] = 1/f0\n",
    "    train_w[train_y==1] = 1/f1\n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        train_w, len(train_y)//batch_size*batch_size, \n",
    "        replacement=True)\n",
    "    # data loaders.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "        collate_fn=mammo_collate)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    \n",
    "    image_only_parameters_rcc = shared_parameters.copy()\n",
    "    image_only_parameters_rcc[\"view\"] = \"R-CC\"\n",
    "    image_only_parameters_rcc[\"use_heatmaps\"] = False\n",
    "    image_only_parameters_rcc[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "    \n",
    "    image_only_parameters_rmlo = shared_parameters.copy()\n",
    "    image_only_parameters_rmlo[\"view\"] = \"R-MLO\"\n",
    "    image_only_parameters_rmlo[\"use_heatmaps\"] = False\n",
    "    image_only_parameters_rmlo[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "    \n",
    "    image_only_parameters_lmlo = shared_parameters.copy()\n",
    "    image_only_parameters_lmlo[\"view\"] = \"L-MLO\"\n",
    "    image_only_parameters_lmlo[\"use_heatmaps\"] = False\n",
    "    image_only_parameters_lmlo[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "    \n",
    "    # Reset model before training.\n",
    "    model, device_ = load_model(image_only_parameters_rcc)\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # train & test.\n",
    "    best_name_ = 'best_model_{}_t1p_second_run_rcc.pt'.format(fold)\n",
    "    print('='*10, 'Fold', fold, '='*10)\n",
    "    _, start_auc = val_loss(model, test_loader, device, return_auc=True)\n",
    "    start_auc_m = test_max_auc(model, test_loader, device)\n",
    "    \n",
    "    print('Test AUC at start={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        start_auc, start_auc_m))\n",
    "    \n",
    "    train(model, train_loader, val_loader, best_name_, device, \n",
    "          epochs=epochs, lr=1e-5, check_iters=5, log_name='finetune_t1p_second_run.txt')\n",
    "    \n",
    "    print('Predicting on the test set...', end='')\n",
    "    subject_list, exam_list, \\\n",
    "    pred_list, label_list, machine_list, \\\n",
    "    age_list, race_list, bmi_list, \\\n",
    "    birads_list, libra_list = do_test(model, test_loader, device)\n",
    "    subject_pool.extend(subject_list)\n",
    "    exam_pool.extend(exam_list)\n",
    "    pred_pool.extend(pred_list)\n",
    "    label_pool.extend(label_list)\n",
    "    machine_pool.extend(machine_list)\n",
    "    age_pool.extend(age_list)\n",
    "    race_pool.extend(race_list)\n",
    "    bmi_pool.extend(bmi_list)\n",
    "    birads_pool.extend(birads_list)\n",
    "    libra_pool.extend(libra_list) \n",
    "    # test AUC.\n",
    "    _, fold_auc = val_loss(model, test_loader, device, return_auc=True)\n",
    "    fold_auc_m = test_max_auc(model, test_loader, device)\n",
    "    print('Done')\n",
    "    print('Test AUC after training={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        fold_auc, fold_auc_m))\n",
    "    if fold < n_folds:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_t1p_lcc = pd.read_csv('../time_set/finetuned_pred_score_4view_T1p_nyu_single_view_train_lcc.csv').iloc[:, [7,8,9,10]].values\n",
    "all_preds_t1p_rcc = pd.read_csv('../time_set/finetuned_pred_score_4view_T1p_nyu_single_view_train_rcc.csv').iloc[:, [7,8,9,10]].values\n",
    "all_preds_t1p_rmlo = pd.read_csv('../time_set/finetuned_pred_score_4view_T1p_nyu_single_view_train_rmlo.csv').iloc[:, [7,8,9,10]].values\n",
    "all_preds_t1p_lmlo = pd.read_csv('../time_set/finetuned_pred_score_4view_T1p_nyu_single_view_train_lmlo.csv').iloc[:, [7,8,9,10]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmlo [[0.161507   0.12555528 0.29154965 0.25918138]\n",
      " [0.11341159 0.24658446 0.03688212 0.17674343]\n",
      " [0.20569111 0.31027606 0.15116781 0.2578532 ]\n",
      " ...\n",
      " [0.44915888 0.57008106 0.44848678 0.51559114]\n",
      " [0.41500372 0.49287605 0.49738955 0.55796516]\n",
      " [0.38368124 0.56306803 0.2826867  0.55068547]]\n",
      "lcc [[0.4191546  0.29541674 0.29511645 0.2852797 ]\n",
      " [0.4352625  0.6389712  0.71996    0.8016393 ]\n",
      " [0.2617122  0.52182364 0.18723193 0.38924935]\n",
      " ...\n",
      " [0.66128916 0.7769715  0.43863666 0.7862541 ]\n",
      " [0.4700572  0.30787483 0.6807544  0.6381429 ]\n",
      " [0.5210918  0.814318   0.45061827 0.29923132]]\n",
      "rcc [[0.31407186 0.18945521 0.21102    0.30241737]\n",
      " [0.3918826  0.56022084 0.21704444 0.23600988]\n",
      " [0.16811895 0.23955213 0.10792593 0.23020208]\n",
      " ...\n",
      " [0.58767843 0.6859469  0.5301991  0.67353046]\n",
      " [0.5459163  0.56371874 0.61010057 0.6019133 ]\n",
      " [0.48880228 0.60398495 0.2575049  0.4818083 ]]\n",
      "rmlo [[0.40923637 0.1820302  0.47830078 0.38353345]\n",
      " [0.21005958 0.52581424 0.19649127 0.6147708 ]\n",
      " [0.31063914 0.6892659  0.30994087 0.45940143]\n",
      " ...\n",
      " [0.31757307 0.47201443 0.55460703 0.44394672]\n",
      " [0.3481073  0.32392326 0.55523306 0.47395313]\n",
      " [0.14177659 0.40390745 0.18202902 0.43446687]]\n"
     ]
    }
   ],
   "source": [
    "print('lmlo', all_preds_t1p_lmlo)\n",
    "print('lcc', all_preds_t1p_lcc)\n",
    "print('rcc', all_preds_t1p_rcc)\n",
    "print('rmlo', all_preds_t1p_rmlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_preds_t1p = (all_preds_t1p_lcc + all_preds_t1p_rcc + all_preds_t1p_rmlo + all_preds_t1p_lmlo) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36661323, 0.24243598, 0.25306822, 0.29384854],\n",
       "       [0.41357255, 0.59959602, 0.46850222, 0.51882459],\n",
       "       [0.21491557, 0.38068789, 0.14757893, 0.30972572],\n",
       "       ...,\n",
       "       [0.62448379, 0.7314592 , 0.48441788, 0.72989228],\n",
       "       [0.50798675, 0.43579678, 0.64542748, 0.6200281 ],\n",
       "       [0.50494704, 0.70915147, 0.35406158, 0.39051981]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((all_preds_t1p_lcc + all_preds_t1p_rcc) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=352\n",
      "4view max AUC=0.601\n",
      "4view mean AUC=0.603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1p_rcc)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_rmlo, avg_preds_t1p.max(1))))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_rmlo, avg_preds_t1p.mean(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=352\n",
      "4view max AUC=0.608\n",
      "4view mean AUC=0.618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1p_rcc)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_rmlo, ((all_preds_t1p + all_preds_t1p_rcc) / 2).max(1))))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_rmlo, ((all_preds_t1p + all_preds_t1p_rcc) / 2).mean(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1p_rcc = np.concatenate(subject_pool)\n",
    "all_exam_t1p_rcc = np.concatenate(exam_pool)\n",
    "all_preds_t1p_rcc = torch.cat(pred_pool)\n",
    "all_labels_t1p_rcc = torch.cat(label_pool)\n",
    "all_preds_t1p_rcc = all_preds_t1p_rcc.cpu().numpy()\n",
    "all_labels_t1p_rcc = all_labels_t1p_rcc.numpy()\n",
    "all_probs_max_t1p_rcc = all_preds_t1p_rcc.max(1)\n",
    "all_machines_t1p_rcc = np.concatenate(machine_pool)\n",
    "all_ages_t1p_rcc = np.concatenate(age_pool)\n",
    "all_races_t1p_rcc = np.concatenate(race_pool)\n",
    "all_bmis_t1p_rcc = np.concatenate(bmi_pool)\n",
    "all_birads_t1p_rcc = np.concatenate(birads_pool)\n",
    "all_libras_t1p_rcc = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject  exam  is_ge  age race        bmi birads\n",
      "0     9112     1      1   48    6  29.679908      b\n",
      "1      180     1      1   63    8  21.173437      b\n",
      "2    35533     1      1   82    8  33.105904      b\n",
      "3    43781     1      1   57    8  22.205153      a\n",
      "4    45170     1      1   64    1  23.643988      b\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "      <th>ips-cc</th>\n",
       "      <th>ips-mlo</th>\n",
       "      <th>contra-cc</th>\n",
       "      <th>contra-mlo</th>\n",
       "      <th>is_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "      <td>0.314072</td>\n",
       "      <td>0.189455</td>\n",
       "      <td>0.211020</td>\n",
       "      <td>0.302417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>21.173437</td>\n",
       "      <td>b</td>\n",
       "      <td>0.391883</td>\n",
       "      <td>0.560221</td>\n",
       "      <td>0.217044</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35533</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>33.105904</td>\n",
       "      <td>b</td>\n",
       "      <td>0.168119</td>\n",
       "      <td>0.239552</td>\n",
       "      <td>0.107926</td>\n",
       "      <td>0.230202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43781</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>22.205153</td>\n",
       "      <td>a</td>\n",
       "      <td>0.501777</td>\n",
       "      <td>0.338702</td>\n",
       "      <td>0.223229</td>\n",
       "      <td>0.150545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "      <td>0.744803</td>\n",
       "      <td>0.672566</td>\n",
       "      <td>0.198790</td>\n",
       "      <td>0.271363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>283018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>26.046094</td>\n",
       "      <td>b</td>\n",
       "      <td>0.585775</td>\n",
       "      <td>0.586799</td>\n",
       "      <td>0.576758</td>\n",
       "      <td>0.603956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>285187</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>27.935803</td>\n",
       "      <td>c</td>\n",
       "      <td>0.491542</td>\n",
       "      <td>0.668285</td>\n",
       "      <td>0.580874</td>\n",
       "      <td>0.647027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>938055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>29.900549</td>\n",
       "      <td>b</td>\n",
       "      <td>0.587678</td>\n",
       "      <td>0.685947</td>\n",
       "      <td>0.530199</td>\n",
       "      <td>0.673530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>288571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>8</td>\n",
       "      <td>34.922548</td>\n",
       "      <td>b</td>\n",
       "      <td>0.545916</td>\n",
       "      <td>0.563719</td>\n",
       "      <td>0.610101</td>\n",
       "      <td>0.601913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>962869</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>33.645150</td>\n",
       "      <td>b</td>\n",
       "      <td>0.488802</td>\n",
       "      <td>0.603985</td>\n",
       "      <td>0.257505</td>\n",
       "      <td>0.481808</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject  exam  is_ge  age race        bmi birads    ips-cc   ips-mlo  \\\n",
       "0       9112     1      1   48    6  29.679908      b  0.314072  0.189455   \n",
       "1        180     1      1   63    8  21.173437      b  0.391883  0.560221   \n",
       "2      35533     1      1   82    8  33.105904      b  0.168119  0.239552   \n",
       "3      43781     1      1   57    8  22.205153      a  0.501777  0.338702   \n",
       "4      45170     1      1   64    1  23.643988      b  0.744803  0.672566   \n",
       "..       ...   ...    ...  ...  ...        ...    ...       ...       ...   \n",
       "347   283018     3      1   55    8  26.046094      b  0.585775  0.586799   \n",
       "348   285187     1      1   57    1  27.935803      c  0.491542  0.668285   \n",
       "349   938055     1      1   63    8  29.900549      b  0.587678  0.685947   \n",
       "350   288571     1      1   61    8  34.922548      b  0.545916  0.563719   \n",
       "351   962869     1      1   65    2  33.645150      b  0.488802  0.603985   \n",
       "\n",
       "     contra-cc  contra-mlo  is_case  \n",
       "0     0.211020    0.302417        0  \n",
       "1     0.217044    0.236010        0  \n",
       "2     0.107926    0.230202        0  \n",
       "3     0.223229    0.150545        0  \n",
       "4     0.198790    0.271363        0  \n",
       "..         ...         ...      ...  \n",
       "347   0.576758    0.603956        0  \n",
       "348   0.580874    0.647027        0  \n",
       "349   0.530199    0.673530        1  \n",
       "350   0.610101    0.601913        0  \n",
       "351   0.257505    0.481808        1  \n",
       "\n",
       "[352 rows x 12 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t1p_rcc = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t1p_rcc, 'exam': all_exam_t1p_rcc, 'is_ge': all_machines_t1p_rcc, \n",
    "     'age': all_ages_t1p_rcc, 'race': all_races_t1p_rcc, 'bmi': all_bmis_t1p_rcc, \n",
    "     'birads': all_birads_t1p_rcc,})\n",
    "print(df_t1p_rcc.head())\n",
    "d_ = np.concatenate([all_preds_t1p_rcc, all_labels_t1p_rcc[:, np.newaxis]], axis=1)\n",
    "d_[:3]\n",
    "df_t1p_rcc = pd.concat([df_t1p_rcc, pd.DataFrame(d_)], axis=1)\n",
    "df_t1p_rcc.head()\n",
    "df_t1p_rcc = df_t1p_rcc.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t1p_rcc = df_t1p_rcc.astype({'is_case': 'int8'})\n",
    "df_t1p_rcc.head()\n",
    "df_t1p_rcc.to_csv('../time_set/finetuned_pred_score_4view_T1p_nyu_single_view_train_rcc.csv', index=False)\n",
    "df_t1p_rcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=352\n",
      "4view max AUC=0.601\n",
      "4view mean AUC=0.602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1p_rcc)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_rcc, all_probs_max_t1p_rcc)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_rcc, all_preds_t1p_rcc.mean(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1p_rmlo = np.concatenate(subject_pool)\n",
    "all_exam_t1p_rmlo = np.concatenate(exam_pool)\n",
    "all_preds_t1p_rmlo = torch.cat(pred_pool)\n",
    "all_labels_t1p_rmlo = torch.cat(label_pool)\n",
    "all_preds_t1p_rmlo = all_preds_t1p_rmlo.cpu().numpy()\n",
    "all_labels_t1p_rmlo = all_labels_t1p_rmlo.numpy()\n",
    "all_probs_max_t1p_rmlo = all_preds_t1p_rmlo.max(1)\n",
    "all_machines_t1p_rmlo = np.concatenate(machine_pool)\n",
    "all_ages_t1p_rmlo = np.concatenate(age_pool)\n",
    "all_races_t1p_rmlo = np.concatenate(race_pool)\n",
    "all_bmis_t1p_rmlo = np.concatenate(bmi_pool)\n",
    "all_birads_t1p_rmlo = np.concatenate(birads_pool)\n",
    "all_libras_t1p_rmlo = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject  exam  is_ge  age race        bmi birads\n",
      "0     9112     1      1   48    6  29.679908      b\n",
      "1      180     1      1   63    8  21.173437      b\n",
      "2    35533     1      1   82    8  33.105904      b\n",
      "3    43781     1      1   57    8  22.205153      a\n",
      "4    45170     1      1   64    1  23.643988      b\n",
      "[[0.40923637 0.1820302  0.47830078 0.38353345 0.        ]\n",
      " [0.21005958 0.52581424 0.19649127 0.6147708  0.        ]\n",
      " [0.31063914 0.6892659  0.30994087 0.45940143 0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "      <th>ips-cc</th>\n",
       "      <th>ips-mlo</th>\n",
       "      <th>contra-cc</th>\n",
       "      <th>contra-mlo</th>\n",
       "      <th>is_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "      <td>0.409236</td>\n",
       "      <td>0.182030</td>\n",
       "      <td>0.478301</td>\n",
       "      <td>0.383533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>21.173437</td>\n",
       "      <td>b</td>\n",
       "      <td>0.210060</td>\n",
       "      <td>0.525814</td>\n",
       "      <td>0.196491</td>\n",
       "      <td>0.614771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35533</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>33.105904</td>\n",
       "      <td>b</td>\n",
       "      <td>0.310639</td>\n",
       "      <td>0.689266</td>\n",
       "      <td>0.309941</td>\n",
       "      <td>0.459401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43781</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>22.205153</td>\n",
       "      <td>a</td>\n",
       "      <td>0.218216</td>\n",
       "      <td>0.259015</td>\n",
       "      <td>0.411544</td>\n",
       "      <td>0.144204</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "      <td>0.780869</td>\n",
       "      <td>0.803897</td>\n",
       "      <td>0.224604</td>\n",
       "      <td>0.728552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>283018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>26.046094</td>\n",
       "      <td>b</td>\n",
       "      <td>0.707281</td>\n",
       "      <td>0.380356</td>\n",
       "      <td>0.644751</td>\n",
       "      <td>0.584245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>285187</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>27.935803</td>\n",
       "      <td>c</td>\n",
       "      <td>0.342118</td>\n",
       "      <td>0.613535</td>\n",
       "      <td>0.335584</td>\n",
       "      <td>0.409683</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>938055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>29.900549</td>\n",
       "      <td>b</td>\n",
       "      <td>0.317573</td>\n",
       "      <td>0.472014</td>\n",
       "      <td>0.554607</td>\n",
       "      <td>0.443947</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>288571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>8</td>\n",
       "      <td>34.922548</td>\n",
       "      <td>b</td>\n",
       "      <td>0.348107</td>\n",
       "      <td>0.323923</td>\n",
       "      <td>0.555233</td>\n",
       "      <td>0.473953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>962869</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>33.645150</td>\n",
       "      <td>b</td>\n",
       "      <td>0.141777</td>\n",
       "      <td>0.403907</td>\n",
       "      <td>0.182029</td>\n",
       "      <td>0.434467</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject  exam  is_ge  age race        bmi birads    ips-cc   ips-mlo  \\\n",
       "0       9112     1      1   48    6  29.679908      b  0.409236  0.182030   \n",
       "1        180     1      1   63    8  21.173437      b  0.210060  0.525814   \n",
       "2      35533     1      1   82    8  33.105904      b  0.310639  0.689266   \n",
       "3      43781     1      1   57    8  22.205153      a  0.218216  0.259015   \n",
       "4      45170     1      1   64    1  23.643988      b  0.780869  0.803897   \n",
       "..       ...   ...    ...  ...  ...        ...    ...       ...       ...   \n",
       "347   283018     3      1   55    8  26.046094      b  0.707281  0.380356   \n",
       "348   285187     1      1   57    1  27.935803      c  0.342118  0.613535   \n",
       "349   938055     1      1   63    8  29.900549      b  0.317573  0.472014   \n",
       "350   288571     1      1   61    8  34.922548      b  0.348107  0.323923   \n",
       "351   962869     1      1   65    2  33.645150      b  0.141777  0.403907   \n",
       "\n",
       "     contra-cc  contra-mlo  is_case  \n",
       "0     0.478301    0.383533        0  \n",
       "1     0.196491    0.614771        0  \n",
       "2     0.309941    0.459401        0  \n",
       "3     0.411544    0.144204        0  \n",
       "4     0.224604    0.728552        0  \n",
       "..         ...         ...      ...  \n",
       "347   0.644751    0.584245        0  \n",
       "348   0.335584    0.409683        0  \n",
       "349   0.554607    0.443947        1  \n",
       "350   0.555233    0.473953        0  \n",
       "351   0.182029    0.434467        1  \n",
       "\n",
       "[352 rows x 12 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t1p_rmlo = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t1p_rmlo, 'exam': all_exam_t1p_rmlo, 'is_ge': all_machines_t1p_rmlo, \n",
    "     'age': all_ages_t1p_rmlo, 'race': all_races_t1p_rmlo, 'bmi': all_bmis_t1p_rmlo, \n",
    "     'birads': all_birads_t1p_rmlo,})\n",
    "print(df_t1p_rmlo.head())\n",
    "d_ = np.concatenate([all_preds_t1p_rmlo, all_labels_t1p_rmlo[:, np.newaxis]], axis=1)\n",
    "print(d_[:3])\n",
    "df_t1p_rmlo = pd.concat([df_t1p_rmlo, pd.DataFrame(d_)], axis=1)\n",
    "df_t1p_rmlo.head()\n",
    "df_t1p_rmlo = df_t1p_rmlo.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t1p_rmlo = df_t1p_rmlo.astype({'is_case': 'int8'})\n",
    "df_t1p_rmlo.head()\n",
    "df_t1p_rmlo.to_csv('../time_set/finetuned_pred_score_4view_T1p_nyu_single_view_train_rmlo.csv', index=False)\n",
    "df_t1p_rmlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=352\n",
      "4view max AUC=0.558\n",
      "4view mean AUC=0.578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1p_rmlo)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_rmlo, all_probs_max_t1p_rmlo)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_rmlo, all_preds_t1p_rmlo.mean(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1p_lmlo = np.concatenate(subject_pool)\n",
    "all_exam_t1p_lmlo = np.concatenate(exam_pool)\n",
    "all_preds_t1p_lmlo = torch.cat(pred_pool)\n",
    "all_labels_t1p_lmlo = torch.cat(label_pool)\n",
    "all_preds_t1p_lmlo = all_preds_t1p_lmlo.cpu().numpy()\n",
    "all_labels_t1p_lmlo = all_labels_t1p_lmlo.numpy()\n",
    "all_probs_max_t1p_lmlo = all_preds_t1p_lmlo.max(1)\n",
    "all_machines_t1p_lmlo = np.concatenate(machine_pool)\n",
    "all_ages_t1p_lmlo = np.concatenate(age_pool)\n",
    "all_races_t1p_lmlo = np.concatenate(race_pool)\n",
    "all_bmis_t1p_lmlo = np.concatenate(bmi_pool)\n",
    "all_birads_t1p_lmlo = np.concatenate(birads_pool)\n",
    "all_libras_t1p_lmlo = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject  exam  is_ge  age race        bmi birads\n",
      "0     9112     1      1   48    6  29.679908      b\n",
      "1      180     1      1   63    8  21.173437      b\n",
      "2    35533     1      1   82    8  33.105904      b\n",
      "3    43781     1      1   57    8  22.205153      a\n",
      "4    45170     1      1   64    1  23.643988      b\n",
      "[[0.161507   0.12555528 0.29154965 0.25918138 0.        ]\n",
      " [0.11341159 0.24658446 0.03688212 0.17674343 0.        ]\n",
      " [0.20569111 0.31027606 0.15116781 0.2578532  0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "      <th>ips-cc</th>\n",
       "      <th>ips-mlo</th>\n",
       "      <th>contra-cc</th>\n",
       "      <th>contra-mlo</th>\n",
       "      <th>is_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "      <td>0.161507</td>\n",
       "      <td>0.125555</td>\n",
       "      <td>0.291550</td>\n",
       "      <td>0.259181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>21.173437</td>\n",
       "      <td>b</td>\n",
       "      <td>0.113412</td>\n",
       "      <td>0.246584</td>\n",
       "      <td>0.036882</td>\n",
       "      <td>0.176743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35533</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>33.105904</td>\n",
       "      <td>b</td>\n",
       "      <td>0.205691</td>\n",
       "      <td>0.310276</td>\n",
       "      <td>0.151168</td>\n",
       "      <td>0.257853</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43781</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>22.205153</td>\n",
       "      <td>a</td>\n",
       "      <td>0.222289</td>\n",
       "      <td>0.156038</td>\n",
       "      <td>0.615847</td>\n",
       "      <td>0.091920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "      <td>0.756499</td>\n",
       "      <td>0.755191</td>\n",
       "      <td>0.139049</td>\n",
       "      <td>0.320534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>283018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>26.046094</td>\n",
       "      <td>b</td>\n",
       "      <td>0.506397</td>\n",
       "      <td>0.451063</td>\n",
       "      <td>0.497091</td>\n",
       "      <td>0.494041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>285187</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>27.935803</td>\n",
       "      <td>c</td>\n",
       "      <td>0.356089</td>\n",
       "      <td>0.513484</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.523130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>938055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>29.900549</td>\n",
       "      <td>b</td>\n",
       "      <td>0.449159</td>\n",
       "      <td>0.570081</td>\n",
       "      <td>0.448487</td>\n",
       "      <td>0.515591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>288571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>8</td>\n",
       "      <td>34.922548</td>\n",
       "      <td>b</td>\n",
       "      <td>0.415004</td>\n",
       "      <td>0.492876</td>\n",
       "      <td>0.497390</td>\n",
       "      <td>0.557965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>962869</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>33.645150</td>\n",
       "      <td>b</td>\n",
       "      <td>0.383681</td>\n",
       "      <td>0.563068</td>\n",
       "      <td>0.282687</td>\n",
       "      <td>0.550685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject  exam  is_ge  age race        bmi birads    ips-cc   ips-mlo  \\\n",
       "0       9112     1      1   48    6  29.679908      b  0.161507  0.125555   \n",
       "1        180     1      1   63    8  21.173437      b  0.113412  0.246584   \n",
       "2      35533     1      1   82    8  33.105904      b  0.205691  0.310276   \n",
       "3      43781     1      1   57    8  22.205153      a  0.222289  0.156038   \n",
       "4      45170     1      1   64    1  23.643988      b  0.756499  0.755191   \n",
       "..       ...   ...    ...  ...  ...        ...    ...       ...       ...   \n",
       "347   283018     3      1   55    8  26.046094      b  0.506397  0.451063   \n",
       "348   285187     1      1   57    1  27.935803      c  0.356089  0.513484   \n",
       "349   938055     1      1   63    8  29.900549      b  0.449159  0.570081   \n",
       "350   288571     1      1   61    8  34.922548      b  0.415004  0.492876   \n",
       "351   962869     1      1   65    2  33.645150      b  0.383681  0.563068   \n",
       "\n",
       "     contra-cc  contra-mlo  is_case  \n",
       "0     0.291550    0.259181        0  \n",
       "1     0.036882    0.176743        0  \n",
       "2     0.151168    0.257853        0  \n",
       "3     0.615847    0.091920        0  \n",
       "4     0.139049    0.320534        0  \n",
       "..         ...         ...      ...  \n",
       "347   0.497091    0.494041        0  \n",
       "348   0.418800    0.523130        0  \n",
       "349   0.448487    0.515591        1  \n",
       "350   0.497390    0.557965        0  \n",
       "351   0.282687    0.550685        1  \n",
       "\n",
       "[352 rows x 12 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t1p_lmlo = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t1p_lmlo, 'exam': all_exam_t1p_lmlo, 'is_ge': all_machines_t1p_lmlo, \n",
    "     'age': all_ages_t1p_lmlo, 'race': all_races_t1p_lmlo, 'bmi': all_bmis_t1p_lmlo, \n",
    "     'birads': all_birads_t1p_lmlo,})\n",
    "print(df_t1p_lmlo.head())\n",
    "d_ = np.concatenate([all_preds_t1p_lmlo, all_labels_t1p_lmlo[:, np.newaxis]], axis=1)\n",
    "print(d_[:3])\n",
    "df_t1p_lmlo = pd.concat([df_t1p_lmlo, pd.DataFrame(d_)], axis=1)\n",
    "df_t1p_lmlo.head()\n",
    "df_t1p_lmlo = df_t1p_lmlo.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t1p_lmlo = df_t1p_lmlo.astype({'is_case': 'int8'})\n",
    "df_t1p_lmlo.head()\n",
    "df_t1p_lmlo.to_csv('../time_set/finetuned_pred_score_4view_T1p_nyu_single_view_train_lmlo.csv', index=False)\n",
    "df_t1p_lmlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=352\n",
      "4view max AUC=0.573\n",
      "4view mean AUC=0.576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1p_lmlo)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_lmlo, all_probs_max_t1p_lmlo)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_lmlo, all_preds_t1p_lmlo.mean(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1p = np.concatenate(subject_pool)\n",
    "all_exam_t1p = np.concatenate(exam_pool)\n",
    "all_preds_t1p = torch.cat(pred_pool)\n",
    "all_labels_t1p = torch.cat(label_pool)\n",
    "all_preds_t1p = all_preds_t1p.cpu().numpy()\n",
    "all_labels_t1p = all_labels_t1p.numpy()\n",
    "all_probs_max_t1p = all_preds_t1p.max(1)\n",
    "all_machines_t1p = np.concatenate(machine_pool)\n",
    "all_ages_t1p = np.concatenate(age_pool)\n",
    "all_races_t1p = np.concatenate(race_pool)\n",
    "all_bmis_t1p = np.concatenate(bmi_pool)\n",
    "all_birads_t1p = np.concatenate(birads_pool)\n",
    "all_libras_t1p = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1p2 = np.concatenate(subject_pool)\n",
    "all_exam_t1p2 = np.concatenate(exam_pool)\n",
    "all_preds_t1p2 = torch.cat(pred_pool)\n",
    "all_labels_t1p2 = torch.cat(label_pool)\n",
    "all_preds_t1p2 = all_preds_t1p2.cpu().numpy()\n",
    "all_labels_t1p2 = all_labels_t1p2.numpy()\n",
    "all_probs_max_t1p2 = all_preds_t1p2.max(1)\n",
    "all_machines_t1p2 = np.concatenate(machine_pool)\n",
    "all_ages_t1p2 = np.concatenate(age_pool)\n",
    "all_races_t1p2 = np.concatenate(race_pool)\n",
    "all_bmis_t1p2 = np.concatenate(bmi_pool)\n",
    "all_birads_t1p2 = np.concatenate(birads_pool)\n",
    "all_libras_t1p2 = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=352\n",
      "4view max AUC=0.621\n",
      "4view mean AUC=0.623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1p)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1p2, all_probs_max_t1p2)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t1p2, all_preds_t1p2.mean(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['009112', '000180', '035533']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subj_t1p2 = [ '{:06d}'.format(s) for s in all_subj_t1p2]\n",
    "all_subj_t1p2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>exam</th>\n",
       "      <th>is_ge</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>bmi</th>\n",
       "      <th>birads</th>\n",
       "      <th>ips-cc</th>\n",
       "      <th>ips-mlo</th>\n",
       "      <th>contra-cc</th>\n",
       "      <th>contra-mlo</th>\n",
       "      <th>is_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>009112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>29.679908</td>\n",
       "      <td>b</td>\n",
       "      <td>0.419155</td>\n",
       "      <td>0.295417</td>\n",
       "      <td>0.295116</td>\n",
       "      <td>0.285280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>21.173437</td>\n",
       "      <td>b</td>\n",
       "      <td>0.435263</td>\n",
       "      <td>0.638971</td>\n",
       "      <td>0.719960</td>\n",
       "      <td>0.801639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>035533</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>33.105904</td>\n",
       "      <td>b</td>\n",
       "      <td>0.261712</td>\n",
       "      <td>0.521824</td>\n",
       "      <td>0.187232</td>\n",
       "      <td>0.389249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043781</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>22.205153</td>\n",
       "      <td>a</td>\n",
       "      <td>0.520196</td>\n",
       "      <td>0.201235</td>\n",
       "      <td>0.252870</td>\n",
       "      <td>0.145785</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>045170</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>23.643988</td>\n",
       "      <td>b</td>\n",
       "      <td>0.637524</td>\n",
       "      <td>0.693089</td>\n",
       "      <td>0.359462</td>\n",
       "      <td>0.319172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject  exam  is_ge  age race        bmi birads    ips-cc   ips-mlo  \\\n",
       "0  009112     1      1   48    6  29.679908      b  0.419155  0.295417   \n",
       "1  000180     1      1   63    8  21.173437      b  0.435263  0.638971   \n",
       "2  035533     1      1   82    8  33.105904      b  0.261712  0.521824   \n",
       "3  043781     1      1   57    8  22.205153      a  0.520196  0.201235   \n",
       "4  045170     1      1   64    1  23.643988      b  0.637524  0.693089   \n",
       "\n",
       "   contra-cc  contra-mlo  is_case  \n",
       "0   0.295116    0.285280        0  \n",
       "1   0.719960    0.801639        0  \n",
       "2   0.187232    0.389249        0  \n",
       "3   0.252870    0.145785        0  \n",
       "4   0.359462    0.319172        0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_t1p2 = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t1p2, 'exam': all_exam_t1p2, 'is_ge': all_machines_t1p2, \n",
    "     'age': all_ages_t1p2, 'race': all_races_t1p2, 'bmi': all_bmis_t1p2, \n",
    "     'birads': all_birads_t1p2,})\n",
    "d_ = np.concatenate([all_preds_t1p2, all_labels_t1p2[:, np.newaxis]], axis=1)\n",
    "df_t1p2 = pd.concat([df_t1p2, pd.DataFrame(d_)], axis=1)\n",
    "df_t1p2 = df_t1p2.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t1p2 = df_t1p2.astype({'is_case': 'int8'})\n",
    "df_t1p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1p2.to_csv('../time_set/finetuned_pred_score_4view_T1p_nyu_single_view_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=352\n",
      "4view max AUC=0.569\n",
      "4view mean AUC=0.565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1p)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1p, all_probs_max_t1p)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t1p, all_preds_t1p.mean(1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-view Training + Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3+ Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Fold 1 ==========\n",
      "Test AUC at start: same-cc=0.619, same-mlo=0.501, opp-cc=0.582, opp-mlo=0.402\n",
      "Max-Score Based AUC Before Training: 0.509, Mean-Score Based AUC Before Training: 0.521\n",
      "Iter=5, avg train loss=0.978, \n",
      "\tAvg Val Loss: same-cc=0.612, AUC: same-cc=0.476 \n",
      "\tAvg Val Loss: same-mlo=0.554, AUC: same-mlo=0.642 \n",
      "\tAvg Val Loss opp-cc=0.626, AUC: opp-cc=0.466 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.441\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=10, avg train loss=0.964, \n",
      "\tAvg Val Loss: same-cc=0.628, AUC: same-cc=0.443 \n",
      "\tAvg Val Loss: same-mlo=0.567, AUC: same-mlo=0.634 \n",
      "\tAvg Val Loss opp-cc=0.659, AUC: opp-cc=0.411 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.447\n",
      "Best opp-mlo model saved.\n",
      "Iter=15, avg train loss=1.151, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.437 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.381 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.457\n",
      "Best opp-mlo model saved.\n",
      "Iter=20, avg train loss=0.794, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.443 \n",
      "\tAvg Val Loss: same-mlo=0.614, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.370 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.472\n",
      "Best opp-mlo model saved.\n",
      "Iter=25, avg train loss=0.602, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.443 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.711, AUC: opp-cc=0.360 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.524\n",
      "Best opp-mlo model saved.\n",
      "Iter=30, avg train loss=0.723, \n",
      "\tAvg Val Loss: same-cc=0.667, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.584, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.753, AUC: opp-cc=0.379 \n",
      "\tAvg Val Loss: opp-mlo=0.579, AUC: opp-mlo=0.553\n",
      "Best opp-mlo model saved.\n",
      "Iter=35, avg train loss=0.708, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.580, AUC: same-mlo=0.591 \n",
      "\tAvg Val Loss opp-cc=0.792, AUC: opp-cc=0.372 \n",
      "\tAvg Val Loss: opp-mlo=0.576, AUC: opp-mlo=0.593\n",
      "Best opp-mlo model saved.\n",
      "Iter=40, avg train loss=0.827, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.474 \n",
      "\tAvg Val Loss: same-mlo=0.580, AUC: same-mlo=0.563 \n",
      "\tAvg Val Loss opp-cc=0.791, AUC: opp-cc=0.348 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.565\n",
      "Iter=45, avg train loss=0.871, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.532 \n",
      "\tAvg Val Loss opp-cc=0.795, AUC: opp-cc=0.314 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.563\n",
      "Iter=50, avg train loss=0.745, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.474 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.514 \n",
      "\tAvg Val Loss opp-cc=0.793, AUC: opp-cc=0.316 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.563\n",
      "Iter=55, avg train loss=0.730, \n",
      "\tAvg Val Loss: same-cc=0.682, AUC: same-cc=0.482 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.522 \n",
      "\tAvg Val Loss opp-cc=0.808, AUC: opp-cc=0.312 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.557\n",
      "Best same-cc model saved.\n",
      "Iter=60, avg train loss=0.649, \n",
      "\tAvg Val Loss: same-cc=0.707, AUC: same-cc=0.480 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.492 \n",
      "\tAvg Val Loss opp-cc=0.835, AUC: opp-cc=0.326 \n",
      "\tAvg Val Loss: opp-mlo=0.634, AUC: opp-mlo=0.540\n",
      "Iter=65, avg train loss=0.708, \n",
      "\tAvg Val Loss: same-cc=0.699, AUC: same-cc=0.480 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.474 \n",
      "\tAvg Val Loss opp-cc=0.793, AUC: opp-cc=0.328 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.528\n",
      "Iter=70, avg train loss=0.739, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.476 \n",
      "\tAvg Val Loss: same-mlo=0.631, AUC: same-mlo=0.470 \n",
      "\tAvg Val Loss opp-cc=0.803, AUC: opp-cc=0.326 \n",
      "\tAvg Val Loss: opp-mlo=0.639, AUC: opp-mlo=0.543\n",
      "Iter=75, avg train loss=0.694, \n",
      "\tAvg Val Loss: same-cc=0.704, AUC: same-cc=0.476 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.476 \n",
      "\tAvg Val Loss opp-cc=0.796, AUC: opp-cc=0.340 \n",
      "\tAvg Val Loss: opp-mlo=0.652, AUC: opp-mlo=0.545\n",
      "Iter=80, avg train loss=0.719, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.644, AUC: same-mlo=0.476 \n",
      "\tAvg Val Loss opp-cc=0.808, AUC: opp-cc=0.334 \n",
      "\tAvg Val Loss: opp-mlo=0.662, AUC: opp-mlo=0.540\n",
      "Iter=85, avg train loss=0.647, \n",
      "\tAvg Val Loss: same-cc=0.725, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.639, AUC: same-mlo=0.484 \n",
      "\tAvg Val Loss opp-cc=0.789, AUC: opp-cc=0.336 \n",
      "\tAvg Val Loss: opp-mlo=0.653, AUC: opp-mlo=0.543\n",
      "Iter=90, avg train loss=0.669, \n",
      "\tAvg Val Loss: same-cc=0.725, AUC: same-cc=0.455 \n",
      "\tAvg Val Loss: same-mlo=0.639, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.785, AUC: opp-cc=0.336 \n",
      "\tAvg Val Loss: opp-mlo=0.644, AUC: opp-mlo=0.569\n",
      "Iter=95, avg train loss=0.642, \n",
      "\tAvg Val Loss: same-cc=0.735, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.480 \n",
      "\tAvg Val Loss opp-cc=0.811, AUC: opp-cc=0.340 \n",
      "\tAvg Val Loss: opp-mlo=0.646, AUC: opp-mlo=0.555\n",
      "Iter=100, avg train loss=0.687, \n",
      "\tAvg Val Loss: same-cc=0.729, AUC: same-cc=0.451 \n",
      "\tAvg Val Loss: same-mlo=0.634, AUC: same-mlo=0.468 \n",
      "\tAvg Val Loss opp-cc=0.810, AUC: opp-cc=0.334 \n",
      "\tAvg Val Loss: opp-mlo=0.642, AUC: opp-mlo=0.555\n",
      "Iter=105, avg train loss=0.653, \n",
      "\tAvg Val Loss: same-cc=0.729, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.626, AUC: same-mlo=0.457 \n",
      "\tAvg Val Loss opp-cc=0.797, AUC: opp-cc=0.346 \n",
      "\tAvg Val Loss: opp-mlo=0.636, AUC: opp-mlo=0.561\n",
      "Iter=110, avg train loss=0.681, \n",
      "\tAvg Val Loss: same-cc=0.717, AUC: same-cc=0.453 \n",
      "\tAvg Val Loss: same-mlo=0.617, AUC: same-mlo=0.476 \n",
      "\tAvg Val Loss opp-cc=0.813, AUC: opp-cc=0.324 \n",
      "\tAvg Val Loss: opp-mlo=0.621, AUC: opp-mlo=0.563\n",
      "Iter=115, avg train loss=0.596, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.451 \n",
      "\tAvg Val Loss: same-mlo=0.617, AUC: same-mlo=0.490 \n",
      "\tAvg Val Loss opp-cc=0.824, AUC: opp-cc=0.336 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.589\n",
      "Iter=120, avg train loss=0.684, \n",
      "\tAvg Val Loss: same-cc=0.710, AUC: same-cc=0.441 \n",
      "\tAvg Val Loss: same-mlo=0.620, AUC: same-mlo=0.494 \n",
      "\tAvg Val Loss opp-cc=0.816, AUC: opp-cc=0.336 \n",
      "\tAvg Val Loss: opp-mlo=0.611, AUC: opp-mlo=0.603\n",
      "Best opp-mlo model saved.\n",
      "Iter=125, avg train loss=0.669, \n",
      "\tAvg Val Loss: same-cc=0.713, AUC: same-cc=0.447 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.829, AUC: opp-cc=0.338 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.611\n",
      "Best opp-mlo model saved.\n",
      "Iter=130, avg train loss=0.601, \n",
      "\tAvg Val Loss: same-cc=0.729, AUC: same-cc=0.441 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.832, AUC: opp-cc=0.330 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.636\n",
      "Best opp-mlo model saved.\n",
      "Iter=135, avg train loss=0.697, \n",
      "\tAvg Val Loss: same-cc=0.721, AUC: same-cc=0.449 \n",
      "\tAvg Val Loss: same-mlo=0.614, AUC: same-mlo=0.504 \n",
      "\tAvg Val Loss opp-cc=0.827, AUC: opp-cc=0.334 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.617\n",
      "Iter=140, avg train loss=0.547, \n",
      "\tAvg Val Loss: same-cc=0.727, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.494 \n",
      "\tAvg Val Loss opp-cc=0.829, AUC: opp-cc=0.346 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.619\n",
      "Iter=145, avg train loss=0.708, \n",
      "\tAvg Val Loss: same-cc=0.732, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.625, AUC: same-mlo=0.466 \n",
      "\tAvg Val Loss opp-cc=0.845, AUC: opp-cc=0.348 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.603\n",
      "Iter=150, avg train loss=0.727, \n",
      "\tAvg Val Loss: same-cc=0.753, AUC: same-cc=0.455 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.464 \n",
      "\tAvg Val Loss opp-cc=0.872, AUC: opp-cc=0.326 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.599\n",
      "Iter=155, avg train loss=0.678, \n",
      "\tAvg Val Loss: same-cc=0.744, AUC: same-cc=0.455 \n",
      "\tAvg Val Loss: same-mlo=0.641, AUC: same-mlo=0.464 \n",
      "\tAvg Val Loss opp-cc=0.892, AUC: opp-cc=0.320 \n",
      "\tAvg Val Loss: opp-mlo=0.669, AUC: opp-mlo=0.583\n",
      "Iter=160, avg train loss=0.653, \n",
      "\tAvg Val Loss: same-cc=0.727, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.476 \n",
      "\tAvg Val Loss opp-cc=0.866, AUC: opp-cc=0.318 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.591\n",
      "Iter=165, avg train loss=0.719, \n",
      "\tAvg Val Loss: same-cc=0.723, AUC: same-cc=0.455 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.474 \n",
      "\tAvg Val Loss opp-cc=0.814, AUC: opp-cc=0.312 \n",
      "\tAvg Val Loss: opp-mlo=0.639, AUC: opp-mlo=0.603\n",
      "Iter=170, avg train loss=0.638, \n",
      "\tAvg Val Loss: same-cc=0.729, AUC: same-cc=0.441 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.813, AUC: opp-cc=0.310 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=175, avg train loss=0.700, \n",
      "\tAvg Val Loss: same-cc=0.709, AUC: same-cc=0.451 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.494 \n",
      "\tAvg Val Loss opp-cc=0.794, AUC: opp-cc=0.314 \n",
      "\tAvg Val Loss: opp-mlo=0.643, AUC: opp-mlo=0.605\n",
      "Iter=180, avg train loss=0.619, \n",
      "\tAvg Val Loss: same-cc=0.699, AUC: same-cc=0.482 \n",
      "\tAvg Val Loss: same-mlo=0.631, AUC: same-mlo=0.506 \n",
      "\tAvg Val Loss opp-cc=0.815, AUC: opp-cc=0.306 \n",
      "\tAvg Val Loss: opp-mlo=0.648, AUC: opp-mlo=0.591\n",
      "Iter=185, avg train loss=0.627, \n",
      "\tAvg Val Loss: same-cc=0.678, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.518 \n",
      "\tAvg Val Loss opp-cc=0.784, AUC: opp-cc=0.294 \n",
      "\tAvg Val Loss: opp-mlo=0.611, AUC: opp-mlo=0.599\n",
      "Iter=190, avg train loss=0.614, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.506 \n",
      "\tAvg Val Loss opp-cc=0.803, AUC: opp-cc=0.298 \n",
      "\tAvg Val Loss: opp-mlo=0.619, AUC: opp-mlo=0.605\n",
      "Iter=195, avg train loss=0.709, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.455 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.496 \n",
      "\tAvg Val Loss opp-cc=0.797, AUC: opp-cc=0.312 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.607\n",
      "Iter=200, avg train loss=0.670, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.472 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.490 \n",
      "\tAvg Val Loss opp-cc=0.800, AUC: opp-cc=0.310 \n",
      "\tAvg Val Loss: opp-mlo=0.626, AUC: opp-mlo=0.607\n",
      "Iter=205, avg train loss=0.581, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.468 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.774, AUC: opp-cc=0.312 \n",
      "\tAvg Val Loss: opp-mlo=0.631, AUC: opp-mlo=0.605\n",
      "Iter=210, avg train loss=0.648, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.466 \n",
      "\tAvg Val Loss: same-mlo=0.648, AUC: same-mlo=0.478 \n",
      "\tAvg Val Loss opp-cc=0.789, AUC: opp-cc=0.296 \n",
      "\tAvg Val Loss: opp-mlo=0.640, AUC: opp-mlo=0.609\n",
      "Iter=215, avg train loss=0.613, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.470 \n",
      "\tAvg Val Loss: same-mlo=0.658, AUC: same-mlo=0.468 \n",
      "\tAvg Val Loss opp-cc=0.793, AUC: opp-cc=0.289 \n",
      "\tAvg Val Loss: opp-mlo=0.656, AUC: opp-mlo=0.611\n",
      "Iter=220, avg train loss=0.607, \n",
      "\tAvg Val Loss: same-cc=0.718, AUC: same-cc=0.476 \n",
      "\tAvg Val Loss: same-mlo=0.660, AUC: same-mlo=0.476 \n",
      "\tAvg Val Loss opp-cc=0.786, AUC: opp-cc=0.296 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.609\n",
      "Iter=225, avg train loss=0.601, \n",
      "\tAvg Val Loss: same-cc=0.704, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.478 \n",
      "\tAvg Val Loss opp-cc=0.799, AUC: opp-cc=0.306 \n",
      "\tAvg Val Loss: opp-mlo=0.644, AUC: opp-mlo=0.605\n",
      "Iter=230, avg train loss=0.580, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.455 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.494 \n",
      "\tAvg Val Loss opp-cc=0.790, AUC: opp-cc=0.300 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.626\n",
      "Iter=235, avg train loss=0.551, \n",
      "\tAvg Val Loss: same-cc=0.699, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.631, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.799, AUC: opp-cc=0.298 \n",
      "\tAvg Val Loss: opp-mlo=0.646, AUC: opp-mlo=0.621\n",
      "Iter=240, avg train loss=0.687, \n",
      "\tAvg Val Loss: same-cc=0.723, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.811, AUC: opp-cc=0.304 \n",
      "\tAvg Val Loss: opp-mlo=0.665, AUC: opp-mlo=0.617\n",
      "Iter=245, avg train loss=0.587, \n",
      "\tAvg Val Loss: same-cc=0.746, AUC: same-cc=0.466 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.844, AUC: opp-cc=0.306 \n",
      "\tAvg Val Loss: opp-mlo=0.721, AUC: opp-mlo=0.615\n",
      "Iter=250, avg train loss=0.580, \n",
      "\tAvg Val Loss: same-cc=0.756, AUC: same-cc=0.468 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.482 \n",
      "\tAvg Val Loss opp-cc=0.863, AUC: opp-cc=0.304 \n",
      "\tAvg Val Loss: opp-mlo=0.765, AUC: opp-mlo=0.587\n",
      "Iter=255, avg train loss=0.648, \n",
      "\tAvg Val Loss: same-cc=0.753, AUC: same-cc=0.474 \n",
      "\tAvg Val Loss: same-mlo=0.670, AUC: same-mlo=0.482 \n",
      "\tAvg Val Loss opp-cc=0.873, AUC: opp-cc=0.306 \n",
      "\tAvg Val Loss: opp-mlo=0.755, AUC: opp-mlo=0.585\n",
      "Iter=260, avg train loss=0.551, \n",
      "\tAvg Val Loss: same-cc=0.722, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.646, AUC: same-mlo=0.486 \n",
      "\tAvg Val Loss opp-cc=0.880, AUC: opp-cc=0.296 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.603\n",
      "Iter=265, avg train loss=0.606, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.641, AUC: same-mlo=0.490 \n",
      "\tAvg Val Loss opp-cc=0.859, AUC: opp-cc=0.344 \n",
      "\tAvg Val Loss: opp-mlo=0.666, AUC: opp-mlo=0.628\n",
      "Iter=270, avg train loss=0.595, \n",
      "\tAvg Val Loss: same-cc=0.703, AUC: same-cc=0.431 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.500 \n",
      "\tAvg Val Loss opp-cc=0.873, AUC: opp-cc=0.348 \n",
      "\tAvg Val Loss: opp-mlo=0.647, AUC: opp-mlo=0.605\n",
      "Iter=275, avg train loss=0.597, \n",
      "\tAvg Val Loss: same-cc=0.698, AUC: same-cc=0.453 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.500 \n",
      "\tAvg Val Loss opp-cc=0.854, AUC: opp-cc=0.336 \n",
      "\tAvg Val Loss: opp-mlo=0.624, AUC: opp-mlo=0.617\n",
      "Iter=280, avg train loss=0.559, \n",
      "\tAvg Val Loss: same-cc=0.690, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.524 \n",
      "\tAvg Val Loss opp-cc=0.846, AUC: opp-cc=0.328 \n",
      "\tAvg Val Loss: opp-mlo=0.624, AUC: opp-mlo=0.636\n",
      "Best opp-mlo model saved.\n",
      "Iter=285, avg train loss=0.525, \n",
      "\tAvg Val Loss: same-cc=0.694, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.530 \n",
      "\tAvg Val Loss opp-cc=0.852, AUC: opp-cc=0.328 \n",
      "\tAvg Val Loss: opp-mlo=0.635, AUC: opp-mlo=0.644\n",
      "Best opp-mlo model saved.\n",
      "Iter=290, avg train loss=0.622, \n",
      "\tAvg Val Loss: same-cc=0.697, AUC: same-cc=0.474 \n",
      "\tAvg Val Loss: same-mlo=0.614, AUC: same-mlo=0.549 \n",
      "\tAvg Val Loss opp-cc=0.833, AUC: opp-cc=0.334 \n",
      "\tAvg Val Loss: opp-mlo=0.635, AUC: opp-mlo=0.646\n",
      "Best opp-mlo model saved.\n",
      "Iter=295, avg train loss=0.529, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.476 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.819, AUC: opp-cc=0.326 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.654\n",
      "Best opp-mlo model saved.\n",
      "Iter=300, avg train loss=0.502, \n",
      "\tAvg Val Loss: same-cc=0.694, AUC: same-cc=0.474 \n",
      "\tAvg Val Loss: same-mlo=0.587, AUC: same-mlo=0.575 \n",
      "\tAvg Val Loss opp-cc=0.850, AUC: opp-cc=0.312 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.644\n",
      "Iter=305, avg train loss=0.564, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.478 \n",
      "\tAvg Val Loss: same-mlo=0.586, AUC: same-mlo=0.585 \n",
      "\tAvg Val Loss opp-cc=0.860, AUC: opp-cc=0.304 \n",
      "\tAvg Val Loss: opp-mlo=0.641, AUC: opp-mlo=0.648\n",
      "Iter=310, avg train loss=0.712, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.470 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.593 \n",
      "\tAvg Val Loss opp-cc=0.854, AUC: opp-cc=0.320 \n",
      "\tAvg Val Loss: opp-mlo=0.762, AUC: opp-mlo=0.609\n",
      "Iter=315, avg train loss=0.633, \n",
      "\tAvg Val Loss: same-cc=0.707, AUC: same-cc=0.472 \n",
      "\tAvg Val Loss: same-mlo=0.639, AUC: same-mlo=0.593 \n",
      "\tAvg Val Loss opp-cc=0.877, AUC: opp-cc=0.283 \n",
      "\tAvg Val Loss: opp-mlo=0.797, AUC: opp-mlo=0.587\n",
      "Iter=320, avg train loss=0.586, \n",
      "\tAvg Val Loss: same-cc=0.760, AUC: same-cc=0.488 \n",
      "\tAvg Val Loss: same-mlo=0.683, AUC: same-mlo=0.595 \n",
      "\tAvg Val Loss opp-cc=0.883, AUC: opp-cc=0.289 \n",
      "\tAvg Val Loss: opp-mlo=0.820, AUC: opp-mlo=0.591\n",
      "Best same-cc model saved.\n",
      "Iter=325, avg train loss=0.614, \n",
      "\tAvg Val Loss: same-cc=0.768, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.675, AUC: same-mlo=0.581 \n",
      "\tAvg Val Loss opp-cc=0.890, AUC: opp-cc=0.283 \n",
      "\tAvg Val Loss: opp-mlo=0.799, AUC: opp-mlo=0.577\n",
      "Iter=330, avg train loss=0.552, \n",
      "\tAvg Val Loss: same-cc=0.729, AUC: same-cc=0.439 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.549 \n",
      "\tAvg Val Loss opp-cc=0.868, AUC: opp-cc=0.287 \n",
      "\tAvg Val Loss: opp-mlo=0.743, AUC: opp-mlo=0.593\n",
      "Iter=335, avg train loss=0.638, \n",
      "\tAvg Val Loss: same-cc=0.749, AUC: same-cc=0.421 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.526 \n",
      "\tAvg Val Loss opp-cc=0.880, AUC: opp-cc=0.283 \n",
      "\tAvg Val Loss: opp-mlo=0.730, AUC: opp-mlo=0.577\n",
      "Iter=340, avg train loss=0.586, \n",
      "\tAvg Val Loss: same-cc=0.741, AUC: same-cc=0.421 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.514 \n",
      "\tAvg Val Loss opp-cc=0.889, AUC: opp-cc=0.296 \n",
      "\tAvg Val Loss: opp-mlo=0.702, AUC: opp-mlo=0.589\n",
      "Iter=345, avg train loss=0.590, \n",
      "\tAvg Val Loss: same-cc=0.761, AUC: same-cc=0.439 \n",
      "\tAvg Val Loss: same-mlo=0.648, AUC: same-mlo=0.498 \n",
      "\tAvg Val Loss opp-cc=0.897, AUC: opp-cc=0.318 \n",
      "\tAvg Val Loss: opp-mlo=0.682, AUC: opp-mlo=0.595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=350, avg train loss=0.589, \n",
      "\tAvg Val Loss: same-cc=0.785, AUC: same-cc=0.435 \n",
      "\tAvg Val Loss: same-mlo=0.672, AUC: same-mlo=0.500 \n",
      "\tAvg Val Loss opp-cc=0.921, AUC: opp-cc=0.306 \n",
      "\tAvg Val Loss: opp-mlo=0.688, AUC: opp-mlo=0.601\n",
      "Iter=355, avg train loss=0.543, \n",
      "\tAvg Val Loss: same-cc=0.791, AUC: same-cc=0.449 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.919, AUC: opp-cc=0.304 \n",
      "\tAvg Val Loss: opp-mlo=0.666, AUC: opp-mlo=0.646\n",
      "Iter=360, avg train loss=0.547, \n",
      "\tAvg Val Loss: same-cc=0.789, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.673, AUC: same-mlo=0.496 \n",
      "\tAvg Val Loss opp-cc=0.922, AUC: opp-cc=0.294 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.658\n",
      "Best opp-mlo model saved.\n",
      "Iter=365, avg train loss=0.501, \n",
      "\tAvg Val Loss: same-cc=0.800, AUC: same-cc=0.443 \n",
      "\tAvg Val Loss: same-mlo=0.690, AUC: same-mlo=0.502 \n",
      "\tAvg Val Loss opp-cc=0.916, AUC: opp-cc=0.316 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.652\n",
      "Iter=370, avg train loss=0.551, \n",
      "\tAvg Val Loss: same-cc=0.771, AUC: same-cc=0.451 \n",
      "\tAvg Val Loss: same-mlo=0.701, AUC: same-mlo=0.518 \n",
      "\tAvg Val Loss opp-cc=0.919, AUC: opp-cc=0.310 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.654\n",
      "Iter=375, avg train loss=0.588, \n",
      "\tAvg Val Loss: same-cc=0.784, AUC: same-cc=0.445 \n",
      "\tAvg Val Loss: same-mlo=0.736, AUC: same-mlo=0.538 \n",
      "\tAvg Val Loss opp-cc=0.900, AUC: opp-cc=0.310 \n",
      "\tAvg Val Loss: opp-mlo=0.708, AUC: opp-mlo=0.654\n",
      "Iter=380, avg train loss=0.517, \n",
      "\tAvg Val Loss: same-cc=0.769, AUC: same-cc=0.449 \n",
      "\tAvg Val Loss: same-mlo=0.707, AUC: same-mlo=0.516 \n",
      "\tAvg Val Loss opp-cc=0.885, AUC: opp-cc=0.308 \n",
      "\tAvg Val Loss: opp-mlo=0.690, AUC: opp-mlo=0.642\n",
      "Iter=385, avg train loss=0.450, \n",
      "\tAvg Val Loss: same-cc=0.757, AUC: same-cc=0.466 \n",
      "\tAvg Val Loss: same-mlo=0.686, AUC: same-mlo=0.516 \n",
      "\tAvg Val Loss opp-cc=0.920, AUC: opp-cc=0.283 \n",
      "\tAvg Val Loss: opp-mlo=0.685, AUC: opp-mlo=0.638\n",
      "Iter=390, avg train loss=0.628, \n",
      "\tAvg Val Loss: same-cc=0.737, AUC: same-cc=0.478 \n",
      "\tAvg Val Loss: same-mlo=0.682, AUC: same-mlo=0.512 \n",
      "\tAvg Val Loss opp-cc=0.915, AUC: opp-cc=0.320 \n",
      "\tAvg Val Loss: opp-mlo=0.665, AUC: opp-mlo=0.658\n",
      "Iter=395, avg train loss=0.512, \n",
      "\tAvg Val Loss: same-cc=0.725, AUC: same-cc=0.480 \n",
      "\tAvg Val Loss: same-mlo=0.678, AUC: same-mlo=0.528 \n",
      "\tAvg Val Loss opp-cc=0.922, AUC: opp-cc=0.322 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.660\n",
      "Best opp-mlo model saved.\n",
      "Iter=400, avg train loss=0.541, \n",
      "\tAvg Val Loss: same-cc=0.721, AUC: same-cc=0.478 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.540 \n",
      "\tAvg Val Loss opp-cc=0.891, AUC: opp-cc=0.334 \n",
      "\tAvg Val Loss: opp-mlo=0.662, AUC: opp-mlo=0.668\n",
      "Best opp-mlo model saved.\n",
      "Iter=405, avg train loss=0.590, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.653, AUC: same-mlo=0.565 \n",
      "\tAvg Val Loss opp-cc=0.900, AUC: opp-cc=0.344 \n",
      "\tAvg Val Loss: opp-mlo=0.640, AUC: opp-mlo=0.670\n",
      "Best opp-mlo model saved.\n",
      "Iter=410, avg train loss=0.596, \n",
      "\tAvg Val Loss: same-cc=0.707, AUC: same-cc=0.451 \n",
      "\tAvg Val Loss: same-mlo=0.743, AUC: same-mlo=0.559 \n",
      "\tAvg Val Loss opp-cc=0.934, AUC: opp-cc=0.342 \n",
      "\tAvg Val Loss: opp-mlo=0.755, AUC: opp-mlo=0.666\n",
      "Iter=415, avg train loss=0.594, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.451 \n",
      "\tAvg Val Loss: same-mlo=0.778, AUC: same-mlo=0.553 \n",
      "\tAvg Val Loss opp-cc=0.972, AUC: opp-cc=0.310 \n",
      "\tAvg Val Loss: opp-mlo=0.701, AUC: opp-mlo=0.672\n",
      "Best opp-mlo model saved.\n",
      "Iter=420, avg train loss=0.552, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.750, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.982, AUC: opp-cc=0.304 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.680\n",
      "Best opp-mlo model saved.\n",
      "Iter=425, avg train loss=0.507, \n",
      "\tAvg Val Loss: same-cc=0.700, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.726, AUC: same-mlo=0.532 \n",
      "\tAvg Val Loss opp-cc=0.940, AUC: opp-cc=0.330 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.680\n",
      "Best opp-mlo model saved.\n",
      "Iter=430, avg train loss=0.586, \n",
      "\tAvg Val Loss: same-cc=0.731, AUC: same-cc=0.453 \n",
      "\tAvg Val Loss: same-mlo=0.722, AUC: same-mlo=0.510 \n",
      "\tAvg Val Loss opp-cc=0.939, AUC: opp-cc=0.316 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.674\n",
      "Iter=435, avg train loss=0.435, \n",
      "\tAvg Val Loss: same-cc=0.746, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.778, AUC: same-mlo=0.518 \n",
      "\tAvg Val Loss opp-cc=0.938, AUC: opp-cc=0.334 \n",
      "\tAvg Val Loss: opp-mlo=0.664, AUC: opp-mlo=0.642\n",
      "Iter=440, avg train loss=0.609, \n",
      "\tAvg Val Loss: same-cc=0.719, AUC: same-cc=0.486 \n",
      "\tAvg Val Loss: same-mlo=0.749, AUC: same-mlo=0.516 \n",
      "\tAvg Val Loss opp-cc=0.951, AUC: opp-cc=0.310 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.652\n",
      "Iter=445, avg train loss=0.567, \n",
      "\tAvg Val Loss: same-cc=0.733, AUC: same-cc=0.496 \n",
      "\tAvg Val Loss: same-mlo=0.706, AUC: same-mlo=0.559 \n",
      "\tAvg Val Loss opp-cc=0.944, AUC: opp-cc=0.302 \n",
      "\tAvg Val Loss: opp-mlo=0.724, AUC: opp-mlo=0.660\n",
      "Best same-cc model saved.\n",
      "Iter=450, avg train loss=0.517, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.504 \n",
      "\tAvg Val Loss: same-mlo=0.696, AUC: same-mlo=0.577 \n",
      "\tAvg Val Loss opp-cc=0.892, AUC: opp-cc=0.314 \n",
      "\tAvg Val Loss: opp-mlo=0.745, AUC: opp-mlo=0.672\n",
      "Best same-cc model saved.\n",
      "Iter=455, avg train loss=0.450, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.476 \n",
      "\tAvg Val Loss: same-mlo=0.678, AUC: same-mlo=0.561 \n",
      "\tAvg Val Loss opp-cc=0.876, AUC: opp-cc=0.330 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.676\n",
      "Iter=460, avg train loss=0.493, \n",
      "\tAvg Val Loss: same-cc=0.709, AUC: same-cc=0.453 \n",
      "\tAvg Val Loss: same-mlo=0.673, AUC: same-mlo=0.551 \n",
      "\tAvg Val Loss opp-cc=0.928, AUC: opp-cc=0.320 \n",
      "\tAvg Val Loss: opp-mlo=0.682, AUC: opp-mlo=0.650\n",
      "Iter=465, avg train loss=0.562, \n",
      "\tAvg Val Loss: same-cc=0.717, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.681, AUC: same-mlo=0.555 \n",
      "\tAvg Val Loss opp-cc=0.968, AUC: opp-cc=0.294 \n",
      "\tAvg Val Loss: opp-mlo=0.731, AUC: opp-mlo=0.650\n",
      "Iter=470, avg train loss=0.527, \n",
      "\tAvg Val Loss: same-cc=0.770, AUC: same-cc=0.478 \n",
      "\tAvg Val Loss: same-mlo=0.721, AUC: same-mlo=0.573 \n",
      "\tAvg Val Loss opp-cc=1.021, AUC: opp-cc=0.269 \n",
      "\tAvg Val Loss: opp-mlo=0.933, AUC: opp-mlo=0.607\n",
      "Iter=475, avg train loss=0.532, \n",
      "\tAvg Val Loss: same-cc=0.777, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.713, AUC: same-mlo=0.589 \n",
      "\tAvg Val Loss opp-cc=0.984, AUC: opp-cc=0.275 \n",
      "\tAvg Val Loss: opp-mlo=0.911, AUC: opp-mlo=0.621\n",
      "Iter=480, avg train loss=0.506, \n",
      "\tAvg Val Loss: same-cc=0.755, AUC: same-cc=0.498 \n",
      "\tAvg Val Loss: same-mlo=0.684, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.917, AUC: opp-cc=0.320 \n",
      "\tAvg Val Loss: opp-mlo=0.825, AUC: opp-mlo=0.632\n",
      "Iter=485, avg train loss=0.449, \n",
      "\tAvg Val Loss: same-cc=0.712, AUC: same-cc=0.486 \n",
      "\tAvg Val Loss: same-mlo=0.657, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.918, AUC: opp-cc=0.302 \n",
      "\tAvg Val Loss: opp-mlo=0.723, AUC: opp-mlo=0.638\n",
      "Iter=490, avg train loss=0.538, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.476 \n",
      "\tAvg Val Loss: same-mlo=0.657, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.903, AUC: opp-cc=0.322 \n",
      "\tAvg Val Loss: opp-mlo=0.663, AUC: opp-mlo=0.650\n",
      "Iter=495, avg train loss=0.501, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.466 \n",
      "\tAvg Val Loss: same-mlo=0.656, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.918, AUC: opp-cc=0.304 \n",
      "\tAvg Val Loss: opp-mlo=0.663, AUC: opp-mlo=0.644\n",
      "Iter=500, avg train loss=0.560, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.484 \n",
      "\tAvg Val Loss: same-mlo=0.644, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=0.929, AUC: opp-cc=0.304 \n",
      "\tAvg Val Loss: opp-mlo=0.694, AUC: opp-mlo=0.638\n",
      "Best same-mlo model saved.\n",
      "Iter=505, avg train loss=0.517, \n",
      "\tAvg Val Loss: same-cc=0.707, AUC: same-cc=0.490 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.666 \n",
      "\tAvg Val Loss opp-cc=0.881, AUC: opp-cc=0.328 \n",
      "\tAvg Val Loss: opp-mlo=0.699, AUC: opp-mlo=0.642\n",
      "Best same-mlo model saved.\n",
      "Iter=510, avg train loss=0.545, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.492 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.678 \n",
      "\tAvg Val Loss opp-cc=0.899, AUC: opp-cc=0.320 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.640\n",
      "Best same-mlo model saved.\n",
      "Iter=515, avg train loss=0.435, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.522 \n",
      "\tAvg Val Loss: same-mlo=0.630, AUC: same-mlo=0.678 \n",
      "\tAvg Val Loss opp-cc=0.942, AUC: opp-cc=0.318 \n",
      "\tAvg Val Loss: opp-mlo=0.690, AUC: opp-mlo=0.636\n",
      "Best same-cc model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=520, avg train loss=0.507, \n",
      "\tAvg Val Loss: same-cc=0.699, AUC: same-cc=0.526 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.964, AUC: opp-cc=0.320 \n",
      "\tAvg Val Loss: opp-mlo=0.665, AUC: opp-mlo=0.644\n",
      "Best same-cc model saved.\n",
      "Iter=525, avg train loss=0.482, \n",
      "\tAvg Val Loss: same-cc=0.697, AUC: same-cc=0.514 \n",
      "\tAvg Val Loss: same-mlo=0.641, AUC: same-mlo=0.579 \n",
      "\tAvg Val Loss opp-cc=0.955, AUC: opp-cc=0.318 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.644\n",
      "Iter=530, avg train loss=0.438, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.520 \n",
      "\tAvg Val Loss: same-mlo=0.667, AUC: same-mlo=0.553 \n",
      "\tAvg Val Loss opp-cc=0.976, AUC: opp-cc=0.328 \n",
      "\tAvg Val Loss: opp-mlo=0.691, AUC: opp-mlo=0.642\n",
      "Iter=535, avg train loss=0.499, \n",
      "\tAvg Val Loss: same-cc=0.661, AUC: same-cc=0.486 \n",
      "\tAvg Val Loss: same-mlo=0.717, AUC: same-mlo=0.553 \n",
      "\tAvg Val Loss opp-cc=0.975, AUC: opp-cc=0.346 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.646\n",
      "Iter=540, avg train loss=0.520, \n",
      "\tAvg Val Loss: same-cc=0.672, AUC: same-cc=0.500 \n",
      "\tAvg Val Loss: same-mlo=0.718, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.947, AUC: opp-cc=0.366 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.642\n",
      "Iter=545, avg train loss=0.441, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.530 \n",
      "\tAvg Val Loss: same-mlo=0.660, AUC: same-mlo=0.587 \n",
      "\tAvg Val Loss opp-cc=0.946, AUC: opp-cc=0.344 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.658\n",
      "Best same-cc model saved.\n",
      "Iter=550, avg train loss=0.518, \n",
      "\tAvg Val Loss: same-cc=0.755, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.644, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.918, AUC: opp-cc=0.350 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.690\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=555, avg train loss=0.469, \n",
      "\tAvg Val Loss: same-cc=0.760, AUC: same-cc=0.545 \n",
      "\tAvg Val Loss: same-mlo=0.667, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.925, AUC: opp-cc=0.342 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.680\n",
      "Iter=560, avg train loss=0.570, \n",
      "\tAvg Val Loss: same-cc=0.740, AUC: same-cc=0.547 \n",
      "\tAvg Val Loss: same-mlo=0.696, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.921, AUC: opp-cc=0.352 \n",
      "\tAvg Val Loss: opp-mlo=0.656, AUC: opp-mlo=0.660\n",
      "Iter=565, avg train loss=0.450, \n",
      "\tAvg Val Loss: same-cc=0.802, AUC: same-cc=0.579 \n",
      "\tAvg Val Loss: same-mlo=0.705, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.897, AUC: opp-cc=0.364 \n",
      "\tAvg Val Loss: opp-mlo=0.688, AUC: opp-mlo=0.654\n",
      "Best same-cc model saved.\n",
      "Iter=570, avg train loss=0.470, \n",
      "\tAvg Val Loss: same-cc=0.780, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.674, AUC: same-mlo=0.666 \n",
      "\tAvg Val Loss opp-cc=0.846, AUC: opp-cc=0.372 \n",
      "\tAvg Val Loss: opp-mlo=0.690, AUC: opp-mlo=0.652\n",
      "Best same-cc model saved.\n",
      "Iter=575, avg train loss=0.490, \n",
      "\tAvg Val Loss: same-cc=0.858, AUC: same-cc=0.603 \n",
      "\tAvg Val Loss: same-mlo=0.701, AUC: same-mlo=0.682 \n",
      "\tAvg Val Loss opp-cc=0.921, AUC: opp-cc=0.350 \n",
      "\tAvg Val Loss: opp-mlo=0.724, AUC: opp-mlo=0.660\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Iter=580, avg train loss=0.475, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.909, AUC: opp-cc=0.352 \n",
      "\tAvg Val Loss: opp-mlo=0.610, AUC: opp-mlo=0.670\n",
      "Iter=585, avg train loss=0.378, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.557 \n",
      "\tAvg Val Loss: same-mlo=0.675, AUC: same-mlo=0.563 \n",
      "\tAvg Val Loss opp-cc=0.864, AUC: opp-cc=0.366 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.686\n",
      "Iter=590, avg train loss=0.373, \n",
      "\tAvg Val Loss: same-cc=0.702, AUC: same-cc=0.561 \n",
      "\tAvg Val Loss: same-mlo=0.728, AUC: same-mlo=0.530 \n",
      "\tAvg Val Loss opp-cc=0.888, AUC: opp-cc=0.368 \n",
      "\tAvg Val Loss: opp-mlo=0.639, AUC: opp-mlo=0.694\n",
      "Best opp-mlo model saved.\n",
      "Iter=595, avg train loss=0.410, \n",
      "\tAvg Val Loss: same-cc=0.724, AUC: same-cc=0.567 \n",
      "\tAvg Val Loss: same-mlo=0.772, AUC: same-mlo=0.516 \n",
      "\tAvg Val Loss opp-cc=0.907, AUC: opp-cc=0.370 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.682\n",
      "Iter=600, avg train loss=0.470, \n",
      "\tAvg Val Loss: same-cc=0.789, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.805, AUC: same-mlo=0.530 \n",
      "\tAvg Val Loss opp-cc=0.960, AUC: opp-cc=0.344 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.694\n",
      "Iter=605, avg train loss=0.479, \n",
      "\tAvg Val Loss: same-cc=0.769, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.844, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.992, AUC: opp-cc=0.342 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.688\n",
      "Iter=610, avg train loss=0.422, \n",
      "\tAvg Val Loss: same-cc=0.742, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.916, AUC: same-mlo=0.591 \n",
      "\tAvg Val Loss opp-cc=1.025, AUC: opp-cc=0.342 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.676\n",
      "Iter=615, avg train loss=0.356, \n",
      "\tAvg Val Loss: same-cc=0.728, AUC: same-cc=0.571 \n",
      "\tAvg Val Loss: same-mlo=0.848, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=1.038, AUC: opp-cc=0.334 \n",
      "\tAvg Val Loss: opp-mlo=0.667, AUC: opp-mlo=0.670\n",
      "Iter=620, avg train loss=0.428, \n",
      "\tAvg Val Loss: same-cc=0.694, AUC: same-cc=0.577 \n",
      "\tAvg Val Loss: same-mlo=0.798, AUC: same-mlo=0.660 \n",
      "\tAvg Val Loss opp-cc=1.047, AUC: opp-cc=0.294 \n",
      "\tAvg Val Loss: opp-mlo=0.645, AUC: opp-mlo=0.682\n",
      "Iter=625, avg train loss=0.463, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.642 \n",
      "\tAvg Val Loss opp-cc=0.931, AUC: opp-cc=0.304 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.688\n",
      "Iter=630, avg train loss=0.392, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.553 \n",
      "\tAvg Val Loss: same-mlo=0.666, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.910, AUC: opp-cc=0.360 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.692\n",
      "Iter=635, avg train loss=0.366, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.526 \n",
      "\tAvg Val Loss: same-mlo=0.715, AUC: same-mlo=0.567 \n",
      "\tAvg Val Loss opp-cc=0.918, AUC: opp-cc=0.368 \n",
      "\tAvg Val Loss: opp-mlo=0.680, AUC: opp-mlo=0.694\n",
      "Iter=640, avg train loss=0.575, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.524 \n",
      "\tAvg Val Loss: same-mlo=0.709, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.978, AUC: opp-cc=0.352 \n",
      "\tAvg Val Loss: opp-mlo=0.776, AUC: opp-mlo=0.664\n",
      "Iter=645, avg train loss=0.436, \n",
      "\tAvg Val Loss: same-cc=0.811, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.962, AUC: same-mlo=0.662 \n",
      "\tAvg Val Loss opp-cc=1.159, AUC: opp-cc=0.324 \n",
      "\tAvg Val Loss: opp-mlo=1.041, AUC: opp-mlo=0.615\n",
      "Iter=650, avg train loss=0.417, \n",
      "\tAvg Val Loss: same-cc=0.804, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=1.099, AUC: same-mlo=0.688 \n",
      "\tAvg Val Loss opp-cc=1.256, AUC: opp-cc=0.304 \n",
      "\tAvg Val Loss: opp-mlo=0.885, AUC: opp-mlo=0.626\n",
      "Best same-mlo model saved.\n",
      "Iter=655, avg train loss=0.511, \n",
      "\tAvg Val Loss: same-cc=0.727, AUC: same-cc=0.565 \n",
      "\tAvg Val Loss: same-mlo=1.055, AUC: same-mlo=0.690 \n",
      "\tAvg Val Loss opp-cc=1.195, AUC: opp-cc=0.294 \n",
      "\tAvg Val Loss: opp-mlo=0.710, AUC: opp-mlo=0.664\n",
      "Best same-mlo model saved.\n",
      "Iter=660, avg train loss=0.438, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.520 \n",
      "\tAvg Val Loss: same-mlo=0.780, AUC: same-mlo=0.668 \n",
      "\tAvg Val Loss opp-cc=1.083, AUC: opp-cc=0.322 \n",
      "\tAvg Val Loss: opp-mlo=0.679, AUC: opp-mlo=0.652\n",
      "Iter=665, avg train loss=0.426, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.510 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.965, AUC: opp-cc=0.346 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.682\n",
      "Iter=670, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.727, AUC: same-cc=0.573 \n",
      "\tAvg Val Loss: same-mlo=0.679, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.956, AUC: opp-cc=0.350 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.670\n",
      "Iter=675, avg train loss=0.381, \n",
      "\tAvg Val Loss: same-cc=0.943, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.658, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.979, AUC: opp-cc=0.350 \n",
      "\tAvg Val Loss: opp-mlo=0.716, AUC: opp-mlo=0.674\n",
      "Best same-cc model saved.\n",
      "Iter=680, avg train loss=0.414, \n",
      "\tAvg Val Loss: same-cc=0.910, AUC: same-cc=0.599 \n",
      "\tAvg Val Loss: same-mlo=0.681, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=1.061, AUC: opp-cc=0.324 \n",
      "\tAvg Val Loss: opp-mlo=0.702, AUC: opp-mlo=0.664\n",
      "Iter=685, avg train loss=0.370, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.557 \n",
      "\tAvg Val Loss: same-mlo=0.711, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=1.055, AUC: opp-cc=0.326 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.662\n",
      "Iter=690, avg train loss=0.437, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.532 \n",
      "\tAvg Val Loss: same-mlo=0.722, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=1.043, AUC: opp-cc=0.338 \n",
      "\tAvg Val Loss: opp-mlo=0.633, AUC: opp-mlo=0.660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=695, avg train loss=0.310, \n",
      "\tAvg Val Loss: same-cc=0.669, AUC: same-cc=0.520 \n",
      "\tAvg Val Loss: same-mlo=0.770, AUC: same-mlo=0.573 \n",
      "\tAvg Val Loss opp-cc=1.008, AUC: opp-cc=0.360 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.638\n",
      "Iter=700, avg train loss=0.359, \n",
      "\tAvg Val Loss: same-cc=0.684, AUC: same-cc=0.516 \n",
      "\tAvg Val Loss: same-mlo=0.783, AUC: same-mlo=0.555 \n",
      "\tAvg Val Loss opp-cc=1.026, AUC: opp-cc=0.360 \n",
      "\tAvg Val Loss: opp-mlo=0.694, AUC: opp-mlo=0.619\n",
      "Iter=705, avg train loss=0.465, \n",
      "\tAvg Val Loss: same-cc=0.734, AUC: same-cc=0.526 \n",
      "\tAvg Val Loss: same-mlo=0.839, AUC: same-mlo=0.538 \n",
      "\tAvg Val Loss opp-cc=1.085, AUC: opp-cc=0.334 \n",
      "\tAvg Val Loss: opp-mlo=0.760, AUC: opp-mlo=0.587\n",
      "Iter=710, avg train loss=0.345, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.506 \n",
      "\tAvg Val Loss: same-mlo=0.976, AUC: same-mlo=0.500 \n",
      "\tAvg Val Loss opp-cc=1.059, AUC: opp-cc=0.354 \n",
      "\tAvg Val Loss: opp-mlo=0.736, AUC: opp-mlo=0.583\n",
      "Iter=715, avg train loss=0.300, \n",
      "\tAvg Val Loss: same-cc=0.703, AUC: same-cc=0.528 \n",
      "\tAvg Val Loss: same-mlo=1.057, AUC: same-mlo=0.478 \n",
      "\tAvg Val Loss opp-cc=1.056, AUC: opp-cc=0.366 \n",
      "\tAvg Val Loss: opp-mlo=0.758, AUC: opp-mlo=0.603\n",
      "Iter=720, avg train loss=0.523, \n",
      "\tAvg Val Loss: same-cc=0.720, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=1.022, AUC: same-mlo=0.474 \n",
      "\tAvg Val Loss opp-cc=1.022, AUC: opp-cc=0.385 \n",
      "\tAvg Val Loss: opp-mlo=0.730, AUC: opp-mlo=0.611\n",
      "Iter=725, avg train loss=0.304, \n",
      "\tAvg Val Loss: same-cc=0.811, AUC: same-cc=0.577 \n",
      "\tAvg Val Loss: same-mlo=0.911, AUC: same-mlo=0.500 \n",
      "\tAvg Val Loss opp-cc=1.061, AUC: opp-cc=0.372 \n",
      "\tAvg Val Loss: opp-mlo=0.865, AUC: opp-mlo=0.609\n",
      "Iter=730, avg train loss=0.256, \n",
      "\tAvg Val Loss: same-cc=0.779, AUC: same-cc=0.567 \n",
      "\tAvg Val Loss: same-mlo=0.926, AUC: same-mlo=0.524 \n",
      "\tAvg Val Loss opp-cc=1.079, AUC: opp-cc=0.358 \n",
      "\tAvg Val Loss: opp-mlo=0.839, AUC: opp-mlo=0.621\n",
      "Iter=735, avg train loss=0.403, \n",
      "\tAvg Val Loss: same-cc=0.738, AUC: same-cc=0.547 \n",
      "\tAvg Val Loss: same-mlo=0.913, AUC: same-mlo=0.540 \n",
      "\tAvg Val Loss opp-cc=1.171, AUC: opp-cc=0.332 \n",
      "\tAvg Val Loss: opp-mlo=0.753, AUC: opp-mlo=0.650\n",
      "Iter=740, avg train loss=0.389, \n",
      "\tAvg Val Loss: same-cc=0.690, AUC: same-cc=0.543 \n",
      "\tAvg Val Loss: same-mlo=0.965, AUC: same-mlo=0.526 \n",
      "\tAvg Val Loss opp-cc=1.069, AUC: opp-cc=0.354 \n",
      "\tAvg Val Loss: opp-mlo=0.652, AUC: opp-mlo=0.692\n",
      "Iter=745, avg train loss=0.314, \n",
      "\tAvg Val Loss: same-cc=0.679, AUC: same-cc=0.528 \n",
      "\tAvg Val Loss: same-mlo=0.815, AUC: same-mlo=0.567 \n",
      "\tAvg Val Loss opp-cc=1.002, AUC: opp-cc=0.381 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.694\n",
      "Iter=750, avg train loss=0.385, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.538 \n",
      "\tAvg Val Loss: same-mlo=0.715, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.945, AUC: opp-cc=0.391 \n",
      "\tAvg Val Loss: opp-mlo=0.656, AUC: opp-mlo=0.698\n",
      "Best opp-mlo model saved.\n",
      "Iter=755, avg train loss=0.533, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.567 \n",
      "\tAvg Val Loss: same-mlo=0.707, AUC: same-mlo=0.652 \n",
      "\tAvg Val Loss opp-cc=0.901, AUC: opp-cc=0.407 \n",
      "\tAvg Val Loss: opp-mlo=0.747, AUC: opp-mlo=0.656\n",
      "Iter=760, avg train loss=0.277, \n",
      "\tAvg Val Loss: same-cc=0.651, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.729, AUC: same-mlo=0.668 \n",
      "\tAvg Val Loss opp-cc=0.892, AUC: opp-cc=0.399 \n",
      "\tAvg Val Loss: opp-mlo=0.813, AUC: opp-mlo=0.654\n",
      "Best models loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training: same-cc=0.540, same-mlo=0.708, opp-cc=0.550, opp-mlo=0.604\n",
      "Max-Score Based AUC After Training: 0.690, Mean-Score Based AUC After Training: 0.643\n",
      "\n",
      "\n",
      "\n",
      "========== Fold 2 ==========\n",
      "Test AUC at start: same-cc=0.393, same-mlo=0.457, opp-cc=0.380, opp-mlo=0.244\n",
      "Max-Score Based AUC Before Training: 0.365, Mean-Score Based AUC Before Training: 0.348\n",
      "Iter=5, avg train loss=1.255, \n",
      "\tAvg Val Loss: same-cc=0.591, AUC: same-cc=0.535 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.582 \n",
      "\tAvg Val Loss opp-cc=0.558, AUC: opp-cc=0.586 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.377\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=10, avg train loss=0.852, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.576, AUC: opp-cc=0.578 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.424\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=15, avg train loss=1.034, \n",
      "\tAvg Val Loss: same-cc=0.641, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.566 \n",
      "\tAvg Val Loss opp-cc=0.584, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.653, AUC: opp-mlo=0.430\n",
      "Best opp-mlo model saved.\n",
      "Iter=20, avg train loss=0.933, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.517 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.556 \n",
      "\tAvg Val Loss opp-cc=0.602, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.420\n",
      "Iter=25, avg train loss=0.771, \n",
      "\tAvg Val Loss: same-cc=0.637, AUC: same-cc=0.511 \n",
      "\tAvg Val Loss: same-mlo=0.614, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.603, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.410\n",
      "Iter=30, avg train loss=0.846, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.501 \n",
      "\tAvg Val Loss: same-mlo=0.605, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.623, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.454\n",
      "Best opp-mlo model saved.\n",
      "Iter=35, avg train loss=0.793, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.523 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.489 \n",
      "\tAvg Val Loss: opp-mlo=0.627, AUC: opp-mlo=0.507\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=40, avg train loss=0.711, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.550 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.641 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.556\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=45, avg train loss=0.632, \n",
      "\tAvg Val Loss: same-cc=0.627, AUC: same-cc=0.554 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.673 \n",
      "\tAvg Val Loss opp-cc=0.638, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.610, AUC: opp-mlo=0.582\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=50, avg train loss=0.776, \n",
      "\tAvg Val Loss: same-cc=0.603, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.562, AUC: same-mlo=0.700 \n",
      "\tAvg Val Loss opp-cc=0.639, AUC: opp-cc=0.489 \n",
      "\tAvg Val Loss: opp-mlo=0.601, AUC: opp-mlo=0.594\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=55, avg train loss=0.807, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.577, AUC: same-mlo=0.665 \n",
      "\tAvg Val Loss opp-cc=0.638, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.609, AUC: opp-mlo=0.604\n",
      "Best opp-mlo model saved.\n",
      "Iter=60, avg train loss=0.774, \n",
      "\tAvg Val Loss: same-cc=0.636, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.588, AUC: same-mlo=0.690 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.610, AUC: opp-mlo=0.617\n",
      "Best opp-mlo model saved.\n",
      "Iter=65, avg train loss=0.686, \n",
      "\tAvg Val Loss: same-cc=0.651, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.706 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.633, AUC: opp-mlo=0.615\n",
      "Best same-mlo model saved.\n",
      "Iter=70, avg train loss=0.644, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.560 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.730 \n",
      "\tAvg Val Loss opp-cc=0.678, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.619\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=75, avg train loss=0.705, \n",
      "\tAvg Val Loss: same-cc=0.610, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.750 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.635\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=80, avg train loss=0.676, \n",
      "\tAvg Val Loss: same-cc=0.610, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.757 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.643\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=85, avg train loss=0.759, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.755 \n",
      "\tAvg Val Loss opp-cc=0.678, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.645\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=90, avg train loss=0.733, \n",
      "\tAvg Val Loss: same-cc=0.642, AUC: same-cc=0.594 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.757 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.642, AUC: opp-mlo=0.637\n",
      "Best same-cc model saved.\n",
      "Iter=95, avg train loss=0.697, \n",
      "\tAvg Val Loss: same-cc=0.642, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.748 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.665, AUC: opp-mlo=0.623\n",
      "Iter=100, avg train loss=0.671, \n",
      "\tAvg Val Loss: same-cc=0.638, AUC: same-cc=0.592 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.763 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.667, AUC: opp-mlo=0.631\n",
      "Best same-mlo model saved.\n",
      "Iter=105, avg train loss=0.697, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.648, AUC: same-mlo=0.771 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.617\n",
      "Best same-mlo model saved.\n",
      "Iter=110, avg train loss=0.691, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.779 \n",
      "\tAvg Val Loss opp-cc=0.710, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.671, AUC: opp-mlo=0.627\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Iter=115, avg train loss=0.698, \n",
      "\tAvg Val Loss: same-cc=0.663, AUC: same-cc=0.602 \n",
      "\tAvg Val Loss: same-mlo=0.662, AUC: same-mlo=0.779 \n",
      "\tAvg Val Loss opp-cc=0.707, AUC: opp-cc=0.456 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.647\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=120, avg train loss=0.688, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.626, AUC: same-mlo=0.805 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.650, AUC: opp-mlo=0.653\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=125, avg train loss=0.668, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.809 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.653\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Iter=130, avg train loss=0.665, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.813 \n",
      "\tAvg Val Loss opp-cc=0.686, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.677\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=135, avg train loss=0.648, \n",
      "\tAvg Val Loss: same-cc=0.640, AUC: same-cc=0.623 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.811 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.601, AUC: opp-mlo=0.694\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=140, avg train loss=0.692, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.613 \n",
      "\tAvg Val Loss: same-mlo=0.551, AUC: same-mlo=0.813 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.499 \n",
      "\tAvg Val Loss: opp-mlo=0.578, AUC: opp-mlo=0.708\n",
      "Best opp-mlo model saved.\n",
      "Iter=145, avg train loss=0.703, \n",
      "\tAvg Val Loss: same-cc=0.643, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.545, AUC: same-mlo=0.815 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.583, AUC: opp-mlo=0.714\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=150, avg train loss=0.711, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.585, AUC: same-mlo=0.807 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.714\n",
      "Iter=155, avg train loss=0.682, \n",
      "\tAvg Val Loss: same-cc=0.674, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.813 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.609, AUC: opp-mlo=0.724\n",
      "Best opp-mlo model saved.\n",
      "Iter=160, avg train loss=0.662, \n",
      "\tAvg Val Loss: same-cc=0.661, AUC: same-cc=0.576 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.819 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.734\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=165, avg train loss=0.642, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.584 \n",
      "\tAvg Val Loss: same-mlo=0.570, AUC: same-mlo=0.811 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.742\n",
      "Best opp-mlo model saved.\n",
      "Iter=170, avg train loss=0.642, \n",
      "\tAvg Val Loss: same-cc=0.605, AUC: same-cc=0.617 \n",
      "\tAvg Val Loss: same-mlo=0.570, AUC: same-mlo=0.821 \n",
      "\tAvg Val Loss opp-cc=0.657, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.761\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=175, avg train loss=0.667, \n",
      "\tAvg Val Loss: same-cc=0.600, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.559, AUC: same-mlo=0.813 \n",
      "\tAvg Val Loss opp-cc=0.635, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.585, AUC: opp-mlo=0.757\n",
      "Iter=180, avg train loss=0.676, \n",
      "\tAvg Val Loss: same-cc=0.608, AUC: same-cc=0.615 \n",
      "\tAvg Val Loss: same-mlo=0.555, AUC: same-mlo=0.822 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.577, AUC: opp-mlo=0.751\n",
      "Best same-mlo model saved.\n",
      "Iter=185, avg train loss=0.606, \n",
      "\tAvg Val Loss: same-cc=0.585, AUC: same-cc=0.641 \n",
      "\tAvg Val Loss: same-mlo=0.543, AUC: same-mlo=0.821 \n",
      "\tAvg Val Loss opp-cc=0.645, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.572, AUC: opp-mlo=0.763\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=190, avg train loss=0.647, \n",
      "\tAvg Val Loss: same-cc=0.568, AUC: same-cc=0.661 \n",
      "\tAvg Val Loss: same-mlo=0.533, AUC: same-mlo=0.822 \n",
      "\tAvg Val Loss opp-cc=0.619, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.559, AUC: opp-mlo=0.757\n",
      "Best same-cc model saved.\n",
      "Iter=195, avg train loss=0.600, \n",
      "\tAvg Val Loss: same-cc=0.571, AUC: same-cc=0.625 \n",
      "\tAvg Val Loss: same-mlo=0.509, AUC: same-mlo=0.826 \n",
      "\tAvg Val Loss opp-cc=0.607, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.538, AUC: opp-mlo=0.744\n",
      "Best same-mlo model saved.\n",
      "Iter=200, avg train loss=0.650, \n",
      "\tAvg Val Loss: same-cc=0.562, AUC: same-cc=0.635 \n",
      "\tAvg Val Loss: same-mlo=0.477, AUC: same-mlo=0.834 \n",
      "\tAvg Val Loss opp-cc=0.601, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.528, AUC: opp-mlo=0.744\n",
      "Best same-mlo model saved.\n",
      "Iter=205, avg train loss=0.595, \n",
      "\tAvg Val Loss: same-cc=0.573, AUC: same-cc=0.639 \n",
      "\tAvg Val Loss: same-mlo=0.469, AUC: same-mlo=0.840 \n",
      "\tAvg Val Loss opp-cc=0.607, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.521, AUC: opp-mlo=0.748\n",
      "Best same-mlo model saved.\n",
      "Iter=210, avg train loss=0.689, \n",
      "\tAvg Val Loss: same-cc=0.636, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.480, AUC: same-mlo=0.840 \n",
      "\tAvg Val Loss opp-cc=0.620, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.542, AUC: opp-mlo=0.757\n",
      "Iter=215, avg train loss=0.640, \n",
      "\tAvg Val Loss: same-cc=0.627, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.519, AUC: same-mlo=0.844 \n",
      "\tAvg Val Loss opp-cc=0.632, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.554, AUC: opp-mlo=0.742\n",
      "Best same-mlo model saved.\n",
      "Iter=220, avg train loss=0.615, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.547, AUC: same-mlo=0.852 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.525 \n",
      "\tAvg Val Loss: opp-mlo=0.570, AUC: opp-mlo=0.748\n",
      "Best same-mlo model saved.\n",
      "Iter=225, avg train loss=0.681, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.564 \n",
      "\tAvg Val Loss: same-mlo=0.537, AUC: same-mlo=0.860 \n",
      "\tAvg Val Loss opp-cc=0.632, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.584, AUC: opp-mlo=0.748\n",
      "Best same-mlo model saved.\n",
      "Iter=230, avg train loss=0.588, \n",
      "\tAvg Val Loss: same-cc=0.611, AUC: same-cc=0.566 \n",
      "\tAvg Val Loss: same-mlo=0.521, AUC: same-mlo=0.854 \n",
      "\tAvg Val Loss opp-cc=0.632, AUC: opp-cc=0.535 \n",
      "\tAvg Val Loss: opp-mlo=0.569, AUC: opp-mlo=0.740\n",
      "Iter=235, avg train loss=0.601, \n",
      "\tAvg Val Loss: same-cc=0.614, AUC: same-cc=0.538 \n",
      "\tAvg Val Loss: same-mlo=0.485, AUC: same-mlo=0.856 \n",
      "\tAvg Val Loss opp-cc=0.638, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.539, AUC: opp-mlo=0.740\n",
      "Iter=240, avg train loss=0.624, \n",
      "\tAvg Val Loss: same-cc=0.596, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.463, AUC: same-mlo=0.858 \n",
      "\tAvg Val Loss opp-cc=0.622, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.517, AUC: opp-mlo=0.736\n",
      "Iter=245, avg train loss=0.680, \n",
      "\tAvg Val Loss: same-cc=0.583, AUC: same-cc=0.556 \n",
      "\tAvg Val Loss: same-mlo=0.470, AUC: same-mlo=0.856 \n",
      "\tAvg Val Loss opp-cc=0.622, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.518, AUC: opp-mlo=0.736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=250, avg train loss=0.687, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.493, AUC: same-mlo=0.856 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.546, AUC: opp-mlo=0.751\n",
      "Iter=255, avg train loss=0.657, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.533 \n",
      "\tAvg Val Loss: same-mlo=0.524, AUC: same-mlo=0.838 \n",
      "\tAvg Val Loss opp-cc=0.658, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.577, AUC: opp-mlo=0.773\n",
      "Best opp-mlo model saved.\n",
      "Iter=260, avg train loss=0.591, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.556, AUC: same-mlo=0.830 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.781\n",
      "Best opp-mlo model saved.\n",
      "Iter=265, avg train loss=0.608, \n",
      "\tAvg Val Loss: same-cc=0.721, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.826 \n",
      "\tAvg Val Loss opp-cc=0.678, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.648, AUC: opp-mlo=0.781\n",
      "Iter=270, avg train loss=0.648, \n",
      "\tAvg Val Loss: same-cc=0.740, AUC: same-cc=0.548 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.830 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.564 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.771\n",
      "Iter=275, avg train loss=0.607, \n",
      "\tAvg Val Loss: same-cc=0.814, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.830 \n",
      "\tAvg Val Loss opp-cc=0.725, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.647, AUC: opp-mlo=0.767\n",
      "Iter=280, avg train loss=0.589, \n",
      "\tAvg Val Loss: same-cc=0.737, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.559, AUC: same-mlo=0.819 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.586 \n",
      "\tAvg Val Loss: opp-mlo=0.584, AUC: opp-mlo=0.763\n",
      "Best opp-cc model saved.\n",
      "Iter=285, avg train loss=0.701, \n",
      "\tAvg Val Loss: same-cc=0.741, AUC: same-cc=0.544 \n",
      "\tAvg Val Loss: same-mlo=0.576, AUC: same-mlo=0.813 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.596 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.781\n",
      "Best opp-cc model saved.\n",
      "Iter=290, avg train loss=0.676, \n",
      "\tAvg Val Loss: same-cc=0.752, AUC: same-cc=0.535 \n",
      "\tAvg Val Loss: same-mlo=0.562, AUC: same-mlo=0.805 \n",
      "\tAvg Val Loss opp-cc=0.702, AUC: opp-cc=0.584 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.779\n",
      "Iter=295, avg train loss=0.578, \n",
      "\tAvg Val Loss: same-cc=0.719, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.547, AUC: same-mlo=0.801 \n",
      "\tAvg Val Loss opp-cc=0.710, AUC: opp-cc=0.576 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.779\n",
      "Iter=300, avg train loss=0.637, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.562 \n",
      "\tAvg Val Loss: same-mlo=0.529, AUC: same-mlo=0.803 \n",
      "\tAvg Val Loss opp-cc=0.701, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.597, AUC: opp-mlo=0.779\n",
      "Iter=305, avg train loss=0.576, \n",
      "\tAvg Val Loss: same-cc=0.721, AUC: same-cc=0.544 \n",
      "\tAvg Val Loss: same-mlo=0.575, AUC: same-mlo=0.797 \n",
      "\tAvg Val Loss opp-cc=0.727, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.644, AUC: opp-mlo=0.759\n",
      "Iter=310, avg train loss=0.617, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.546, AUC: same-mlo=0.799 \n",
      "\tAvg Val Loss opp-cc=0.725, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.597, AUC: opp-mlo=0.753\n",
      "Iter=315, avg train loss=0.570, \n",
      "\tAvg Val Loss: same-cc=0.709, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.787 \n",
      "\tAvg Val Loss opp-cc=0.732, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.738\n",
      "Iter=320, avg train loss=0.645, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.787 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.722\n",
      "Iter=325, avg train loss=0.595, \n",
      "\tAvg Val Loss: same-cc=0.644, AUC: same-cc=0.554 \n",
      "\tAvg Val Loss: same-mlo=0.573, AUC: same-mlo=0.779 \n",
      "\tAvg Val Loss opp-cc=0.711, AUC: opp-cc=0.471 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.706\n",
      "Iter=330, avg train loss=0.643, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.562, AUC: same-mlo=0.763 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.471 \n",
      "\tAvg Val Loss: opp-mlo=0.648, AUC: opp-mlo=0.688\n",
      "Iter=335, avg train loss=0.621, \n",
      "\tAvg Val Loss: same-cc=0.713, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.628, AUC: same-mlo=0.744 \n",
      "\tAvg Val Loss opp-cc=0.732, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.744, AUC: opp-mlo=0.667\n",
      "Iter=340, avg train loss=0.668, \n",
      "\tAvg Val Loss: same-cc=0.750, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.651, AUC: same-mlo=0.742 \n",
      "\tAvg Val Loss opp-cc=0.713, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.758, AUC: opp-mlo=0.673\n",
      "Iter=345, avg train loss=0.599, \n",
      "\tAvg Val Loss: same-cc=0.717, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.627, AUC: same-mlo=0.773 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.756, AUC: opp-mlo=0.712\n",
      "Iter=350, avg train loss=0.557, \n",
      "\tAvg Val Loss: same-cc=0.642, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.556, AUC: same-mlo=0.807 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.698, AUC: opp-mlo=0.710\n",
      "Iter=355, avg train loss=0.561, \n",
      "\tAvg Val Loss: same-cc=0.638, AUC: same-cc=0.598 \n",
      "\tAvg Val Loss: same-mlo=0.492, AUC: same-mlo=0.836 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.646, AUC: opp-mlo=0.696\n",
      "Iter=360, avg train loss=0.605, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.543, AUC: same-mlo=0.830 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.700\n",
      "Iter=365, avg train loss=0.569, \n",
      "\tAvg Val Loss: same-cc=0.710, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.822 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.690, AUC: opp-mlo=0.720\n",
      "Iter=370, avg train loss=0.610, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.617, AUC: same-mlo=0.807 \n",
      "\tAvg Val Loss opp-cc=0.719, AUC: opp-cc=0.499 \n",
      "\tAvg Val Loss: opp-mlo=0.693, AUC: opp-mlo=0.732\n",
      "Iter=375, avg train loss=0.550, \n",
      "\tAvg Val Loss: same-cc=0.622, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.477, AUC: same-mlo=0.815 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.587, AUC: opp-mlo=0.738\n",
      "Iter=380, avg train loss=0.530, \n",
      "\tAvg Val Loss: same-cc=0.605, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.435, AUC: same-mlo=0.809 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.533 \n",
      "\tAvg Val Loss: opp-mlo=0.554, AUC: opp-mlo=0.730\n",
      "Iter=385, avg train loss=0.652, \n",
      "\tAvg Val Loss: same-cc=0.616, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.441, AUC: same-mlo=0.787 \n",
      "\tAvg Val Loss opp-cc=0.653, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.548, AUC: opp-mlo=0.742\n",
      "Iter=390, avg train loss=0.579, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.485, AUC: same-mlo=0.767 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.744\n",
      "Iter=395, avg train loss=0.507, \n",
      "\tAvg Val Loss: same-cc=0.639, AUC: same-cc=0.560 \n",
      "\tAvg Val Loss: same-mlo=0.499, AUC: same-mlo=0.726 \n",
      "\tAvg Val Loss opp-cc=0.653, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.609, AUC: opp-mlo=0.734\n",
      "Iter=400, avg train loss=0.602, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.556 \n",
      "\tAvg Val Loss: same-mlo=0.520, AUC: same-mlo=0.734 \n",
      "\tAvg Val Loss opp-cc=0.655, AUC: opp-cc=0.497 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.720\n",
      "Iter=405, avg train loss=0.557, \n",
      "\tAvg Val Loss: same-cc=0.639, AUC: same-cc=0.538 \n",
      "\tAvg Val Loss: same-mlo=0.500, AUC: same-mlo=0.732 \n",
      "\tAvg Val Loss opp-cc=0.667, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.592, AUC: opp-mlo=0.718\n",
      "Iter=410, avg train loss=0.560, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.523 \n",
      "\tAvg Val Loss: same-mlo=0.500, AUC: same-mlo=0.732 \n",
      "\tAvg Val Loss opp-cc=0.701, AUC: opp-cc=0.464 \n",
      "\tAvg Val Loss: opp-mlo=0.583, AUC: opp-mlo=0.698\n",
      "Iter=415, avg train loss=0.565, \n",
      "\tAvg Val Loss: same-cc=0.682, AUC: same-cc=0.533 \n",
      "\tAvg Val Loss: same-mlo=0.509, AUC: same-mlo=0.736 \n",
      "\tAvg Val Loss opp-cc=0.741, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.589, AUC: opp-mlo=0.684\n",
      "Iter=420, avg train loss=0.533, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.560 \n",
      "\tAvg Val Loss: same-mlo=0.533, AUC: same-mlo=0.718 \n",
      "\tAvg Val Loss opp-cc=0.773, AUC: opp-cc=0.440 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.675\n",
      "Iter=425, avg train loss=0.588, \n",
      "\tAvg Val Loss: same-cc=0.680, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.495, AUC: same-mlo=0.753 \n",
      "\tAvg Val Loss opp-cc=0.738, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.575, AUC: opp-mlo=0.704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=430, avg train loss=0.412, \n",
      "\tAvg Val Loss: same-cc=0.700, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.509, AUC: same-mlo=0.769 \n",
      "\tAvg Val Loss opp-cc=0.700, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.716\n",
      "Iter=435, avg train loss=0.492, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.470, AUC: same-mlo=0.783 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.550, AUC: opp-mlo=0.694\n",
      "Iter=440, avg train loss=0.613, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.576 \n",
      "\tAvg Val Loss: same-mlo=0.511, AUC: same-mlo=0.771 \n",
      "\tAvg Val Loss opp-cc=0.712, AUC: opp-cc=0.452 \n",
      "\tAvg Val Loss: opp-mlo=0.592, AUC: opp-mlo=0.700\n",
      "Iter=445, avg train loss=0.526, \n",
      "\tAvg Val Loss: same-cc=0.718, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.556, AUC: same-mlo=0.755 \n",
      "\tAvg Val Loss opp-cc=0.771, AUC: opp-cc=0.448 \n",
      "\tAvg Val Loss: opp-mlo=0.636, AUC: opp-mlo=0.700\n",
      "Iter=450, avg train loss=0.442, \n",
      "\tAvg Val Loss: same-cc=0.719, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.569, AUC: same-mlo=0.751 \n",
      "\tAvg Val Loss opp-cc=0.836, AUC: opp-cc=0.424 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.700\n",
      "Iter=455, avg train loss=0.586, \n",
      "\tAvg Val Loss: same-cc=0.690, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.521, AUC: same-mlo=0.755 \n",
      "\tAvg Val Loss opp-cc=0.838, AUC: opp-cc=0.442 \n",
      "\tAvg Val Loss: opp-mlo=0.586, AUC: opp-mlo=0.716\n",
      "Iter=460, avg train loss=0.529, \n",
      "\tAvg Val Loss: same-cc=0.699, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.494, AUC: same-mlo=0.779 \n",
      "\tAvg Val Loss opp-cc=0.827, AUC: opp-cc=0.442 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.706\n",
      "Iter=465, avg train loss=0.506, \n",
      "\tAvg Val Loss: same-cc=0.703, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.504, AUC: same-mlo=0.763 \n",
      "\tAvg Val Loss opp-cc=0.711, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.688\n",
      "Iter=470, avg train loss=0.495, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.706 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.637, AUC: opp-mlo=0.684\n",
      "Iter=475, avg train loss=0.521, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.592 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.694 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.641, AUC: opp-mlo=0.702\n",
      "Iter=480, avg train loss=0.455, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.706, AUC: same-mlo=0.677 \n",
      "\tAvg Val Loss opp-cc=0.678, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.743, AUC: opp-mlo=0.692\n",
      "Iter=485, avg train loss=0.657, \n",
      "\tAvg Val Loss: same-cc=0.649, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.555, AUC: same-mlo=0.716 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.704, AUC: opp-mlo=0.700\n",
      "Iter=490, avg train loss=0.457, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.480, AUC: same-mlo=0.757 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.584, AUC: opp-mlo=0.716\n",
      "Iter=495, avg train loss=0.439, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.600 \n",
      "\tAvg Val Loss: same-mlo=0.458, AUC: same-mlo=0.797 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.609, AUC: opp-mlo=0.755\n",
      "Iter=500, avg train loss=0.424, \n",
      "\tAvg Val Loss: same-cc=0.614, AUC: same-cc=0.613 \n",
      "\tAvg Val Loss: same-mlo=0.452, AUC: same-mlo=0.803 \n",
      "\tAvg Val Loss opp-cc=0.609, AUC: opp-cc=0.562 \n",
      "\tAvg Val Loss: opp-mlo=0.566, AUC: opp-mlo=0.779\n",
      "Iter=505, avg train loss=0.593, \n",
      "\tAvg Val Loss: same-cc=0.657, AUC: same-cc=0.594 \n",
      "\tAvg Val Loss: same-mlo=0.462, AUC: same-mlo=0.801 \n",
      "\tAvg Val Loss opp-cc=0.671, AUC: opp-cc=0.550 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.779\n",
      "Iter=510, avg train loss=0.524, \n",
      "\tAvg Val Loss: same-cc=0.767, AUC: same-cc=0.564 \n",
      "\tAvg Val Loss: same-mlo=0.553, AUC: same-mlo=0.757 \n",
      "\tAvg Val Loss opp-cc=0.777, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.716, AUC: opp-mlo=0.765\n",
      "Iter=515, avg train loss=0.605, \n",
      "\tAvg Val Loss: same-cc=0.758, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.551, AUC: same-mlo=0.718 \n",
      "\tAvg Val Loss opp-cc=0.786, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.673, AUC: opp-mlo=0.750\n",
      "Iter=520, avg train loss=0.446, \n",
      "\tAvg Val Loss: same-cc=0.684, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.510, AUC: same-mlo=0.714 \n",
      "\tAvg Val Loss opp-cc=0.759, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.540, AUC: opp-mlo=0.694\n",
      "Iter=525, avg train loss=0.482, \n",
      "\tAvg Val Loss: same-cc=0.646, AUC: same-cc=0.592 \n",
      "\tAvg Val Loss: same-mlo=0.512, AUC: same-mlo=0.686 \n",
      "\tAvg Val Loss opp-cc=0.730, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.542, AUC: opp-mlo=0.673\n",
      "Iter=530, avg train loss=0.419, \n",
      "\tAvg Val Loss: same-cc=0.662, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.513, AUC: same-mlo=0.698 \n",
      "\tAvg Val Loss opp-cc=0.715, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.667\n",
      "Iter=535, avg train loss=0.463, \n",
      "\tAvg Val Loss: same-cc=0.684, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.533, AUC: same-mlo=0.704 \n",
      "\tAvg Val Loss opp-cc=0.744, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.655, AUC: opp-mlo=0.651\n",
      "Iter=540, avg train loss=0.544, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.584 \n",
      "\tAvg Val Loss: same-mlo=0.536, AUC: same-mlo=0.716 \n",
      "\tAvg Val Loss opp-cc=0.795, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.648, AUC: opp-mlo=0.661\n",
      "Iter=545, avg train loss=0.460, \n",
      "\tAvg Val Loss: same-cc=0.802, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.563, AUC: same-mlo=0.714 \n",
      "\tAvg Val Loss opp-cc=0.795, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.653, AUC: opp-mlo=0.657\n",
      "Iter=550, avg train loss=0.440, \n",
      "\tAvg Val Loss: same-cc=0.765, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.714 \n",
      "\tAvg Val Loss opp-cc=0.742, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.635, AUC: opp-mlo=0.675\n",
      "Iter=555, avg train loss=0.404, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.718 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.662, AUC: opp-mlo=0.698\n",
      "Iter=560, avg train loss=0.461, \n",
      "\tAvg Val Loss: same-cc=0.618, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.552, AUC: same-mlo=0.746 \n",
      "\tAvg Val Loss opp-cc=0.664, AUC: opp-cc=0.544 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.712\n",
      "Iter=565, avg train loss=0.428, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.584 \n",
      "\tAvg Val Loss: same-mlo=0.527, AUC: same-mlo=0.744 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.726\n",
      "Iter=570, avg train loss=0.497, \n",
      "\tAvg Val Loss: same-cc=0.622, AUC: same-cc=0.562 \n",
      "\tAvg Val Loss: same-mlo=0.478, AUC: same-mlo=0.759 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.541, AUC: opp-mlo=0.692\n",
      "Iter=575, avg train loss=0.448, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.484, AUC: same-mlo=0.736 \n",
      "\tAvg Val Loss opp-cc=0.710, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.593, AUC: opp-mlo=0.688\n",
      "Iter=580, avg train loss=0.417, \n",
      "\tAvg Val Loss: same-cc=0.718, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.515, AUC: same-mlo=0.732 \n",
      "\tAvg Val Loss opp-cc=0.841, AUC: opp-cc=0.501 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.750\n",
      "Iter=585, avg train loss=0.419, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.506, AUC: same-mlo=0.732 \n",
      "\tAvg Val Loss opp-cc=0.733, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.579, AUC: opp-mlo=0.761\n",
      "Iter=590, avg train loss=0.499, \n",
      "\tAvg Val Loss: same-cc=0.745, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.554, AUC: same-mlo=0.732 \n",
      "\tAvg Val Loss opp-cc=0.742, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.601, AUC: opp-mlo=0.763\n",
      "Iter=595, avg train loss=0.460, \n",
      "\tAvg Val Loss: same-cc=0.801, AUC: same-cc=0.556 \n",
      "\tAvg Val Loss: same-mlo=0.624, AUC: same-mlo=0.722 \n",
      "\tAvg Val Loss opp-cc=0.766, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.751\n",
      "Iter=600, avg train loss=0.536, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.592 \n",
      "\tAvg Val Loss: same-mlo=0.552, AUC: same-mlo=0.726 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.564, AUC: opp-mlo=0.724\n",
      "Iter=605, avg train loss=0.515, \n",
      "\tAvg Val Loss: same-cc=0.611, AUC: same-cc=0.598 \n",
      "\tAvg Val Loss: same-mlo=0.500, AUC: same-mlo=0.734 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.533 \n",
      "\tAvg Val Loss: opp-mlo=0.541, AUC: opp-mlo=0.680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=610, avg train loss=0.412, \n",
      "\tAvg Val Loss: same-cc=0.622, AUC: same-cc=0.602 \n",
      "\tAvg Val Loss: same-mlo=0.484, AUC: same-mlo=0.736 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.544 \n",
      "\tAvg Val Loss: opp-mlo=0.548, AUC: opp-mlo=0.694\n",
      "Iter=615, avg train loss=0.419, \n",
      "\tAvg Val Loss: same-cc=0.720, AUC: same-cc=0.576 \n",
      "\tAvg Val Loss: same-mlo=0.487, AUC: same-mlo=0.748 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.587, AUC: opp-mlo=0.714\n",
      "Iter=620, avg train loss=0.393, \n",
      "\tAvg Val Loss: same-cc=0.810, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.481, AUC: same-mlo=0.740 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.674, AUC: opp-mlo=0.740\n",
      "Iter=625, avg train loss=0.464, \n",
      "\tAvg Val Loss: same-cc=0.884, AUC: same-cc=0.568 \n",
      "\tAvg Val Loss: same-mlo=0.567, AUC: same-mlo=0.714 \n",
      "\tAvg Val Loss opp-cc=0.811, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.669, AUC: opp-mlo=0.722\n",
      "Iter=630, avg train loss=0.460, \n",
      "\tAvg Val Loss: same-cc=0.640, AUC: same-cc=0.586 \n",
      "\tAvg Val Loss: same-mlo=0.591, AUC: same-mlo=0.714 \n",
      "\tAvg Val Loss opp-cc=0.888, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.587, AUC: opp-mlo=0.712\n",
      "Iter=635, avg train loss=0.379, \n",
      "\tAvg Val Loss: same-cc=0.601, AUC: same-cc=0.600 \n",
      "\tAvg Val Loss: same-mlo=0.520, AUC: same-mlo=0.738 \n",
      "\tAvg Val Loss opp-cc=0.743, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.558, AUC: opp-mlo=0.718\n",
      "Iter=640, avg train loss=0.395, \n",
      "\tAvg Val Loss: same-cc=0.618, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.491, AUC: same-mlo=0.765 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.525 \n",
      "\tAvg Val Loss: opp-mlo=0.563, AUC: opp-mlo=0.696\n",
      "Iter=645, avg train loss=0.460, \n",
      "\tAvg Val Loss: same-cc=0.735, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.510, AUC: same-mlo=0.730 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.700\n",
      "Iter=650, avg train loss=0.417, \n",
      "\tAvg Val Loss: same-cc=0.864, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.718 \n",
      "\tAvg Val Loss opp-cc=0.787, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.687, AUC: opp-mlo=0.704\n",
      "Iter=655, avg train loss=0.478, \n",
      "\tAvg Val Loss: same-cc=0.854, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.721, AUC: same-mlo=0.673 \n",
      "\tAvg Val Loss opp-cc=0.846, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.697, AUC: opp-mlo=0.686\n",
      "Iter=660, avg train loss=0.419, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.647, AUC: same-mlo=0.661 \n",
      "\tAvg Val Loss opp-cc=0.828, AUC: opp-cc=0.467 \n",
      "\tAvg Val Loss: opp-mlo=0.639, AUC: opp-mlo=0.663\n",
      "Iter=665, avg train loss=0.454, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.578, AUC: same-mlo=0.663 \n",
      "\tAvg Val Loss opp-cc=0.717, AUC: opp-cc=0.511 \n",
      "\tAvg Val Loss: opp-mlo=0.599, AUC: opp-mlo=0.659\n",
      "Iter=670, avg train loss=0.440, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.671 \n",
      "\tAvg Val Loss opp-cc=0.703, AUC: opp-cc=0.535 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.684\n",
      "Iter=675, avg train loss=0.365, \n",
      "\tAvg Val Loss: same-cc=0.676, AUC: same-cc=0.584 \n",
      "\tAvg Val Loss: same-mlo=0.520, AUC: same-mlo=0.714 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.544 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.686\n",
      "Iter=680, avg train loss=0.411, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.757 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.551, AUC: opp-mlo=0.694\n",
      "Iter=685, avg train loss=0.443, \n",
      "\tAvg Val Loss: same-cc=0.641, AUC: same-cc=0.607 \n",
      "\tAvg Val Loss: same-mlo=0.497, AUC: same-mlo=0.750 \n",
      "\tAvg Val Loss opp-cc=0.653, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.561, AUC: opp-mlo=0.698\n",
      "Iter=690, avg train loss=0.532, \n",
      "\tAvg Val Loss: same-cc=0.803, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.512, AUC: same-mlo=0.730 \n",
      "\tAvg Val Loss opp-cc=0.712, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.781, AUC: opp-mlo=0.698\n",
      "Iter=695, avg train loss=0.355, \n",
      "\tAvg Val Loss: same-cc=0.989, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.682 \n",
      "\tAvg Val Loss opp-cc=0.802, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.850, AUC: opp-mlo=0.669\n",
      "Iter=700, avg train loss=0.284, \n",
      "\tAvg Val Loss: same-cc=0.862, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.682 \n",
      "\tAvg Val Loss opp-cc=0.823, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.797, AUC: opp-mlo=0.673\n",
      "Iter=705, avg train loss=0.434, \n",
      "\tAvg Val Loss: same-cc=0.633, AUC: same-cc=0.623 \n",
      "\tAvg Val Loss: same-mlo=0.530, AUC: same-mlo=0.694 \n",
      "\tAvg Val Loss opp-cc=0.713, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.669\n",
      "Iter=710, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.617 \n",
      "\tAvg Val Loss: same-mlo=0.563, AUC: same-mlo=0.667 \n",
      "\tAvg Val Loss opp-cc=0.695, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.645\n",
      "Iter=715, avg train loss=0.391, \n",
      "\tAvg Val Loss: same-cc=0.657, AUC: same-cc=0.600 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.737, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.641, AUC: opp-mlo=0.635\n",
      "Iter=720, avg train loss=0.363, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.584 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.740, AUC: opp-cc=0.462 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.633\n",
      "Iter=725, avg train loss=0.394, \n",
      "\tAvg Val Loss: same-cc=0.643, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.587, AUC: same-mlo=0.647 \n",
      "\tAvg Val Loss opp-cc=0.753, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.639\n",
      "Iter=730, avg train loss=0.402, \n",
      "\tAvg Val Loss: same-cc=0.626, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.561, AUC: same-mlo=0.686 \n",
      "\tAvg Val Loss opp-cc=0.727, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.601, AUC: opp-mlo=0.663\n",
      "Iter=735, avg train loss=0.455, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.576 \n",
      "\tAvg Val Loss: same-mlo=0.548, AUC: same-mlo=0.696 \n",
      "\tAvg Val Loss opp-cc=0.733, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.680\n",
      "Iter=740, avg train loss=0.348, \n",
      "\tAvg Val Loss: same-cc=0.755, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.561, AUC: same-mlo=0.696 \n",
      "\tAvg Val Loss opp-cc=0.767, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.702\n",
      "Iter=745, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.600 \n",
      "\tAvg Val Loss: same-mlo=0.526, AUC: same-mlo=0.722 \n",
      "\tAvg Val Loss opp-cc=0.803, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.724\n",
      "Iter=750, avg train loss=0.329, \n",
      "\tAvg Val Loss: same-cc=0.618, AUC: same-cc=0.631 \n",
      "\tAvg Val Loss: same-mlo=0.495, AUC: same-mlo=0.746 \n",
      "\tAvg Val Loss opp-cc=0.744, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.738\n",
      "Iter=755, avg train loss=0.327, \n",
      "\tAvg Val Loss: same-cc=0.627, AUC: same-cc=0.627 \n",
      "\tAvg Val Loss: same-mlo=0.493, AUC: same-mlo=0.748 \n",
      "\tAvg Val Loss opp-cc=0.712, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.602, AUC: opp-mlo=0.722\n",
      "Iter=760, avg train loss=0.366, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.631 \n",
      "\tAvg Val Loss: same-mlo=0.490, AUC: same-mlo=0.753 \n",
      "\tAvg Val Loss opp-cc=0.695, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.545, AUC: opp-mlo=0.708\n",
      "Best models loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training: same-cc=0.628, same-mlo=0.673, opp-cc=0.451, opp-mlo=0.650\n",
      "Max-Score Based AUC After Training: 0.588, Mean-Score Based AUC After Training: 0.632\n",
      "\n",
      "\n",
      "\n",
      "========== Fold 3 ==========\n",
      "Test AUC at start: same-cc=0.429, same-mlo=0.397, opp-cc=0.358, opp-mlo=0.348\n",
      "Max-Score Based AUC Before Training: 0.399, Mean-Score Based AUC Before Training: 0.370\n",
      "Iter=5, avg train loss=1.131, \n",
      "\tAvg Val Loss: same-cc=0.578, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.546, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.590, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.598, AUC: opp-mlo=0.471\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=10, avg train loss=0.870, \n",
      "\tAvg Val Loss: same-cc=0.593, AUC: same-cc=0.548 \n",
      "\tAvg Val Loss: same-mlo=0.553, AUC: same-mlo=0.635 \n",
      "\tAvg Val Loss opp-cc=0.593, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.585, AUC: opp-mlo=0.515\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=15, avg train loss=0.698, \n",
      "\tAvg Val Loss: same-cc=0.614, AUC: same-cc=0.511 \n",
      "\tAvg Val Loss: same-mlo=0.558, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.604, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.588, AUC: opp-mlo=0.546\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=20, avg train loss=0.544, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.552, AUC: same-mlo=0.663 \n",
      "\tAvg Val Loss opp-cc=0.603, AUC: opp-cc=0.467 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.556\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=25, avg train loss=0.762, \n",
      "\tAvg Val Loss: same-cc=0.622, AUC: same-cc=0.487 \n",
      "\tAvg Val Loss: same-mlo=0.538, AUC: same-mlo=0.671 \n",
      "\tAvg Val Loss opp-cc=0.615, AUC: opp-cc=0.450 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.517\n",
      "Best same-mlo model saved.\n",
      "Iter=30, avg train loss=0.668, \n",
      "\tAvg Val Loss: same-cc=0.641, AUC: same-cc=0.487 \n",
      "\tAvg Val Loss: same-mlo=0.540, AUC: same-mlo=0.679 \n",
      "\tAvg Val Loss opp-cc=0.615, AUC: opp-cc=0.440 \n",
      "\tAvg Val Loss: opp-mlo=0.583, AUC: opp-mlo=0.535\n",
      "Best same-mlo model saved.\n",
      "Iter=35, avg train loss=0.867, \n",
      "\tAvg Val Loss: same-cc=0.657, AUC: same-cc=0.452 \n",
      "\tAvg Val Loss: same-mlo=0.538, AUC: same-mlo=0.694 \n",
      "\tAvg Val Loss opp-cc=0.619, AUC: opp-cc=0.430 \n",
      "\tAvg Val Loss: opp-mlo=0.586, AUC: opp-mlo=0.538\n",
      "Best same-mlo model saved.\n",
      "Iter=40, avg train loss=0.720, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.448 \n",
      "\tAvg Val Loss: same-mlo=0.543, AUC: same-mlo=0.694 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.432 \n",
      "\tAvg Val Loss: opp-mlo=0.586, AUC: opp-mlo=0.556\n",
      "Iter=45, avg train loss=0.722, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.446 \n",
      "\tAvg Val Loss: same-mlo=0.536, AUC: same-mlo=0.692 \n",
      "\tAvg Val Loss opp-cc=0.643, AUC: opp-cc=0.436 \n",
      "\tAvg Val Loss: opp-mlo=0.589, AUC: opp-mlo=0.566\n",
      "Best opp-mlo model saved.\n",
      "Iter=50, avg train loss=0.732, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.446 \n",
      "\tAvg Val Loss: same-mlo=0.537, AUC: same-mlo=0.700 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.428 \n",
      "\tAvg Val Loss: opp-mlo=0.586, AUC: opp-mlo=0.582\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=55, avg train loss=0.688, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.442 \n",
      "\tAvg Val Loss: same-mlo=0.525, AUC: same-mlo=0.706 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.389 \n",
      "\tAvg Val Loss: opp-mlo=0.571, AUC: opp-mlo=0.582\n",
      "Best same-mlo model saved.\n",
      "Iter=60, avg train loss=0.618, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.517, AUC: same-mlo=0.716 \n",
      "\tAvg Val Loss opp-cc=0.616, AUC: opp-cc=0.398 \n",
      "\tAvg Val Loss: opp-mlo=0.565, AUC: opp-mlo=0.621\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=65, avg train loss=0.809, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.520, AUC: same-mlo=0.706 \n",
      "\tAvg Val Loss opp-cc=0.618, AUC: opp-cc=0.398 \n",
      "\tAvg Val Loss: opp-mlo=0.560, AUC: opp-mlo=0.615\n",
      "Iter=70, avg train loss=0.666, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.471 \n",
      "\tAvg Val Loss: same-mlo=0.529, AUC: same-mlo=0.718 \n",
      "\tAvg Val Loss opp-cc=0.632, AUC: opp-cc=0.408 \n",
      "\tAvg Val Loss: opp-mlo=0.563, AUC: opp-mlo=0.615\n",
      "Best same-mlo model saved.\n",
      "Iter=75, avg train loss=0.763, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.442 \n",
      "\tAvg Val Loss: same-mlo=0.535, AUC: same-mlo=0.722 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.387 \n",
      "\tAvg Val Loss: opp-mlo=0.570, AUC: opp-mlo=0.625\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=80, avg train loss=0.692, \n",
      "\tAvg Val Loss: same-cc=0.674, AUC: same-cc=0.422 \n",
      "\tAvg Val Loss: same-mlo=0.546, AUC: same-mlo=0.710 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.389 \n",
      "\tAvg Val Loss: opp-mlo=0.572, AUC: opp-mlo=0.629\n",
      "Best opp-mlo model saved.\n",
      "Iter=85, avg train loss=0.771, \n",
      "\tAvg Val Loss: same-cc=0.672, AUC: same-cc=0.440 \n",
      "\tAvg Val Loss: same-mlo=0.547, AUC: same-mlo=0.700 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.367 \n",
      "\tAvg Val Loss: opp-mlo=0.578, AUC: opp-mlo=0.621\n",
      "Iter=90, avg train loss=0.623, \n",
      "\tAvg Val Loss: same-cc=0.674, AUC: same-cc=0.448 \n",
      "\tAvg Val Loss: same-mlo=0.553, AUC: same-mlo=0.702 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.367 \n",
      "\tAvg Val Loss: opp-mlo=0.582, AUC: opp-mlo=0.615\n",
      "Iter=95, avg train loss=0.644, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.428 \n",
      "\tAvg Val Loss: same-mlo=0.552, AUC: same-mlo=0.698 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.337 \n",
      "\tAvg Val Loss: opp-mlo=0.581, AUC: opp-mlo=0.609\n",
      "Iter=100, avg train loss=0.664, \n",
      "\tAvg Val Loss: same-cc=0.671, AUC: same-cc=0.434 \n",
      "\tAvg Val Loss: same-mlo=0.548, AUC: same-mlo=0.688 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.345 \n",
      "\tAvg Val Loss: opp-mlo=0.566, AUC: opp-mlo=0.627\n",
      "Iter=105, avg train loss=0.614, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.458 \n",
      "\tAvg Val Loss: same-mlo=0.537, AUC: same-mlo=0.698 \n",
      "\tAvg Val Loss opp-cc=0.621, AUC: opp-cc=0.383 \n",
      "\tAvg Val Loss: opp-mlo=0.555, AUC: opp-mlo=0.625\n",
      "Iter=110, avg train loss=0.645, \n",
      "\tAvg Val Loss: same-cc=0.646, AUC: same-cc=0.465 \n",
      "\tAvg Val Loss: same-mlo=0.543, AUC: same-mlo=0.684 \n",
      "\tAvg Val Loss opp-cc=0.613, AUC: opp-cc=0.387 \n",
      "\tAvg Val Loss: opp-mlo=0.555, AUC: opp-mlo=0.635\n",
      "Best opp-mlo model saved.\n",
      "Iter=115, avg train loss=0.641, \n",
      "\tAvg Val Loss: same-cc=0.637, AUC: same-cc=0.473 \n",
      "\tAvg Val Loss: same-mlo=0.538, AUC: same-mlo=0.679 \n",
      "\tAvg Val Loss opp-cc=0.611, AUC: opp-cc=0.398 \n",
      "\tAvg Val Loss: opp-mlo=0.554, AUC: opp-mlo=0.607\n",
      "Iter=120, avg train loss=0.671, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.485 \n",
      "\tAvg Val Loss: same-mlo=0.532, AUC: same-mlo=0.667 \n",
      "\tAvg Val Loss opp-cc=0.590, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.555, AUC: opp-mlo=0.602\n",
      "Iter=125, avg train loss=0.744, \n",
      "\tAvg Val Loss: same-cc=0.649, AUC: same-cc=0.467 \n",
      "\tAvg Val Loss: same-mlo=0.553, AUC: same-mlo=0.673 \n",
      "\tAvg Val Loss opp-cc=0.619, AUC: opp-cc=0.412 \n",
      "\tAvg Val Loss: opp-mlo=0.575, AUC: opp-mlo=0.602\n",
      "Iter=130, avg train loss=0.749, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.475 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.677 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.377 \n",
      "\tAvg Val Loss: opp-mlo=0.602, AUC: opp-mlo=0.606\n",
      "Iter=135, avg train loss=0.723, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.452 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.675 \n",
      "\tAvg Val Loss opp-cc=0.728, AUC: opp-cc=0.341 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.600\n",
      "Iter=140, avg train loss=0.680, \n",
      "\tAvg Val Loss: same-cc=0.739, AUC: same-cc=0.458 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.667 \n",
      "\tAvg Val Loss opp-cc=0.745, AUC: opp-cc=0.343 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.613\n",
      "Iter=145, avg train loss=0.695, \n",
      "\tAvg Val Loss: same-cc=0.740, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.659 \n",
      "\tAvg Val Loss opp-cc=0.734, AUC: opp-cc=0.353 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.604\n",
      "Iter=150, avg train loss=0.660, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.465 \n",
      "\tAvg Val Loss: same-mlo=0.582, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.705, AUC: opp-cc=0.351 \n",
      "\tAvg Val Loss: opp-mlo=0.606, AUC: opp-mlo=0.588\n",
      "Iter=155, avg train loss=0.664, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.473 \n",
      "\tAvg Val Loss: same-mlo=0.580, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.367 \n",
      "\tAvg Val Loss: opp-mlo=0.606, AUC: opp-mlo=0.588\n",
      "Iter=160, avg train loss=0.682, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.589, AUC: same-mlo=0.641 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.387 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.604\n",
      "Iter=165, avg train loss=0.638, \n",
      "\tAvg Val Loss: same-cc=0.722, AUC: same-cc=0.452 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.661 \n",
      "\tAvg Val Loss opp-cc=0.722, AUC: opp-cc=0.385 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.604\n",
      "Iter=170, avg train loss=0.636, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.450 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.406 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.598\n",
      "Iter=175, avg train loss=0.637, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.414 \n",
      "\tAvg Val Loss: opp-mlo=0.583, AUC: opp-mlo=0.598\n",
      "Iter=180, avg train loss=0.642, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.655, AUC: opp-cc=0.430 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=185, avg train loss=0.742, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.620, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.410 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.611\n",
      "Iter=190, avg train loss=0.652, \n",
      "\tAvg Val Loss: same-cc=0.680, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.387 \n",
      "\tAvg Val Loss: opp-mlo=0.626, AUC: opp-mlo=0.602\n",
      "Iter=195, avg train loss=0.593, \n",
      "\tAvg Val Loss: same-cc=0.669, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.393 \n",
      "\tAvg Val Loss: opp-mlo=0.627, AUC: opp-mlo=0.609\n",
      "Iter=200, avg train loss=0.691, \n",
      "\tAvg Val Loss: same-cc=0.646, AUC: same-cc=0.467 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.646, AUC: opp-cc=0.422 \n",
      "\tAvg Val Loss: opp-mlo=0.611, AUC: opp-mlo=0.623\n",
      "Iter=205, avg train loss=0.592, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.454 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.607 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.428 \n",
      "\tAvg Val Loss: opp-mlo=0.603, AUC: opp-mlo=0.627\n",
      "Iter=210, avg train loss=0.674, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.430 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.410 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.617\n",
      "Iter=215, avg train loss=0.626, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.412 \n",
      "\tAvg Val Loss: same-mlo=0.602, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.416 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.615\n",
      "Iter=220, avg train loss=0.614, \n",
      "\tAvg Val Loss: same-cc=0.726, AUC: same-cc=0.383 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.418 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.604\n",
      "Iter=225, avg train loss=0.615, \n",
      "\tAvg Val Loss: same-cc=0.730, AUC: same-cc=0.377 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.584 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.396 \n",
      "\tAvg Val Loss: opp-mlo=0.597, AUC: opp-mlo=0.617\n",
      "Iter=230, avg train loss=0.588, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.406 \n",
      "\tAvg Val Loss: same-mlo=0.591, AUC: same-mlo=0.590 \n",
      "\tAvg Val Loss opp-cc=0.654, AUC: opp-cc=0.424 \n",
      "\tAvg Val Loss: opp-mlo=0.580, AUC: opp-mlo=0.607\n",
      "Iter=235, avg train loss=0.699, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.438 \n",
      "\tAvg Val Loss: same-mlo=0.576, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.424 \n",
      "\tAvg Val Loss: opp-mlo=0.589, AUC: opp-mlo=0.588\n",
      "Iter=240, avg train loss=0.654, \n",
      "\tAvg Val Loss: same-cc=0.672, AUC: same-cc=0.438 \n",
      "\tAvg Val Loss: same-mlo=0.573, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.651, AUC: opp-cc=0.456 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.609\n",
      "Iter=245, avg train loss=0.651, \n",
      "\tAvg Val Loss: same-cc=0.671, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.633, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.673, AUC: opp-mlo=0.596\n",
      "Best opp-cc model saved.\n",
      "Iter=250, avg train loss=0.627, \n",
      "\tAvg Val Loss: same-cc=0.697, AUC: same-cc=0.448 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.639, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.700, AUC: opp-mlo=0.584\n",
      "Iter=255, avg train loss=0.628, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.444 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.454 \n",
      "\tAvg Val Loss: opp-mlo=0.691, AUC: opp-mlo=0.562\n",
      "Iter=260, avg train loss=0.652, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.442 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.446 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.566\n",
      "Iter=265, avg train loss=0.647, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.442 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.438 \n",
      "\tAvg Val Loss: opp-mlo=0.656, AUC: opp-mlo=0.574\n",
      "Iter=270, avg train loss=0.639, \n",
      "\tAvg Val Loss: same-cc=0.742, AUC: same-cc=0.432 \n",
      "\tAvg Val Loss: same-mlo=0.641, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.720, AUC: opp-cc=0.424 \n",
      "\tAvg Val Loss: opp-mlo=0.710, AUC: opp-mlo=0.572\n",
      "Iter=275, avg train loss=0.615, \n",
      "\tAvg Val Loss: same-cc=0.703, AUC: same-cc=0.448 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.584 \n",
      "\tAvg Val Loss opp-cc=0.768, AUC: opp-cc=0.394 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.588\n",
      "Iter=280, avg train loss=0.628, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.454 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.566 \n",
      "\tAvg Val Loss opp-cc=0.745, AUC: opp-cc=0.387 \n",
      "\tAvg Val Loss: opp-mlo=0.656, AUC: opp-mlo=0.572\n",
      "Iter=285, avg train loss=0.671, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.450 \n",
      "\tAvg Val Loss: same-mlo=0.606, AUC: same-mlo=0.556 \n",
      "\tAvg Val Loss opp-cc=0.720, AUC: opp-cc=0.398 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.570\n",
      "Iter=290, avg train loss=0.573, \n",
      "\tAvg Val Loss: same-cc=0.671, AUC: same-cc=0.454 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.428 \n",
      "\tAvg Val Loss: opp-mlo=0.621, AUC: opp-mlo=0.576\n",
      "Iter=295, avg train loss=0.621, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.724, AUC: opp-cc=0.402 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.564\n",
      "Iter=300, avg train loss=0.544, \n",
      "\tAvg Val Loss: same-cc=0.712, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.605, AUC: same-mlo=0.558 \n",
      "\tAvg Val Loss opp-cc=0.752, AUC: opp-cc=0.361 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.560\n",
      "Iter=305, avg train loss=0.612, \n",
      "\tAvg Val Loss: same-cc=0.682, AUC: same-cc=0.467 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.529 \n",
      "\tAvg Val Loss opp-cc=0.746, AUC: opp-cc=0.365 \n",
      "\tAvg Val Loss: opp-mlo=0.624, AUC: opp-mlo=0.529\n",
      "Iter=310, avg train loss=0.589, \n",
      "\tAvg Val Loss: same-cc=0.667, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.624, AUC: same-mlo=0.497 \n",
      "\tAvg Val Loss opp-cc=0.710, AUC: opp-cc=0.371 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.523\n",
      "Iter=315, avg train loss=0.644, \n",
      "\tAvg Val Loss: same-cc=0.724, AUC: same-cc=0.446 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.513 \n",
      "\tAvg Val Loss opp-cc=0.741, AUC: opp-cc=0.361 \n",
      "\tAvg Val Loss: opp-mlo=0.666, AUC: opp-mlo=0.495\n",
      "Iter=320, avg train loss=0.623, \n",
      "\tAvg Val Loss: same-cc=0.731, AUC: same-cc=0.450 \n",
      "\tAvg Val Loss: same-mlo=0.666, AUC: same-mlo=0.540 \n",
      "\tAvg Val Loss opp-cc=0.724, AUC: opp-cc=0.410 \n",
      "\tAvg Val Loss: opp-mlo=0.698, AUC: opp-mlo=0.501\n",
      "Iter=325, avg train loss=0.524, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.636, AUC: same-mlo=0.584 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.652, AUC: opp-mlo=0.548\n",
      "Best opp-cc model saved.\n",
      "Iter=330, avg train loss=0.617, \n",
      "\tAvg Val Loss: same-cc=0.682, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.606, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.471 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.570\n",
      "Iter=335, avg train loss=0.565, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.452 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.637, AUC: opp-mlo=0.572\n",
      "Iter=340, avg train loss=0.546, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.454 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.689, AUC: opp-cc=0.387 \n",
      "\tAvg Val Loss: opp-mlo=0.603, AUC: opp-mlo=0.602\n",
      "Iter=345, avg train loss=0.609, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.432 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.727, AUC: opp-cc=0.398 \n",
      "\tAvg Val Loss: opp-mlo=0.611, AUC: opp-mlo=0.586\n",
      "Iter=350, avg train loss=0.549, \n",
      "\tAvg Val Loss: same-cc=0.746, AUC: same-cc=0.450 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.533 \n",
      "\tAvg Val Loss opp-cc=0.752, AUC: opp-cc=0.383 \n",
      "\tAvg Val Loss: opp-mlo=0.637, AUC: opp-mlo=0.578\n",
      "Iter=355, avg train loss=0.609, \n",
      "\tAvg Val Loss: same-cc=0.747, AUC: same-cc=0.458 \n",
      "\tAvg Val Loss: same-mlo=0.678, AUC: same-mlo=0.519 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.414 \n",
      "\tAvg Val Loss: opp-mlo=0.653, AUC: opp-mlo=0.574\n",
      "Iter=360, avg train loss=0.651, \n",
      "\tAvg Val Loss: same-cc=0.812, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.714, AUC: same-mlo=0.538 \n",
      "\tAvg Val Loss opp-cc=0.778, AUC: opp-cc=0.410 \n",
      "\tAvg Val Loss: opp-mlo=0.686, AUC: opp-mlo=0.598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=365, avg train loss=0.581, \n",
      "\tAvg Val Loss: same-cc=0.830, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.738, AUC: same-mlo=0.527 \n",
      "\tAvg Val Loss opp-cc=0.814, AUC: opp-cc=0.402 \n",
      "\tAvg Val Loss: opp-mlo=0.685, AUC: opp-mlo=0.606\n",
      "Iter=370, avg train loss=0.611, \n",
      "\tAvg Val Loss: same-cc=0.823, AUC: same-cc=0.448 \n",
      "\tAvg Val Loss: same-mlo=0.739, AUC: same-mlo=0.533 \n",
      "\tAvg Val Loss opp-cc=0.788, AUC: opp-cc=0.393 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.607\n",
      "Iter=375, avg train loss=0.623, \n",
      "\tAvg Val Loss: same-cc=0.771, AUC: same-cc=0.465 \n",
      "\tAvg Val Loss: same-mlo=0.716, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.749, AUC: opp-cc=0.422 \n",
      "\tAvg Val Loss: opp-mlo=0.663, AUC: opp-mlo=0.598\n",
      "Iter=380, avg train loss=0.603, \n",
      "\tAvg Val Loss: same-cc=0.757, AUC: same-cc=0.487 \n",
      "\tAvg Val Loss: same-mlo=0.649, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.733, AUC: opp-cc=0.438 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.611\n",
      "Iter=385, avg train loss=0.523, \n",
      "\tAvg Val Loss: same-cc=0.758, AUC: same-cc=0.477 \n",
      "\tAvg Val Loss: same-mlo=0.662, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.727, AUC: opp-cc=0.420 \n",
      "\tAvg Val Loss: opp-mlo=0.650, AUC: opp-mlo=0.598\n",
      "Iter=390, avg train loss=0.521, \n",
      "\tAvg Val Loss: same-cc=0.680, AUC: same-cc=0.495 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.542 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.440 \n",
      "\tAvg Val Loss: opp-mlo=0.643, AUC: opp-mlo=0.576\n",
      "Iter=395, avg train loss=0.632, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.505 \n",
      "\tAvg Val Loss: same-mlo=0.679, AUC: same-mlo=0.497 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.436 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.570\n",
      "Iter=400, avg train loss=0.538, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.499 \n",
      "\tAvg Val Loss: same-mlo=0.687, AUC: same-mlo=0.475 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.434 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.572\n",
      "Iter=405, avg train loss=0.604, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.473 \n",
      "\tAvg Val Loss: same-mlo=0.702, AUC: same-mlo=0.465 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.412 \n",
      "\tAvg Val Loss: opp-mlo=0.694, AUC: opp-mlo=0.560\n",
      "Iter=410, avg train loss=0.539, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.452 \n",
      "\tAvg Val Loss: same-mlo=0.688, AUC: same-mlo=0.479 \n",
      "\tAvg Val Loss opp-cc=0.706, AUC: opp-cc=0.393 \n",
      "\tAvg Val Loss: opp-mlo=0.710, AUC: opp-mlo=0.568\n",
      "Iter=415, avg train loss=0.579, \n",
      "\tAvg Val Loss: same-cc=0.731, AUC: same-cc=0.465 \n",
      "\tAvg Val Loss: same-mlo=0.683, AUC: same-mlo=0.507 \n",
      "\tAvg Val Loss opp-cc=0.750, AUC: opp-cc=0.416 \n",
      "\tAvg Val Loss: opp-mlo=0.745, AUC: opp-mlo=0.566\n",
      "Iter=420, avg train loss=0.607, \n",
      "\tAvg Val Loss: same-cc=0.754, AUC: same-cc=0.467 \n",
      "\tAvg Val Loss: same-mlo=0.655, AUC: same-mlo=0.562 \n",
      "\tAvg Val Loss opp-cc=0.767, AUC: opp-cc=0.410 \n",
      "\tAvg Val Loss: opp-mlo=0.722, AUC: opp-mlo=0.592\n",
      "Iter=425, avg train loss=0.493, \n",
      "\tAvg Val Loss: same-cc=0.756, AUC: same-cc=0.465 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.562 \n",
      "\tAvg Val Loss opp-cc=0.804, AUC: opp-cc=0.389 \n",
      "\tAvg Val Loss: opp-mlo=0.681, AUC: opp-mlo=0.606\n",
      "Iter=430, avg train loss=0.574, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.481 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.590 \n",
      "\tAvg Val Loss opp-cc=0.837, AUC: opp-cc=0.365 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.606\n",
      "Iter=435, avg train loss=0.528, \n",
      "\tAvg Val Loss: same-cc=0.642, AUC: same-cc=0.473 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.778, AUC: opp-cc=0.393 \n",
      "\tAvg Val Loss: opp-mlo=0.606, AUC: opp-mlo=0.592\n",
      "Iter=440, avg train loss=0.577, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.481 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.734, AUC: opp-cc=0.428 \n",
      "\tAvg Val Loss: opp-mlo=0.655, AUC: opp-mlo=0.606\n",
      "Iter=445, avg train loss=0.527, \n",
      "\tAvg Val Loss: same-cc=0.621, AUC: same-cc=0.489 \n",
      "\tAvg Val Loss: same-mlo=0.614, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.400 \n",
      "\tAvg Val Loss: opp-mlo=0.681, AUC: opp-mlo=0.604\n",
      "Iter=450, avg train loss=0.511, \n",
      "\tAvg Val Loss: same-cc=0.636, AUC: same-cc=0.481 \n",
      "\tAvg Val Loss: same-mlo=0.602, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.440 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.615\n",
      "Iter=455, avg train loss=0.592, \n",
      "\tAvg Val Loss: same-cc=0.729, AUC: same-cc=0.469 \n",
      "\tAvg Val Loss: same-mlo=0.640, AUC: same-mlo=0.588 \n",
      "\tAvg Val Loss opp-cc=0.738, AUC: opp-cc=0.410 \n",
      "\tAvg Val Loss: opp-mlo=0.793, AUC: opp-mlo=0.619\n",
      "Iter=460, avg train loss=0.535, \n",
      "\tAvg Val Loss: same-cc=0.828, AUC: same-cc=0.438 \n",
      "\tAvg Val Loss: same-mlo=0.698, AUC: same-mlo=0.588 \n",
      "\tAvg Val Loss opp-cc=0.773, AUC: opp-cc=0.442 \n",
      "\tAvg Val Loss: opp-mlo=0.784, AUC: opp-mlo=0.619\n",
      "Iter=465, avg train loss=0.526, \n",
      "\tAvg Val Loss: same-cc=0.810, AUC: same-cc=0.442 \n",
      "\tAvg Val Loss: same-mlo=0.656, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.776, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.720, AUC: opp-mlo=0.621\n",
      "Iter=470, avg train loss=0.547, \n",
      "\tAvg Val Loss: same-cc=0.765, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.647, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.777, AUC: opp-cc=0.432 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.633\n",
      "Iter=475, avg train loss=0.457, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.465 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.454 \n",
      "\tAvg Val Loss: opp-mlo=0.589, AUC: opp-mlo=0.611\n",
      "Iter=480, avg train loss=0.563, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.469 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.556 \n",
      "\tAvg Val Loss opp-cc=0.686, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.590\n",
      "Iter=485, avg train loss=0.496, \n",
      "\tAvg Val Loss: same-cc=0.669, AUC: same-cc=0.493 \n",
      "\tAvg Val Loss: same-mlo=0.693, AUC: same-mlo=0.519 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.452 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.582\n",
      "Iter=490, avg train loss=0.522, \n",
      "\tAvg Val Loss: same-cc=0.724, AUC: same-cc=0.477 \n",
      "\tAvg Val Loss: same-mlo=0.670, AUC: same-mlo=0.546 \n",
      "\tAvg Val Loss opp-cc=0.743, AUC: opp-cc=0.404 \n",
      "\tAvg Val Loss: opp-mlo=0.681, AUC: opp-mlo=0.606\n",
      "Iter=495, avg train loss=0.503, \n",
      "\tAvg Val Loss: same-cc=0.800, AUC: same-cc=0.452 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.812, AUC: opp-cc=0.404 \n",
      "\tAvg Val Loss: opp-mlo=0.700, AUC: opp-mlo=0.613\n",
      "Iter=500, avg train loss=0.522, \n",
      "\tAvg Val Loss: same-cc=0.832, AUC: same-cc=0.450 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.570 \n",
      "\tAvg Val Loss opp-cc=0.859, AUC: opp-cc=0.424 \n",
      "\tAvg Val Loss: opp-mlo=0.694, AUC: opp-mlo=0.600\n",
      "Iter=505, avg train loss=0.456, \n",
      "\tAvg Val Loss: same-cc=0.840, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.726, AUC: same-mlo=0.574 \n",
      "\tAvg Val Loss opp-cc=0.943, AUC: opp-cc=0.406 \n",
      "\tAvg Val Loss: opp-mlo=0.715, AUC: opp-mlo=0.582\n",
      "Iter=510, avg train loss=0.456, \n",
      "\tAvg Val Loss: same-cc=0.853, AUC: same-cc=0.479 \n",
      "\tAvg Val Loss: same-mlo=0.715, AUC: same-mlo=0.655 \n",
      "\tAvg Val Loss opp-cc=0.922, AUC: opp-cc=0.414 \n",
      "\tAvg Val Loss: opp-mlo=0.642, AUC: opp-mlo=0.611\n",
      "Iter=515, avg train loss=0.523, \n",
      "\tAvg Val Loss: same-cc=0.755, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.684 \n",
      "\tAvg Val Loss opp-cc=0.778, AUC: opp-cc=0.467 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.633\n",
      "Iter=520, avg train loss=0.486, \n",
      "\tAvg Val Loss: same-cc=0.640, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.550, AUC: same-mlo=0.692 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.556, AUC: opp-mlo=0.645\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=525, avg train loss=0.480, \n",
      "\tAvg Val Loss: same-cc=0.599, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.576, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.632, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.555, AUC: opp-mlo=0.667\n",
      "Best same-cc model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=530, avg train loss=0.608, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.550 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.630, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.770, AUC: opp-mlo=0.637\n",
      "Iter=535, avg train loss=0.541, \n",
      "\tAvg Val Loss: same-cc=0.755, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.641, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.667, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.935, AUC: opp-mlo=0.606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=540, avg train loss=0.459, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.646, AUC: same-mlo=0.582 \n",
      "\tAvg Val Loss opp-cc=0.726, AUC: opp-cc=0.501 \n",
      "\tAvg Val Loss: opp-mlo=0.726, AUC: opp-mlo=0.566\n",
      "Iter=545, avg train loss=0.460, \n",
      "\tAvg Val Loss: same-cc=0.700, AUC: same-cc=0.501 \n",
      "\tAvg Val Loss: same-mlo=0.653, AUC: same-mlo=0.596 \n",
      "\tAvg Val Loss opp-cc=0.857, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.570\n",
      "Iter=550, avg train loss=0.574, \n",
      "\tAvg Val Loss: same-cc=0.683, AUC: same-cc=0.501 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.843, AUC: opp-cc=0.442 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.554\n",
      "Iter=555, avg train loss=0.533, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.582 \n",
      "\tAvg Val Loss opp-cc=0.776, AUC: opp-cc=0.497 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.542\n",
      "Iter=560, avg train loss=0.504, \n",
      "\tAvg Val Loss: same-cc=0.761, AUC: same-cc=0.511 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.785, AUC: opp-cc=0.501 \n",
      "\tAvg Val Loss: opp-mlo=0.667, AUC: opp-mlo=0.564\n",
      "Iter=565, avg train loss=0.428, \n",
      "\tAvg Val Loss: same-cc=0.770, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.655 \n",
      "\tAvg Val Loss opp-cc=0.749, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.722, AUC: opp-mlo=0.607\n",
      "Iter=570, avg train loss=0.525, \n",
      "\tAvg Val Loss: same-cc=0.727, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.655 \n",
      "\tAvg Val Loss opp-cc=0.735, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.707, AUC: opp-mlo=0.629\n",
      "Iter=575, avg train loss=0.486, \n",
      "\tAvg Val Loss: same-cc=0.601, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.697, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.596\n",
      "Iter=580, avg train loss=0.439, \n",
      "\tAvg Val Loss: same-cc=0.633, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.499 \n",
      "\tAvg Val Loss opp-cc=0.751, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.637, AUC: opp-mlo=0.544\n",
      "Iter=585, avg train loss=0.385, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.720, AUC: same-mlo=0.491 \n",
      "\tAvg Val Loss opp-cc=0.768, AUC: opp-cc=0.442 \n",
      "\tAvg Val Loss: opp-mlo=0.662, AUC: opp-mlo=0.529\n",
      "Iter=590, avg train loss=0.471, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.517 \n",
      "\tAvg Val Loss: same-mlo=0.732, AUC: same-mlo=0.477 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.521\n",
      "Iter=595, avg train loss=0.419, \n",
      "\tAvg Val Loss: same-cc=0.754, AUC: same-cc=0.507 \n",
      "\tAvg Val Loss: same-mlo=0.703, AUC: same-mlo=0.517 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.730, AUC: opp-mlo=0.515\n",
      "Iter=600, avg train loss=0.477, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.511 \n",
      "\tAvg Val Loss: same-mlo=0.660, AUC: same-mlo=0.540 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.511 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.519\n",
      "Iter=605, avg train loss=0.378, \n",
      "\tAvg Val Loss: same-cc=0.622, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.697, AUC: same-mlo=0.493 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.507\n",
      "Iter=610, avg train loss=0.503, \n",
      "\tAvg Val Loss: same-cc=0.728, AUC: same-cc=0.511 \n",
      "\tAvg Val Loss: same-mlo=0.729, AUC: same-mlo=0.483 \n",
      "\tAvg Val Loss opp-cc=0.785, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.717, AUC: opp-mlo=0.517\n",
      "Iter=615, avg train loss=0.344, \n",
      "\tAvg Val Loss: same-cc=0.859, AUC: same-cc=0.491 \n",
      "\tAvg Val Loss: same-mlo=0.724, AUC: same-mlo=0.584 \n",
      "\tAvg Val Loss opp-cc=0.940, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.782, AUC: opp-mlo=0.546\n",
      "Iter=620, avg train loss=0.345, \n",
      "\tAvg Val Loss: same-cc=0.699, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.916, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.688, AUC: opp-mlo=0.566\n",
      "Iter=625, avg train loss=0.450, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.554 \n",
      "\tAvg Val Loss: same-mlo=0.627, AUC: same-mlo=0.562 \n",
      "\tAvg Val Loss opp-cc=0.721, AUC: opp-cc=0.533 \n",
      "\tAvg Val Loss: opp-mlo=0.643, AUC: opp-mlo=0.582\n",
      "Iter=630, avg train loss=0.386, \n",
      "\tAvg Val Loss: same-cc=0.711, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.620, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.710, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.594\n",
      "Iter=635, avg train loss=0.439, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.525 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.588\n",
      "Iter=640, avg train loss=0.471, \n",
      "\tAvg Val Loss: same-cc=0.782, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.657, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.775, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.584\n",
      "Iter=645, avg train loss=0.395, \n",
      "\tAvg Val Loss: same-cc=0.656, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.614, AUC: same-mlo=0.576 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.544 \n",
      "\tAvg Val Loss: opp-mlo=0.664, AUC: opp-mlo=0.558\n",
      "Iter=650, avg train loss=0.432, \n",
      "\tAvg Val Loss: same-cc=0.706, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.546 \n",
      "\tAvg Val Loss: opp-mlo=0.671, AUC: opp-mlo=0.564\n",
      "Iter=655, avg train loss=0.438, \n",
      "\tAvg Val Loss: same-cc=0.748, AUC: same-cc=0.511 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.661 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.562 \n",
      "\tAvg Val Loss: opp-mlo=0.757, AUC: opp-mlo=0.570\n",
      "Iter=660, avg train loss=0.360, \n",
      "\tAvg Val Loss: same-cc=0.730, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.834, AUC: same-mlo=0.661 \n",
      "\tAvg Val Loss opp-cc=0.761, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.794, AUC: opp-mlo=0.550\n",
      "Best opp-cc model saved.\n",
      "Iter=665, avg train loss=0.311, \n",
      "\tAvg Val Loss: same-cc=0.736, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.785, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.833, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.746, AUC: opp-mlo=0.548\n",
      "Iter=670, avg train loss=0.456, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.685, AUC: same-mlo=0.544 \n",
      "\tAvg Val Loss opp-cc=0.779, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.697, AUC: opp-mlo=0.523\n",
      "Iter=675, avg train loss=0.425, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.721, AUC: same-mlo=0.483 \n",
      "\tAvg Val Loss opp-cc=0.700, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.715, AUC: opp-mlo=0.493\n",
      "Iter=680, avg train loss=0.418, \n",
      "\tAvg Val Loss: same-cc=0.621, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.752, AUC: same-mlo=0.458 \n",
      "\tAvg Val Loss opp-cc=0.700, AUC: opp-cc=0.511 \n",
      "\tAvg Val Loss: opp-mlo=0.701, AUC: opp-mlo=0.505\n",
      "Iter=685, avg train loss=0.466, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.505 \n",
      "\tAvg Val Loss: same-mlo=0.794, AUC: same-mlo=0.442 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.788, AUC: opp-mlo=0.479\n",
      "Iter=690, avg train loss=0.519, \n",
      "\tAvg Val Loss: same-cc=0.789, AUC: same-cc=0.477 \n",
      "\tAvg Val Loss: same-mlo=0.773, AUC: same-mlo=0.487 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.737, AUC: opp-mlo=0.505\n",
      "Iter=695, avg train loss=0.400, \n",
      "\tAvg Val Loss: same-cc=0.801, AUC: same-cc=0.475 \n",
      "\tAvg Val Loss: same-mlo=0.771, AUC: same-mlo=0.489 \n",
      "\tAvg Val Loss opp-cc=0.736, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.734, AUC: opp-mlo=0.515\n",
      "Iter=700, avg train loss=0.359, \n",
      "\tAvg Val Loss: same-cc=0.725, AUC: same-cc=0.493 \n",
      "\tAvg Val Loss: same-mlo=0.696, AUC: same-mlo=0.527 \n",
      "\tAvg Val Loss opp-cc=0.808, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.711, AUC: opp-mlo=0.525\n",
      "Iter=705, avg train loss=0.415, \n",
      "\tAvg Val Loss: same-cc=0.703, AUC: same-cc=0.503 \n",
      "\tAvg Val Loss: same-mlo=0.698, AUC: same-mlo=0.525 \n",
      "\tAvg Val Loss opp-cc=0.813, AUC: opp-cc=0.511 \n",
      "\tAvg Val Loss: opp-mlo=0.753, AUC: opp-mlo=0.558\n",
      "Iter=710, avg train loss=0.315, \n",
      "\tAvg Val Loss: same-cc=0.731, AUC: same-cc=0.495 \n",
      "\tAvg Val Loss: same-mlo=0.733, AUC: same-mlo=0.493 \n",
      "\tAvg Val Loss opp-cc=0.768, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.574\n",
      "Iter=715, avg train loss=0.341, \n",
      "\tAvg Val Loss: same-cc=0.734, AUC: same-cc=0.489 \n",
      "\tAvg Val Loss: same-mlo=0.743, AUC: same-mlo=0.505 \n",
      "\tAvg Val Loss opp-cc=0.810, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.631, AUC: opp-mlo=0.578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=720, avg train loss=0.366, \n",
      "\tAvg Val Loss: same-cc=0.758, AUC: same-cc=0.495 \n",
      "\tAvg Val Loss: same-mlo=0.748, AUC: same-mlo=0.487 \n",
      "\tAvg Val Loss opp-cc=0.739, AUC: opp-cc=0.562 \n",
      "\tAvg Val Loss: opp-mlo=0.697, AUC: opp-mlo=0.592\n",
      "Iter=725, avg train loss=0.332, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.718, AUC: same-mlo=0.487 \n",
      "\tAvg Val Loss opp-cc=0.827, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.775, AUC: opp-mlo=0.592\n",
      "Iter=730, avg train loss=0.342, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.523 \n",
      "\tAvg Val Loss: same-mlo=0.715, AUC: same-mlo=0.493 \n",
      "\tAvg Val Loss opp-cc=0.775, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.768, AUC: opp-mlo=0.590\n",
      "Iter=735, avg train loss=0.329, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.718, AUC: same-mlo=0.515 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.554 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.562\n",
      "Iter=740, avg train loss=0.357, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.747, AUC: same-mlo=0.517 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.574 \n",
      "\tAvg Val Loss: opp-mlo=0.707, AUC: opp-mlo=0.550\n",
      "Best opp-cc model saved.\n",
      "Iter=745, avg train loss=0.453, \n",
      "\tAvg Val Loss: same-cc=0.938, AUC: same-cc=0.477 \n",
      "\tAvg Val Loss: same-mlo=0.830, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.713, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.753, AUC: opp-mlo=0.564\n",
      "Iter=750, avg train loss=0.319, \n",
      "\tAvg Val Loss: same-cc=0.826, AUC: same-cc=0.479 \n",
      "\tAvg Val Loss: same-mlo=0.882, AUC: same-mlo=0.558 \n",
      "\tAvg Val Loss opp-cc=0.768, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.755, AUC: opp-mlo=0.550\n",
      "Iter=755, avg train loss=0.262, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.818, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.707, AUC: opp-cc=0.582 \n",
      "\tAvg Val Loss: opp-mlo=0.700, AUC: opp-mlo=0.552\n",
      "Best opp-cc model saved.\n",
      "Iter=760, avg train loss=0.382, \n",
      "\tAvg Val Loss: same-cc=0.646, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.691, AUC: same-mlo=0.564 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.582 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.558\n",
      "Best models loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training: same-cc=0.551, same-mlo=0.623, opp-cc=0.439, opp-mlo=0.713\n",
      "Max-Score Based AUC After Training: 0.605, Mean-Score Based AUC After Training: 0.593\n",
      "\n",
      "\n",
      "\n",
      "========== Fold 4 ==========\n",
      "Test AUC at start: same-cc=0.443, same-mlo=0.423, opp-cc=0.294, opp-mlo=0.427\n",
      "Max-Score Based AUC Before Training: 0.372, Mean-Score Based AUC Before Training: 0.360\n",
      "Iter=5, avg train loss=1.535, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.454 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.467 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.400 \n",
      "\tAvg Val Loss: opp-mlo=0.645, AUC: opp-mlo=0.373\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=10, avg train loss=0.969, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.475 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.501 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.436 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.404\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=15, avg train loss=0.800, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.491 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.529 \n",
      "\tAvg Val Loss opp-cc=0.632, AUC: opp-cc=0.467 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.430\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=20, avg train loss=0.940, \n",
      "\tAvg Val Loss: same-cc=0.632, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.591, AUC: same-mlo=0.542 \n",
      "\tAvg Val Loss opp-cc=0.630, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.609, AUC: opp-mlo=0.448\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=25, avg train loss=0.682, \n",
      "\tAvg Val Loss: same-cc=0.642, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.589, AUC: same-mlo=0.538 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.458\n",
      "Best same-cc model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=30, avg train loss=0.850, \n",
      "\tAvg Val Loss: same-cc=0.637, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.578, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.634, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.594, AUC: opp-mlo=0.481\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=35, avg train loss=0.675, \n",
      "\tAvg Val Loss: same-cc=0.639, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.550 \n",
      "\tAvg Val Loss opp-cc=0.627, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.477\n",
      "Iter=40, avg train loss=0.718, \n",
      "\tAvg Val Loss: same-cc=0.671, AUC: same-cc=0.513 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.651, AUC: opp-cc=0.450 \n",
      "\tAvg Val Loss: opp-mlo=0.599, AUC: opp-mlo=0.479\n",
      "Iter=45, avg train loss=0.769, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.503 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.554 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.444 \n",
      "\tAvg Val Loss: opp-mlo=0.597, AUC: opp-mlo=0.479\n",
      "Iter=50, avg train loss=0.750, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.507 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.546 \n",
      "\tAvg Val Loss opp-cc=0.656, AUC: opp-cc=0.448 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.483\n",
      "Best opp-mlo model saved.\n",
      "Iter=55, avg train loss=0.753, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.493 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.452 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.495\n",
      "Best opp-mlo model saved.\n",
      "Iter=60, avg train loss=0.725, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.491 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.444 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.511\n",
      "Best opp-mlo model saved.\n",
      "Iter=65, avg train loss=0.623, \n",
      "\tAvg Val Loss: same-cc=0.704, AUC: same-cc=0.499 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.562 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.438 \n",
      "\tAvg Val Loss: opp-mlo=0.619, AUC: opp-mlo=0.531\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=70, avg train loss=0.686, \n",
      "\tAvg Val Loss: same-cc=0.676, AUC: same-cc=0.497 \n",
      "\tAvg Val Loss: same-mlo=0.588, AUC: same-mlo=0.582 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.438 \n",
      "\tAvg Val Loss: opp-mlo=0.606, AUC: opp-mlo=0.519\n",
      "Best same-mlo model saved.\n",
      "Iter=75, avg train loss=0.595, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.489 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.420 \n",
      "\tAvg Val Loss: opp-mlo=0.606, AUC: opp-mlo=0.519\n",
      "Iter=80, avg train loss=0.690, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.487 \n",
      "\tAvg Val Loss: same-mlo=0.573, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.408 \n",
      "\tAvg Val Loss: opp-mlo=0.599, AUC: opp-mlo=0.517\n",
      "Iter=85, avg train loss=0.643, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.487 \n",
      "\tAvg Val Loss: same-mlo=0.572, AUC: same-mlo=0.570 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.426 \n",
      "\tAvg Val Loss: opp-mlo=0.603, AUC: opp-mlo=0.497\n",
      "Iter=90, avg train loss=0.656, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.489 \n",
      "\tAvg Val Loss: same-mlo=0.576, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.665, AUC: opp-cc=0.418 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.473\n",
      "Iter=95, avg train loss=0.687, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.483 \n",
      "\tAvg Val Loss: same-mlo=0.579, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.448 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.493\n",
      "Iter=100, avg train loss=0.640, \n",
      "\tAvg Val Loss: same-cc=0.668, AUC: same-cc=0.473 \n",
      "\tAvg Val Loss: same-mlo=0.585, AUC: same-mlo=0.562 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.452 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.509\n",
      "Iter=105, avg train loss=0.624, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.582 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.462 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.511\n",
      "Iter=110, avg train loss=0.641, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.606, AUC: same-mlo=0.588 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.450 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.507\n",
      "Best same-mlo model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=115, avg train loss=0.628, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.450 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.586 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.462 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.503\n",
      "Iter=120, avg train loss=0.741, \n",
      "\tAvg Val Loss: same-cc=0.684, AUC: same-cc=0.450 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.578 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.499\n",
      "Iter=125, avg train loss=0.588, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.442 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.584 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.521\n",
      "Iter=130, avg train loss=0.701, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.462 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.592 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.519\n",
      "Best same-mlo model saved.\n",
      "Iter=135, avg train loss=0.686, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.507\n",
      "Best same-mlo model saved.\n",
      "Iter=140, avg train loss=0.637, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.503\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Iter=145, avg train loss=0.620, \n",
      "\tAvg Val Loss: same-cc=0.719, AUC: same-cc=0.458 \n",
      "\tAvg Val Loss: same-mlo=0.588, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.513\n",
      "Best opp-cc model saved.\n",
      "Iter=150, avg train loss=0.632, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.458 \n",
      "\tAvg Val Loss: same-mlo=0.572, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.501\n",
      "Iter=155, avg train loss=0.672, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.446 \n",
      "\tAvg Val Loss: same-mlo=0.589, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.467 \n",
      "\tAvg Val Loss: opp-mlo=0.631, AUC: opp-mlo=0.513\n",
      "Iter=160, avg train loss=0.639, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.448 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.705, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.653, AUC: opp-mlo=0.535\n",
      "Best opp-mlo model saved.\n",
      "Iter=165, avg train loss=0.586, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.448 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.695, AUC: opp-cc=0.436 \n",
      "\tAvg Val Loss: opp-mlo=0.647, AUC: opp-mlo=0.525\n",
      "Iter=170, avg train loss=0.606, \n",
      "\tAvg Val Loss: same-cc=0.668, AUC: same-cc=0.481 \n",
      "\tAvg Val Loss: same-mlo=0.572, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.393 \n",
      "\tAvg Val Loss: opp-mlo=0.637, AUC: opp-mlo=0.509\n",
      "Iter=175, avg train loss=0.602, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.469 \n",
      "\tAvg Val Loss: same-mlo=0.576, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.393 \n",
      "\tAvg Val Loss: opp-mlo=0.626, AUC: opp-mlo=0.505\n",
      "Iter=180, avg train loss=0.677, \n",
      "\tAvg Val Loss: same-cc=0.662, AUC: same-cc=0.475 \n",
      "\tAvg Val Loss: same-mlo=0.580, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.689, AUC: opp-cc=0.404 \n",
      "\tAvg Val Loss: opp-mlo=0.626, AUC: opp-mlo=0.513\n",
      "Iter=185, avg train loss=0.675, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.477 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.588 \n",
      "\tAvg Val Loss opp-cc=0.696, AUC: opp-cc=0.444 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.533\n",
      "Iter=190, avg train loss=0.703, \n",
      "\tAvg Val Loss: same-cc=0.697, AUC: same-cc=0.454 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.744, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.665, AUC: opp-mlo=0.546\n",
      "Best opp-mlo model saved.\n",
      "Iter=195, avg train loss=0.624, \n",
      "\tAvg Val Loss: same-cc=0.703, AUC: same-cc=0.448 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.763, AUC: opp-cc=0.481 \n",
      "\tAvg Val Loss: opp-mlo=0.670, AUC: opp-mlo=0.548\n",
      "Best opp-mlo model saved.\n",
      "Iter=200, avg train loss=0.559, \n",
      "\tAvg Val Loss: same-cc=0.712, AUC: same-cc=0.444 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.743, AUC: opp-cc=0.462 \n",
      "\tAvg Val Loss: opp-mlo=0.669, AUC: opp-mlo=0.552\n",
      "Best opp-mlo model saved.\n",
      "Iter=205, avg train loss=0.652, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.446 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.592 \n",
      "\tAvg Val Loss opp-cc=0.729, AUC: opp-cc=0.452 \n",
      "\tAvg Val Loss: opp-mlo=0.642, AUC: opp-mlo=0.538\n",
      "Iter=210, avg train loss=0.615, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.436 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.731, AUC: opp-cc=0.467 \n",
      "\tAvg Val Loss: opp-mlo=0.666, AUC: opp-mlo=0.540\n",
      "Iter=215, avg train loss=0.593, \n",
      "\tAvg Val Loss: same-cc=0.717, AUC: same-cc=0.436 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.734, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.550\n",
      "Iter=220, avg train loss=0.528, \n",
      "\tAvg Val Loss: same-cc=0.694, AUC: same-cc=0.448 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.596 \n",
      "\tAvg Val Loss opp-cc=0.720, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.531\n",
      "Iter=225, avg train loss=0.673, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.540\n",
      "Iter=230, avg train loss=0.671, \n",
      "\tAvg Val Loss: same-cc=0.709, AUC: same-cc=0.442 \n",
      "\tAvg Val Loss: same-mlo=0.625, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.655, AUC: opp-mlo=0.568\n",
      "Best opp-mlo model saved.\n",
      "Iter=235, avg train loss=0.565, \n",
      "\tAvg Val Loss: same-cc=0.745, AUC: same-cc=0.426 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.598 \n",
      "\tAvg Val Loss opp-cc=0.754, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.741, AUC: opp-mlo=0.548\n",
      "Iter=240, avg train loss=0.578, \n",
      "\tAvg Val Loss: same-cc=0.742, AUC: same-cc=0.436 \n",
      "\tAvg Val Loss: same-mlo=0.687, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.765, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.739, AUC: opp-mlo=0.544\n",
      "Iter=245, avg train loss=0.610, \n",
      "\tAvg Val Loss: same-cc=0.724, AUC: same-cc=0.452 \n",
      "\tAvg Val Loss: same-mlo=0.672, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.769, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.700, AUC: opp-mlo=0.550\n",
      "Iter=250, avg train loss=0.635, \n",
      "\tAvg Val Loss: same-cc=0.702, AUC: same-cc=0.454 \n",
      "\tAvg Val Loss: same-mlo=0.636, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.754, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.542\n",
      "Iter=255, avg train loss=0.659, \n",
      "\tAvg Val Loss: same-cc=0.700, AUC: same-cc=0.458 \n",
      "\tAvg Val Loss: same-mlo=0.630, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.755, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.552\n",
      "Best same-mlo model saved.\n",
      "Iter=260, avg train loss=0.582, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.721, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.540\n",
      "Iter=265, avg train loss=0.568, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.485 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.733, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.533\n",
      "Iter=270, avg train loss=0.616, \n",
      "\tAvg Val Loss: same-cc=0.681, AUC: same-cc=0.491 \n",
      "\tAvg Val Loss: same-mlo=0.587, AUC: same-mlo=0.607 \n",
      "\tAvg Val Loss opp-cc=0.751, AUC: opp-cc=0.460 \n",
      "\tAvg Val Loss: opp-mlo=0.643, AUC: opp-mlo=0.471\n",
      "Iter=275, avg train loss=0.532, \n",
      "\tAvg Val Loss: same-cc=0.713, AUC: same-cc=0.485 \n",
      "\tAvg Val Loss: same-mlo=0.571, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.758, AUC: opp-cc=0.448 \n",
      "\tAvg Val Loss: opp-mlo=0.658, AUC: opp-mlo=0.465\n",
      "Iter=280, avg train loss=0.628, \n",
      "\tAvg Val Loss: same-cc=0.706, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.569, AUC: same-mlo=0.621 \n",
      "\tAvg Val Loss opp-cc=0.746, AUC: opp-cc=0.438 \n",
      "\tAvg Val Loss: opp-mlo=0.653, AUC: opp-mlo=0.477\n",
      "Best same-mlo model saved.\n",
      "Iter=285, avg train loss=0.631, \n",
      "\tAvg Val Loss: same-cc=0.730, AUC: same-cc=0.465 \n",
      "\tAvg Val Loss: same-mlo=0.586, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.737, AUC: opp-cc=0.434 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.519\n",
      "Best same-mlo model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=290, avg train loss=0.590, \n",
      "\tAvg Val Loss: same-cc=0.781, AUC: same-cc=0.460 \n",
      "\tAvg Val Loss: same-mlo=0.624, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.742, AUC: opp-cc=0.446 \n",
      "\tAvg Val Loss: opp-mlo=0.719, AUC: opp-mlo=0.536\n",
      "Iter=295, avg train loss=0.569, \n",
      "\tAvg Val Loss: same-cc=0.825, AUC: same-cc=0.465 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.764, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.732, AUC: opp-mlo=0.548\n",
      "Best same-mlo model saved.\n",
      "Iter=300, avg train loss=0.565, \n",
      "\tAvg Val Loss: same-cc=0.806, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.641 \n",
      "\tAvg Val Loss opp-cc=0.765, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.708, AUC: opp-mlo=0.564\n",
      "Best same-mlo model saved.\n",
      "Iter=305, avg train loss=0.625, \n",
      "\tAvg Val Loss: same-cc=0.789, AUC: same-cc=0.469 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.720, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.558\n",
      "Iter=310, avg train loss=0.617, \n",
      "\tAvg Val Loss: same-cc=0.764, AUC: same-cc=0.456 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.650, AUC: opp-mlo=0.542\n",
      "Iter=315, avg train loss=0.561, \n",
      "\tAvg Val Loss: same-cc=0.752, AUC: same-cc=0.479 \n",
      "\tAvg Val Loss: same-mlo=0.617, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.542\n",
      "Best opp-cc model saved.\n",
      "Iter=320, avg train loss=0.568, \n",
      "\tAvg Val Loss: same-cc=0.694, AUC: same-cc=0.471 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.621 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.546\n",
      "Iter=325, avg train loss=0.539, \n",
      "\tAvg Val Loss: same-cc=0.700, AUC: same-cc=0.454 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.705, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.558\n",
      "Iter=330, avg train loss=0.538, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.458 \n",
      "\tAvg Val Loss: same-mlo=0.614, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.755, AUC: opp-cc=0.499 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.566\n",
      "Iter=335, avg train loss=0.607, \n",
      "\tAvg Val Loss: same-cc=0.742, AUC: same-cc=0.464 \n",
      "\tAvg Val Loss: same-mlo=0.626, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.780, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.691, AUC: opp-mlo=0.546\n",
      "Iter=340, avg train loss=0.569, \n",
      "\tAvg Val Loss: same-cc=0.760, AUC: same-cc=0.454 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.768, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.718, AUC: opp-mlo=0.558\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2205dd16b1f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     train_all_views(models, train_loader, val_loader, fold, device, \n\u001b[0;32m---> 82\u001b[0;31m           epochs=epochs, lr=1e-5, check_iters=5, log_name='finetune_multiview_t3p_second_run.txt')\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicting on the test set...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bc_risk_pred/breast_cancer_classifier/model.py\u001b[0m in \u001b[0;36mtrain_all_views\u001b[0;34m(models, train_loader, val_loader, fold_num, device, epochs, lr, weight_decay, check_iters, log_name)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mavg_val_loss_same_cc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc_same_cc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss_single_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_same_cc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'same-cc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_auc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mavg_val_loss_same_mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc_same_mlo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss_single_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_same_mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'same-mlo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mreturn_auc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mavg_val_loss_opp_cc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc_opp_cc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss_single_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_opp_cc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opposite-cc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mreturn_auc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mavg_val_loss_opp_mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc_opp_mlo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss_single_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_opp_mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opposite-mlo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mreturn_auc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bc_risk_pred/breast_cancer_classifier/model.py\u001b[0m in \u001b[0;36mval_loss_single_view\u001b[0;34m(model, val_loader, view, device, return_auc)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0my_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mbatch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "cpu_threads = 4\n",
    "batch_size = 4\n",
    "n_folds = 5\n",
    "epochs = 20\n",
    "subject_pool, exam_pool = [], []\n",
    "pred_pool, label_pool, machine_pool = [], [], []\n",
    "age_pool, race_pool, bmi_pool = [], [], []\n",
    "birads_pool, libra_pool = [], []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=12345)\n",
    "fold = 0\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t3), 1)), ys_t3):\n",
    "    fold += 1\n",
    "    # train-val-test idx.\n",
    "    train_y = ys_t3[train_ix_]\n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(\n",
    "        train_ix_, test_size=test_prop, \n",
    "        stratify=train_y, random_state=12345)\n",
    "    # subset.\n",
    "    train_dataset = Subset(risk_dataset_t3, train_ix)\n",
    "    val_dataset = Subset(risk_dataset_t3, val_ix)\n",
    "    test_dataset = Subset(risk_dataset_t3, test_ix)\n",
    "    train_y = ys_t3[train_ix]\n",
    "    val_y = ys_t3[val_ix]\n",
    "    test_y = ys_t3[test_ix]\n",
    "    # weighted sampler.\n",
    "    f0, f1 = np.bincount(train_y)\n",
    "    train_w = np.zeros_like(train_y, dtype='float')\n",
    "    train_w[train_y==0] = 1/f0\n",
    "    train_w[train_y==1] = 1/f1\n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        train_w, len(train_y)//batch_size*batch_size, \n",
    "        replacement=True)\n",
    "    \n",
    "    # data loaders.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "        collate_fn=mammo_collate)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    \n",
    "    models = dict.fromkeys(('same-cc', 'same-mlo', 'opp-cc', 'opp-mlo'))\n",
    "    \n",
    "    image_only_parameters_rcc = shared_parameters.copy()\n",
    "    image_only_parameters_rcc[\"view\"] = \"R-CC\"\n",
    "    image_only_parameters_rcc[\"use_heatmaps\"] = False\n",
    "    image_only_parameters_rcc[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "    \n",
    "    image_only_parameters_rmlo = shared_parameters.copy()\n",
    "    image_only_parameters_rmlo[\"view\"] = \"R-MLO\"\n",
    "    image_only_parameters_rmlo[\"use_heatmaps\"] = False\n",
    "    image_only_parameters_rmlo[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "    \n",
    "    image_only_parameters_lmlo = shared_parameters.copy()\n",
    "    image_only_parameters_lmlo[\"view\"] = \"L-MLO\"\n",
    "    image_only_parameters_lmlo[\"use_heatmaps\"] = False\n",
    "    image_only_parameters_lmlo[\"model_path\"] = \"models/ImageOnly__ModeImage_weights.p\"\n",
    "    \n",
    "    model_rcc, _ = load_model(image_only_parameters_rcc)\n",
    "    model_rmlo, _ = load_model(image_only_parameters_rmlo)\n",
    "    model_lmlo, _ = load_model(image_only_parameters_lmlo)\n",
    "    \n",
    "    model_rcc = nn.DataParallel(model_rcc)\n",
    "    model_rcc = model_rcc.to(device)\n",
    "    \n",
    "    model_rmlo = nn.DataParallel(model_rmlo)\n",
    "    model_rmlo = model_rmlo.to(device)\n",
    "    \n",
    "    model_lmlo = nn.DataParallel(model_lmlo)\n",
    "    model_lmlo = model_lmlo.to(device)\n",
    "    \n",
    "    models = {\n",
    "        'rcc': model_rcc,\n",
    "        'rmlo': model_rmlo,\n",
    "        'lmlo': model_lmlo\n",
    "    }\n",
    "        \n",
    "    # train & test.\n",
    "    best_name_ = 'best_model_{}_placeholder.pt'.format(fold)\n",
    "    print('='*10, 'Fold', fold, '='*10)\n",
    "    \n",
    "    _, start_auc_rcc = val_loss(model_rcc, test_loader, device, return_auc=True)\n",
    "    start_auc_m_rcc = test_max_auc(model_rcc, test_loader, device)\n",
    "    \n",
    "    _, start_auc_rmlo = val_loss(model_rmlo, test_loader, device, return_auc=True)\n",
    "    start_auc_m_rmlo = test_max_auc(model_rmlo, test_loader, device)\n",
    "    \n",
    "    _, start_auc_lmlo = val_loss(model_lmlo, test_loader, device, return_auc=True)\n",
    "    start_auc_m_lmlo = test_max_auc(model_lmlo, test_loader, device)\n",
    "    \n",
    "    #start_auc_m = test_max_auc(model, test_loader, device)\n",
    "    \n",
    "    print('Test AUC at start: rcc={:.3f}, rmlo={:.3f}, lmlo={:.3f}'.format(start_auc_rcc, start_auc_rmlo, start_auc_lmlo))\n",
    "    print('max-score-based AUC: rcc={:.3f}, rmlo={:.3f}, lmlo={:.3f}'.format(start_auc_m_rcc, start_auc_m_rmlo, start_auc_m_lmlo))\n",
    "    \n",
    "    train_all_views(models, train_loader, val_loader, fold, device, \n",
    "          epochs=epochs, lr=1e-5, check_iters=5, log_name='finetune_multiview_t3p_third_run.txt')\n",
    "    \n",
    "    print('Predicting on the test set...', end='')\n",
    "    subject_list, exam_list, \\\n",
    "    pred_list, label_list, machine_list, \\\n",
    "    age_list, race_list, bmi_list, \\\n",
    "    birads_list, libra_list = test_all_views(models, test_loader, device)\n",
    "    \n",
    "    subject_pool.extend(subject_list)\n",
    "    exam_pool.extend(exam_list)\n",
    "    pred_pool.extend(pred_list)\n",
    "    label_pool.extend(label_list)\n",
    "    machine_pool.extend(machine_list)\n",
    "    age_pool.extend(age_list)\n",
    "    race_pool.extend(race_list)\n",
    "    bmi_pool.extend(bmi_list)\n",
    "    birads_pool.extend(birads_list)\n",
    "    libra_pool.extend(libra_list) \n",
    "    \n",
    "    # test AUC.\n",
    "    _, end_auc_same_cc = val_loss_single_view(models['same-cc'], test_loader, 'same-cc', device, return_auc=True)\n",
    "    _, end_auc_same_mlo = val_loss_single_view(models['same-mlo'], test_loader, 'same-mlo', device, return_auc=True)\n",
    "    _, end_auc_opp_cc = val_loss_single_view(models['opp-cc'], test_loader, 'opposite-cc', device, return_auc=True)\n",
    "    _, end_auc_opp_mlo = val_loss_single_view(models['opp-mlo'], test_loader, 'opposite-mlo', device, return_auc=True)\n",
    "    \n",
    "    \n",
    "    fold_auc_max, fold_auc_mean = test_auc_all_views(models, test_loader, device)\n",
    "    \n",
    "    print('Done')\n",
    "    print('Test AUC after training: same-cc={:.3f}, same-mlo={:.3f}, opp-cc={:.3f}, opp-mlo={:.3f}'.format(\n",
    "        end_auc_same_cc, end_auc_same_mlo, end_auc_opp_cc, end_auc_opp_mlo))\n",
    "    print('Max-Score Based AUC After Training: {:.3f}, Mean-Score Based AUC After Training: {:.3f}'.format(fold_auc_max, fold_auc_mean))\n",
    "    print()\n",
    "    \n",
    "    if fold < n_folds:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t3p_mv = np.concatenate(subject_pool)\n",
    "all_exam_t3p_mv = np.concatenate(exam_pool)\n",
    "all_preds_t3p_mv = torch.cat(pred_pool)\n",
    "all_labels_t3p_mv = torch.cat(label_pool)\n",
    "all_preds_t3p_mv = all_preds_t3p_mv.cpu().numpy()\n",
    "all_labels_t3p_mv = all_labels_t3p_mv.numpy()\n",
    "all_probs_max_t3p_mv = all_preds_t3p_mv.max(1)\n",
    "all_probs_mean_t3p_mv = all_preds_t3p_mv.mean(1)\n",
    "\n",
    "all_machines_t3p_mv = np.concatenate(machine_pool)\n",
    "all_ages_t3p_mv = np.concatenate(age_pool)\n",
    "all_races_t3p_mv = np.concatenate(race_pool)\n",
    "all_bmis_t3p_mv = np.concatenate(bmi_pool)\n",
    "all_birads_t3p_mv = np.concatenate(birads_pool)\n",
    "all_libras_t3p_mv = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_auroc = 0\n",
    "for i in range(4):\n",
    "    roc = roc_auc_score(all_labels_t3p_mv, all_preds_t3p_mv[:, i])\n",
    "    print(i, roc)\n",
    "    total_auroc += roc\n",
    "print(total_auroc / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=256\n",
      "4view max AUC=0.592\n",
      "4view mean AUC=0.609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t3p_mv)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t3p_mv, all_probs_max_t3p_mv)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t3p_mv, all_preds_t3p_mv.mean(1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1+ Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Fold 1 ==========\n",
      "Test AUC at start: same-cc=0.474, same-mlo=0.465, opp-cc=0.494, opp-mlo=0.501\n",
      "Max-Score Based AUC Before Training: 0.456, Mean-Score Based AUC Before Training: 0.484\n",
      "Iter=5, avg train loss=1.010, \n",
      "\tAvg Val Loss: same-cc=0.626, AUC: same-cc=0.423 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.360 \n",
      "\tAvg Val Loss opp-cc=0.651, AUC: opp-cc=0.400 \n",
      "\tAvg Val Loss: opp-mlo=0.644, AUC: opp-mlo=0.378\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=10, avg train loss=0.696, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.414 \n",
      "\tAvg Val Loss: same-mlo=0.663, AUC: same-mlo=0.370 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.415 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.377\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Iter=15, avg train loss=0.905, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.408 \n",
      "\tAvg Val Loss: same-mlo=0.671, AUC: same-mlo=0.391 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.406 \n",
      "\tAvg Val Loss: opp-mlo=0.665, AUC: opp-mlo=0.363\n",
      "Best same-mlo model saved.\n",
      "Iter=20, avg train loss=0.847, \n",
      "\tAvg Val Loss: same-cc=0.684, AUC: same-cc=0.410 \n",
      "\tAvg Val Loss: same-mlo=0.688, AUC: same-mlo=0.396 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.418 \n",
      "\tAvg Val Loss: opp-mlo=0.685, AUC: opp-mlo=0.384\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=25, avg train loss=0.611, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.413 \n",
      "\tAvg Val Loss: same-mlo=0.687, AUC: same-mlo=0.394 \n",
      "\tAvg Val Loss opp-cc=0.722, AUC: opp-cc=0.420 \n",
      "\tAvg Val Loss: opp-mlo=0.683, AUC: opp-mlo=0.388\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=30, avg train loss=0.842, \n",
      "\tAvg Val Loss: same-cc=0.704, AUC: same-cc=0.404 \n",
      "\tAvg Val Loss: same-mlo=0.672, AUC: same-mlo=0.377 \n",
      "\tAvg Val Loss opp-cc=0.715, AUC: opp-cc=0.432 \n",
      "\tAvg Val Loss: opp-mlo=0.670, AUC: opp-mlo=0.376\n",
      "Best opp-cc model saved.\n",
      "Iter=35, avg train loss=0.624, \n",
      "\tAvg Val Loss: same-cc=0.723, AUC: same-cc=0.407 \n",
      "\tAvg Val Loss: same-mlo=0.674, AUC: same-mlo=0.397 \n",
      "\tAvg Val Loss opp-cc=0.741, AUC: opp-cc=0.425 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.381\n",
      "Best same-mlo model saved.\n",
      "Iter=40, avg train loss=0.727, \n",
      "\tAvg Val Loss: same-cc=0.734, AUC: same-cc=0.415 \n",
      "\tAvg Val Loss: same-mlo=0.682, AUC: same-mlo=0.412 \n",
      "\tAvg Val Loss opp-cc=0.750, AUC: opp-cc=0.419 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.379\n",
      "Best same-mlo model saved.\n",
      "Iter=45, avg train loss=0.674, \n",
      "\tAvg Val Loss: same-cc=0.729, AUC: same-cc=0.428 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.415 \n",
      "\tAvg Val Loss opp-cc=0.744, AUC: opp-cc=0.414 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.371\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Iter=50, avg train loss=0.646, \n",
      "\tAvg Val Loss: same-cc=0.731, AUC: same-cc=0.423 \n",
      "\tAvg Val Loss: same-mlo=0.671, AUC: same-mlo=0.404 \n",
      "\tAvg Val Loss opp-cc=0.742, AUC: opp-cc=0.413 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.375\n",
      "Iter=55, avg train loss=0.868, \n",
      "\tAvg Val Loss: same-cc=0.747, AUC: same-cc=0.418 \n",
      "\tAvg Val Loss: same-mlo=0.671, AUC: same-mlo=0.399 \n",
      "\tAvg Val Loss opp-cc=0.731, AUC: opp-cc=0.414 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.387\n",
      "Iter=60, avg train loss=0.717, \n",
      "\tAvg Val Loss: same-cc=0.740, AUC: same-cc=0.399 \n",
      "\tAvg Val Loss: same-mlo=0.658, AUC: same-mlo=0.399 \n",
      "\tAvg Val Loss opp-cc=0.739, AUC: opp-cc=0.419 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.384\n",
      "Iter=65, avg train loss=0.706, \n",
      "\tAvg Val Loss: same-cc=0.732, AUC: same-cc=0.393 \n",
      "\tAvg Val Loss: same-mlo=0.667, AUC: same-mlo=0.417 \n",
      "\tAvg Val Loss opp-cc=0.754, AUC: opp-cc=0.412 \n",
      "\tAvg Val Loss: opp-mlo=0.686, AUC: opp-mlo=0.393\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=70, avg train loss=0.635, \n",
      "\tAvg Val Loss: same-cc=0.744, AUC: same-cc=0.384 \n",
      "\tAvg Val Loss: same-mlo=0.660, AUC: same-mlo=0.411 \n",
      "\tAvg Val Loss opp-cc=0.770, AUC: opp-cc=0.401 \n",
      "\tAvg Val Loss: opp-mlo=0.681, AUC: opp-mlo=0.373\n",
      "Iter=75, avg train loss=0.774, \n",
      "\tAvg Val Loss: same-cc=0.744, AUC: same-cc=0.385 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.419 \n",
      "\tAvg Val Loss opp-cc=0.784, AUC: opp-cc=0.392 \n",
      "\tAvg Val Loss: opp-mlo=0.683, AUC: opp-mlo=0.373\n",
      "Best same-mlo model saved.\n",
      "Iter=80, avg train loss=0.664, \n",
      "\tAvg Val Loss: same-cc=0.737, AUC: same-cc=0.395 \n",
      "\tAvg Val Loss: same-mlo=0.662, AUC: same-mlo=0.416 \n",
      "\tAvg Val Loss opp-cc=0.774, AUC: opp-cc=0.383 \n",
      "\tAvg Val Loss: opp-mlo=0.687, AUC: opp-mlo=0.382\n",
      "Iter=85, avg train loss=0.629, \n",
      "\tAvg Val Loss: same-cc=0.773, AUC: same-cc=0.403 \n",
      "\tAvg Val Loss: same-mlo=0.675, AUC: same-mlo=0.433 \n",
      "\tAvg Val Loss opp-cc=0.807, AUC: opp-cc=0.382 \n",
      "\tAvg Val Loss: opp-mlo=0.710, AUC: opp-mlo=0.386\n",
      "Best same-mlo model saved.\n",
      "Iter=90, avg train loss=0.616, \n",
      "\tAvg Val Loss: same-cc=0.774, AUC: same-cc=0.405 \n",
      "\tAvg Val Loss: same-mlo=0.675, AUC: same-mlo=0.449 \n",
      "\tAvg Val Loss opp-cc=0.811, AUC: opp-cc=0.371 \n",
      "\tAvg Val Loss: opp-mlo=0.719, AUC: opp-mlo=0.383\n",
      "Best same-mlo model saved.\n",
      "Iter=95, avg train loss=0.710, \n",
      "\tAvg Val Loss: same-cc=0.755, AUC: same-cc=0.414 \n",
      "\tAvg Val Loss: same-mlo=0.682, AUC: same-mlo=0.445 \n",
      "\tAvg Val Loss opp-cc=0.817, AUC: opp-cc=0.366 \n",
      "\tAvg Val Loss: opp-mlo=0.727, AUC: opp-mlo=0.390\n",
      "Iter=100, avg train loss=0.653, \n",
      "\tAvg Val Loss: same-cc=0.783, AUC: same-cc=0.414 \n",
      "\tAvg Val Loss: same-mlo=0.704, AUC: same-mlo=0.449 \n",
      "\tAvg Val Loss opp-cc=0.861, AUC: opp-cc=0.361 \n",
      "\tAvg Val Loss: opp-mlo=0.740, AUC: opp-mlo=0.368\n",
      "Iter=105, avg train loss=0.717, \n",
      "\tAvg Val Loss: same-cc=0.801, AUC: same-cc=0.411 \n",
      "\tAvg Val Loss: same-mlo=0.714, AUC: same-mlo=0.448 \n",
      "\tAvg Val Loss opp-cc=0.871, AUC: opp-cc=0.358 \n",
      "\tAvg Val Loss: opp-mlo=0.746, AUC: opp-mlo=0.372\n",
      "Iter=110, avg train loss=0.637, \n",
      "\tAvg Val Loss: same-cc=0.804, AUC: same-cc=0.416 \n",
      "\tAvg Val Loss: same-mlo=0.712, AUC: same-mlo=0.457 \n",
      "\tAvg Val Loss opp-cc=0.831, AUC: opp-cc=0.370 \n",
      "\tAvg Val Loss: opp-mlo=0.727, AUC: opp-mlo=0.386\n",
      "Best same-mlo model saved.\n",
      "Iter=115, avg train loss=0.704, \n",
      "\tAvg Val Loss: same-cc=0.790, AUC: same-cc=0.427 \n",
      "\tAvg Val Loss: same-mlo=0.702, AUC: same-mlo=0.486 \n",
      "\tAvg Val Loss opp-cc=0.797, AUC: opp-cc=0.373 \n",
      "\tAvg Val Loss: opp-mlo=0.736, AUC: opp-mlo=0.410\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=120, avg train loss=0.647, \n",
      "\tAvg Val Loss: same-cc=0.800, AUC: same-cc=0.435 \n",
      "\tAvg Val Loss: same-mlo=0.709, AUC: same-mlo=0.492 \n",
      "\tAvg Val Loss opp-cc=0.788, AUC: opp-cc=0.379 \n",
      "\tAvg Val Loss: opp-mlo=0.725, AUC: opp-mlo=0.422\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=125, avg train loss=0.628, \n",
      "\tAvg Val Loss: same-cc=0.773, AUC: same-cc=0.447 \n",
      "\tAvg Val Loss: same-mlo=0.701, AUC: same-mlo=0.501 \n",
      "\tAvg Val Loss opp-cc=0.800, AUC: opp-cc=0.365 \n",
      "\tAvg Val Loss: opp-mlo=0.736, AUC: opp-mlo=0.425\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=130, avg train loss=0.703, \n",
      "\tAvg Val Loss: same-cc=0.765, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.708, AUC: same-mlo=0.491 \n",
      "\tAvg Val Loss opp-cc=0.774, AUC: opp-cc=0.382 \n",
      "\tAvg Val Loss: opp-mlo=0.728, AUC: opp-mlo=0.431\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=135, avg train loss=0.681, \n",
      "\tAvg Val Loss: same-cc=0.763, AUC: same-cc=0.452 \n",
      "\tAvg Val Loss: same-mlo=0.706, AUC: same-mlo=0.498 \n",
      "\tAvg Val Loss opp-cc=0.775, AUC: opp-cc=0.375 \n",
      "\tAvg Val Loss: opp-mlo=0.721, AUC: opp-mlo=0.438\n",
      "Best opp-mlo model saved.\n",
      "Iter=140, avg train loss=0.714, \n",
      "\tAvg Val Loss: same-cc=0.758, AUC: same-cc=0.461 \n",
      "\tAvg Val Loss: same-mlo=0.714, AUC: same-mlo=0.499 \n",
      "\tAvg Val Loss opp-cc=0.812, AUC: opp-cc=0.368 \n",
      "\tAvg Val Loss: opp-mlo=0.736, AUC: opp-mlo=0.441\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=145, avg train loss=0.681, \n",
      "\tAvg Val Loss: same-cc=0.722, AUC: same-cc=0.497 \n",
      "\tAvg Val Loss: same-mlo=0.703, AUC: same-mlo=0.502 \n",
      "\tAvg Val Loss opp-cc=0.795, AUC: opp-cc=0.375 \n",
      "\tAvg Val Loss: opp-mlo=0.710, AUC: opp-mlo=0.441\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Iter=150, avg train loss=0.559, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.507 \n",
      "\tAvg Val Loss: same-mlo=0.695, AUC: same-mlo=0.508 \n",
      "\tAvg Val Loss opp-cc=0.792, AUC: opp-cc=0.372 \n",
      "\tAvg Val Loss: opp-mlo=0.700, AUC: opp-mlo=0.454\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=155, avg train loss=0.651, \n",
      "\tAvg Val Loss: same-cc=0.711, AUC: same-cc=0.530 \n",
      "\tAvg Val Loss: same-mlo=0.691, AUC: same-mlo=0.521 \n",
      "\tAvg Val Loss opp-cc=0.791, AUC: opp-cc=0.383 \n",
      "\tAvg Val Loss: opp-mlo=0.706, AUC: opp-mlo=0.460\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=160, avg train loss=0.689, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.537 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.523 \n",
      "\tAvg Val Loss opp-cc=0.746, AUC: opp-cc=0.394 \n",
      "\tAvg Val Loss: opp-mlo=0.699, AUC: opp-mlo=0.468\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=165, avg train loss=0.672, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.520 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.531 \n",
      "\tAvg Val Loss opp-cc=0.786, AUC: opp-cc=0.395 \n",
      "\tAvg Val Loss: opp-mlo=0.715, AUC: opp-mlo=0.463\n",
      "Best same-mlo model saved.\n",
      "Iter=170, avg train loss=0.679, \n",
      "\tAvg Val Loss: same-cc=0.735, AUC: same-cc=0.523 \n",
      "\tAvg Val Loss: same-mlo=0.697, AUC: same-mlo=0.544 \n",
      "\tAvg Val Loss opp-cc=0.806, AUC: opp-cc=0.388 \n",
      "\tAvg Val Loss: opp-mlo=0.718, AUC: opp-mlo=0.459\n",
      "Best same-mlo model saved.\n",
      "Iter=175, avg train loss=0.619, \n",
      "\tAvg Val Loss: same-cc=0.740, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.699, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.788, AUC: opp-cc=0.403 \n",
      "\tAvg Val Loss: opp-mlo=0.727, AUC: opp-mlo=0.465\n",
      "Best same-mlo model saved.\n",
      "Iter=180, avg train loss=0.624, \n",
      "\tAvg Val Loss: same-cc=0.753, AUC: same-cc=0.501 \n",
      "\tAvg Val Loss: same-mlo=0.693, AUC: same-mlo=0.553 \n",
      "\tAvg Val Loss opp-cc=0.772, AUC: opp-cc=0.394 \n",
      "\tAvg Val Loss: opp-mlo=0.714, AUC: opp-mlo=0.464\n",
      "Best same-mlo model saved.\n",
      "Iter=185, avg train loss=0.663, \n",
      "\tAvg Val Loss: same-cc=0.720, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.686, AUC: same-mlo=0.556 \n",
      "\tAvg Val Loss opp-cc=0.747, AUC: opp-cc=0.418 \n",
      "\tAvg Val Loss: opp-mlo=0.696, AUC: opp-mlo=0.471\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=190, avg train loss=0.659, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.523 \n",
      "\tAvg Val Loss: same-mlo=0.666, AUC: same-mlo=0.574 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.432 \n",
      "\tAvg Val Loss: opp-mlo=0.691, AUC: opp-mlo=0.472\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=195, avg train loss=0.633, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.532 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.574 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.447 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.473\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=200, avg train loss=0.574, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.625, AUC: same-mlo=0.575 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.440 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.474\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=205, avg train loss=0.605, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.528 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.426 \n",
      "\tAvg Val Loss: opp-mlo=0.648, AUC: opp-mlo=0.471\n",
      "Iter=210, avg train loss=0.587, \n",
      "\tAvg Val Loss: same-cc=0.649, AUC: same-cc=0.518 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.567 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.421 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.479\n",
      "Best opp-mlo model saved.\n",
      "Iter=215, avg train loss=0.632, \n",
      "\tAvg Val Loss: same-cc=0.681, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.550 \n",
      "\tAvg Val Loss opp-cc=0.723, AUC: opp-cc=0.423 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.490\n",
      "Best opp-mlo model saved.\n",
      "Iter=220, avg train loss=0.667, \n",
      "\tAvg Val Loss: same-cc=0.721, AUC: same-cc=0.530 \n",
      "\tAvg Val Loss: same-mlo=0.702, AUC: same-mlo=0.547 \n",
      "\tAvg Val Loss opp-cc=0.766, AUC: opp-cc=0.413 \n",
      "\tAvg Val Loss: opp-mlo=0.697, AUC: opp-mlo=0.478\n",
      "Iter=225, avg train loss=0.596, \n",
      "\tAvg Val Loss: same-cc=0.743, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.724, AUC: same-mlo=0.545 \n",
      "\tAvg Val Loss opp-cc=0.794, AUC: opp-cc=0.408 \n",
      "\tAvg Val Loss: opp-mlo=0.716, AUC: opp-mlo=0.469\n",
      "Iter=230, avg train loss=0.710, \n",
      "\tAvg Val Loss: same-cc=0.736, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.720, AUC: same-mlo=0.564 \n",
      "\tAvg Val Loss opp-cc=0.797, AUC: opp-cc=0.415 \n",
      "\tAvg Val Loss: opp-mlo=0.712, AUC: opp-mlo=0.486\n",
      "Iter=235, avg train loss=0.636, \n",
      "\tAvg Val Loss: same-cc=0.745, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.721, AUC: same-mlo=0.573 \n",
      "\tAvg Val Loss opp-cc=0.787, AUC: opp-cc=0.420 \n",
      "\tAvg Val Loss: opp-mlo=0.732, AUC: opp-mlo=0.475\n",
      "Iter=240, avg train loss=0.566, \n",
      "\tAvg Val Loss: same-cc=0.762, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.759, AUC: same-mlo=0.567 \n",
      "\tAvg Val Loss opp-cc=0.784, AUC: opp-cc=0.411 \n",
      "\tAvg Val Loss: opp-mlo=0.809, AUC: opp-mlo=0.453\n",
      "Iter=245, avg train loss=0.619, \n",
      "\tAvg Val Loss: same-cc=0.742, AUC: same-cc=0.517 \n",
      "\tAvg Val Loss: same-mlo=0.785, AUC: same-mlo=0.564 \n",
      "\tAvg Val Loss opp-cc=0.740, AUC: opp-cc=0.416 \n",
      "\tAvg Val Loss: opp-mlo=0.854, AUC: opp-mlo=0.440\n",
      "Iter=250, avg train loss=0.627, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.523 \n",
      "\tAvg Val Loss: same-mlo=0.758, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.728, AUC: opp-cc=0.423 \n",
      "\tAvg Val Loss: opp-mlo=0.814, AUC: opp-mlo=0.449\n",
      "Iter=255, avg train loss=0.632, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.534 \n",
      "\tAvg Val Loss: same-mlo=0.718, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.730, AUC: opp-cc=0.414 \n",
      "\tAvg Val Loss: opp-mlo=0.773, AUC: opp-mlo=0.462\n",
      "Iter=260, avg train loss=0.650, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.699, AUC: same-mlo=0.555 \n",
      "\tAvg Val Loss opp-cc=0.731, AUC: opp-cc=0.413 \n",
      "\tAvg Val Loss: opp-mlo=0.767, AUC: opp-mlo=0.466\n",
      "Iter=265, avg train loss=0.606, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.516 \n",
      "\tAvg Val Loss: same-mlo=0.697, AUC: same-mlo=0.547 \n",
      "\tAvg Val Loss opp-cc=0.743, AUC: opp-cc=0.409 \n",
      "\tAvg Val Loss: opp-mlo=0.757, AUC: opp-mlo=0.452\n",
      "Iter=270, avg train loss=0.604, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.524 \n",
      "\tAvg Val Loss: same-mlo=0.664, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.420 \n",
      "\tAvg Val Loss: opp-mlo=0.721, AUC: opp-mlo=0.454\n",
      "Iter=275, avg train loss=0.545, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.686, AUC: opp-cc=0.413 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.455\n",
      "Iter=280, avg train loss=0.601, \n",
      "\tAvg Val Loss: same-cc=0.616, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.617, AUC: same-mlo=0.557 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.400 \n",
      "\tAvg Val Loss: opp-mlo=0.627, AUC: opp-mlo=0.471\n",
      "Iter=285, avg train loss=0.671, \n",
      "\tAvg Val Loss: same-cc=0.627, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.558 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.434 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.479\n",
      "Best same-cc model saved.\n",
      "Iter=290, avg train loss=0.655, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.532 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.555 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.468 \n",
      "\tAvg Val Loss: opp-mlo=0.619, AUC: opp-mlo=0.495\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=295, avg train loss=0.672, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.556 \n",
      "\tAvg Val Loss opp-cc=0.711, AUC: opp-cc=0.470 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.506\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=300, avg train loss=0.606, \n",
      "\tAvg Val Loss: same-cc=0.680, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.724, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.736, AUC: opp-cc=0.453 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.493\n",
      "Iter=305, avg train loss=0.612, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.510 \n",
      "\tAvg Val Loss: same-mlo=0.728, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.741, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.687, AUC: opp-mlo=0.492\n",
      "Best opp-cc model saved.\n",
      "Iter=310, avg train loss=0.651, \n",
      "\tAvg Val Loss: same-cc=0.738, AUC: same-cc=0.513 \n",
      "\tAvg Val Loss: same-mlo=0.763, AUC: same-mlo=0.534 \n",
      "\tAvg Val Loss opp-cc=0.743, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.760, AUC: opp-mlo=0.477\n",
      "Best opp-cc model saved.\n",
      "Iter=315, avg train loss=0.674, \n",
      "\tAvg Val Loss: same-cc=0.805, AUC: same-cc=0.479 \n",
      "\tAvg Val Loss: same-mlo=0.810, AUC: same-mlo=0.540 \n",
      "\tAvg Val Loss opp-cc=0.814, AUC: opp-cc=0.460 \n",
      "\tAvg Val Loss: opp-mlo=0.851, AUC: opp-mlo=0.454\n",
      "Iter=320, avg train loss=0.646, \n",
      "\tAvg Val Loss: same-cc=0.747, AUC: same-cc=0.504 \n",
      "\tAvg Val Loss: same-mlo=0.769, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.764, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.807, AUC: opp-mlo=0.458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=325, avg train loss=0.612, \n",
      "\tAvg Val Loss: same-cc=0.678, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.698, AUC: same-mlo=0.549 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.744, AUC: opp-mlo=0.480\n",
      "Iter=330, avg train loss=0.550, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.530 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.549 \n",
      "\tAvg Val Loss opp-cc=0.720, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.779, AUC: opp-mlo=0.468\n",
      "Iter=335, avg train loss=0.552, \n",
      "\tAvg Val Loss: same-cc=0.681, AUC: same-cc=0.512 \n",
      "\tAvg Val Loss: same-mlo=0.666, AUC: same-mlo=0.551 \n",
      "\tAvg Val Loss opp-cc=0.715, AUC: opp-cc=0.464 \n",
      "\tAvg Val Loss: opp-mlo=0.775, AUC: opp-mlo=0.470\n",
      "Iter=340, avg train loss=0.510, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.508 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.462 \n",
      "\tAvg Val Loss: opp-mlo=0.727, AUC: opp-mlo=0.481\n",
      "Iter=345, avg train loss=0.619, \n",
      "\tAvg Val Loss: same-cc=0.768, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.660, AUC: same-mlo=0.544 \n",
      "\tAvg Val Loss opp-cc=0.713, AUC: opp-cc=0.455 \n",
      "\tAvg Val Loss: opp-mlo=0.744, AUC: opp-mlo=0.480\n",
      "Iter=350, avg train loss=0.551, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.514 \n",
      "\tAvg Val Loss: same-mlo=0.624, AUC: same-mlo=0.562 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.458 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.476\n",
      "Iter=355, avg train loss=0.640, \n",
      "\tAvg Val Loss: same-cc=0.713, AUC: same-cc=0.508 \n",
      "\tAvg Val Loss: same-mlo=0.626, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.689, AUC: opp-cc=0.462 \n",
      "\tAvg Val Loss: opp-mlo=0.671, AUC: opp-mlo=0.474\n",
      "Iter=360, avg train loss=0.571, \n",
      "\tAvg Val Loss: same-cc=0.698, AUC: same-cc=0.510 \n",
      "\tAvg Val Loss: same-mlo=0.647, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.461 \n",
      "\tAvg Val Loss: opp-mlo=0.694, AUC: opp-mlo=0.472\n",
      "Iter=365, avg train loss=0.639, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.674, AUC: same-mlo=0.534 \n",
      "\tAvg Val Loss opp-cc=0.729, AUC: opp-cc=0.461 \n",
      "\tAvg Val Loss: opp-mlo=0.770, AUC: opp-mlo=0.461\n",
      "Iter=370, avg train loss=0.603, \n",
      "\tAvg Val Loss: same-cc=0.663, AUC: same-cc=0.535 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.553 \n",
      "\tAvg Val Loss opp-cc=0.710, AUC: opp-cc=0.457 \n",
      "\tAvg Val Loss: opp-mlo=0.723, AUC: opp-mlo=0.457\n",
      "Iter=375, avg train loss=0.619, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.518 \n",
      "\tAvg Val Loss: same-mlo=0.666, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.739, AUC: opp-cc=0.472 \n",
      "\tAvg Val Loss: opp-mlo=0.739, AUC: opp-mlo=0.464\n",
      "Iter=380, avg train loss=0.573, \n",
      "\tAvg Val Loss: same-cc=0.711, AUC: same-cc=0.534 \n",
      "\tAvg Val Loss: same-mlo=0.696, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.747, AUC: opp-cc=0.452 \n",
      "\tAvg Val Loss: opp-mlo=0.733, AUC: opp-mlo=0.463\n",
      "Iter=385, avg train loss=0.604, \n",
      "\tAvg Val Loss: same-cc=0.669, AUC: same-cc=0.541 \n",
      "\tAvg Val Loss: same-mlo=0.698, AUC: same-mlo=0.530 \n",
      "\tAvg Val Loss opp-cc=0.748, AUC: opp-cc=0.438 \n",
      "\tAvg Val Loss: opp-mlo=0.731, AUC: opp-mlo=0.458\n",
      "Iter=390, avg train loss=0.638, \n",
      "\tAvg Val Loss: same-cc=0.641, AUC: same-cc=0.528 \n",
      "\tAvg Val Loss: same-mlo=0.655, AUC: same-mlo=0.527 \n",
      "\tAvg Val Loss opp-cc=0.707, AUC: opp-cc=0.435 \n",
      "\tAvg Val Loss: opp-mlo=0.692, AUC: opp-mlo=0.472\n",
      "Iter=395, avg train loss=0.576, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.544 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.534 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.426 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.464\n",
      "Best same-cc model saved.\n",
      "Iter=400, avg train loss=0.534, \n",
      "\tAvg Val Loss: same-cc=0.604, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.625, AUC: same-mlo=0.532 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.461 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.478\n",
      "Best same-cc model saved.\n",
      "Iter=405, avg train loss=0.592, \n",
      "\tAvg Val Loss: same-cc=0.590, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.537 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.727, AUC: opp-mlo=0.477\n",
      "Best same-cc model saved.\n",
      "Iter=410, avg train loss=0.538, \n",
      "\tAvg Val Loss: same-cc=0.610, AUC: same-cc=0.595 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.549 \n",
      "\tAvg Val Loss opp-cc=0.704, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.800, AUC: opp-mlo=0.460\n",
      "Best same-cc model saved.\n",
      "Iter=415, avg train loss=0.546, \n",
      "\tAvg Val Loss: same-cc=0.669, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.639, AUC: same-mlo=0.545 \n",
      "\tAvg Val Loss opp-cc=0.746, AUC: opp-cc=0.462 \n",
      "\tAvg Val Loss: opp-mlo=0.803, AUC: opp-mlo=0.457\n",
      "Iter=420, avg train loss=0.505, \n",
      "\tAvg Val Loss: same-cc=0.668, AUC: same-cc=0.584 \n",
      "\tAvg Val Loss: same-mlo=0.631, AUC: same-mlo=0.550 \n",
      "\tAvg Val Loss opp-cc=0.773, AUC: opp-cc=0.447 \n",
      "\tAvg Val Loss: opp-mlo=0.803, AUC: opp-mlo=0.460\n",
      "Iter=425, avg train loss=0.521, \n",
      "\tAvg Val Loss: same-cc=0.721, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.686, AUC: same-mlo=0.553 \n",
      "\tAvg Val Loss opp-cc=0.843, AUC: opp-cc=0.440 \n",
      "\tAvg Val Loss: opp-mlo=0.857, AUC: opp-mlo=0.454\n",
      "Iter=430, avg train loss=0.605, \n",
      "\tAvg Val Loss: same-cc=0.637, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.678, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.764, AUC: opp-cc=0.457 \n",
      "\tAvg Val Loss: opp-mlo=0.803, AUC: opp-mlo=0.464\n",
      "Iter=435, avg train loss=0.520, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.712, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.768, AUC: opp-cc=0.445 \n",
      "\tAvg Val Loss: opp-mlo=0.815, AUC: opp-mlo=0.459\n",
      "Iter=440, avg train loss=0.620, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.715, AUC: same-mlo=0.546 \n",
      "\tAvg Val Loss opp-cc=0.745, AUC: opp-cc=0.466 \n",
      "\tAvg Val Loss: opp-mlo=0.818, AUC: opp-mlo=0.460\n",
      "Iter=445, avg train loss=0.578, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.585 \n",
      "\tAvg Val Loss: same-mlo=0.744, AUC: same-mlo=0.529 \n",
      "\tAvg Val Loss opp-cc=0.748, AUC: opp-cc=0.475 \n",
      "\tAvg Val Loss: opp-mlo=0.857, AUC: opp-mlo=0.442\n",
      "Iter=450, avg train loss=0.635, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.749, AUC: same-mlo=0.519 \n",
      "\tAvg Val Loss opp-cc=0.755, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.851, AUC: opp-mlo=0.430\n",
      "Iter=455, avg train loss=0.542, \n",
      "\tAvg Val Loss: same-cc=0.603, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.760, AUC: same-mlo=0.519 \n",
      "\tAvg Val Loss opp-cc=0.708, AUC: opp-cc=0.458 \n",
      "\tAvg Val Loss: opp-mlo=0.819, AUC: opp-mlo=0.429\n",
      "Iter=460, avg train loss=0.602, \n",
      "\tAvg Val Loss: same-cc=0.576, AUC: same-cc=0.603 \n",
      "\tAvg Val Loss: same-mlo=0.697, AUC: same-mlo=0.530 \n",
      "\tAvg Val Loss opp-cc=0.667, AUC: opp-cc=0.440 \n",
      "\tAvg Val Loss: opp-mlo=0.717, AUC: opp-mlo=0.421\n",
      "Best same-cc model saved.\n",
      "Iter=465, avg train loss=0.530, \n",
      "\tAvg Val Loss: same-cc=0.586, AUC: same-cc=0.602 \n",
      "\tAvg Val Loss: same-mlo=0.691, AUC: same-mlo=0.524 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.455 \n",
      "\tAvg Val Loss: opp-mlo=0.693, AUC: opp-mlo=0.428\n",
      "Iter=470, avg train loss=0.533, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.571 \n",
      "\tAvg Val Loss: same-mlo=0.685, AUC: same-mlo=0.513 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.456 \n",
      "\tAvg Val Loss: opp-mlo=0.698, AUC: opp-mlo=0.437\n",
      "Iter=475, avg train loss=0.580, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.561 \n",
      "\tAvg Val Loss: same-mlo=0.662, AUC: same-mlo=0.513 \n",
      "\tAvg Val Loss opp-cc=0.723, AUC: opp-cc=0.452 \n",
      "\tAvg Val Loss: opp-mlo=0.702, AUC: opp-mlo=0.428\n",
      "Iter=480, avg train loss=0.526, \n",
      "\tAvg Val Loss: same-cc=0.725, AUC: same-cc=0.550 \n",
      "\tAvg Val Loss: same-mlo=0.665, AUC: same-mlo=0.518 \n",
      "\tAvg Val Loss opp-cc=0.745, AUC: opp-cc=0.472 \n",
      "\tAvg Val Loss: opp-mlo=0.700, AUC: opp-mlo=0.437\n",
      "Iter=485, avg train loss=0.550, \n",
      "\tAvg Val Loss: same-cc=0.669, AUC: same-cc=0.561 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.522 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.466 \n",
      "\tAvg Val Loss: opp-mlo=0.664, AUC: opp-mlo=0.442\n",
      "Iter=490, avg train loss=0.506, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.568 \n",
      "\tAvg Val Loss: same-mlo=0.653, AUC: same-mlo=0.524 \n",
      "\tAvg Val Loss opp-cc=0.681, AUC: opp-cc=0.456 \n",
      "\tAvg Val Loss: opp-mlo=0.658, AUC: opp-mlo=0.439\n",
      "Iter=495, avg train loss=0.510, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.562 \n",
      "\tAvg Val Loss: same-mlo=0.645, AUC: same-mlo=0.518 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.471 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=500, avg train loss=0.497, \n",
      "\tAvg Val Loss: same-cc=0.646, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.629, AUC: same-mlo=0.525 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.471 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.463\n",
      "Iter=505, avg train loss=0.560, \n",
      "\tAvg Val Loss: same-cc=0.720, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.535 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.691, AUC: opp-mlo=0.480\n",
      "Iter=510, avg train loss=0.463, \n",
      "\tAvg Val Loss: same-cc=0.784, AUC: same-cc=0.539 \n",
      "\tAvg Val Loss: same-mlo=0.644, AUC: same-mlo=0.532 \n",
      "\tAvg Val Loss opp-cc=0.701, AUC: opp-cc=0.498 \n",
      "\tAvg Val Loss: opp-mlo=0.752, AUC: opp-mlo=0.449\n",
      "Best opp-cc model saved.\n",
      "Iter=515, avg train loss=0.478, \n",
      "\tAvg Val Loss: same-cc=0.643, AUC: same-cc=0.567 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.513 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.478 \n",
      "\tAvg Val Loss: opp-mlo=0.692, AUC: opp-mlo=0.466\n",
      "Iter=520, avg train loss=0.590, \n",
      "\tAvg Val Loss: same-cc=0.606, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.630, AUC: same-mlo=0.522 \n",
      "\tAvg Val Loss opp-cc=0.701, AUC: opp-cc=0.482 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.488\n",
      "Iter=525, avg train loss=0.533, \n",
      "\tAvg Val Loss: same-cc=0.603, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.631, AUC: same-mlo=0.517 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.500\n",
      "Iter=530, avg train loss=0.448, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.522 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.619, AUC: opp-mlo=0.482\n",
      "Iter=535, avg train loss=0.557, \n",
      "\tAvg Val Loss: same-cc=0.657, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.664, AUC: same-mlo=0.510 \n",
      "\tAvg Val Loss opp-cc=0.713, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.671, AUC: opp-mlo=0.457\n",
      "Iter=540, avg train loss=0.467, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.530 \n",
      "\tAvg Val Loss: same-mlo=0.684, AUC: same-mlo=0.503 \n",
      "\tAvg Val Loss opp-cc=0.749, AUC: opp-cc=0.463 \n",
      "\tAvg Val Loss: opp-mlo=0.716, AUC: opp-mlo=0.425\n",
      "Iter=545, avg train loss=0.540, \n",
      "\tAvg Val Loss: same-cc=0.698, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.512 \n",
      "\tAvg Val Loss opp-cc=0.718, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.713, AUC: opp-mlo=0.440\n",
      "Iter=550, avg train loss=0.538, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.689, AUC: same-mlo=0.522 \n",
      "\tAvg Val Loss opp-cc=0.748, AUC: opp-cc=0.478 \n",
      "\tAvg Val Loss: opp-mlo=0.787, AUC: opp-mlo=0.409\n",
      "Iter=555, avg train loss=0.515, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.516 \n",
      "\tAvg Val Loss opp-cc=0.717, AUC: opp-cc=0.462 \n",
      "\tAvg Val Loss: opp-mlo=0.744, AUC: opp-mlo=0.434\n",
      "Iter=560, avg train loss=0.536, \n",
      "\tAvg Val Loss: same-cc=0.627, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.513 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.458 \n",
      "\tAvg Val Loss: opp-mlo=0.718, AUC: opp-mlo=0.429\n",
      "Iter=565, avg train loss=0.496, \n",
      "\tAvg Val Loss: same-cc=0.672, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.712, AUC: same-mlo=0.517 \n",
      "\tAvg Val Loss opp-cc=0.740, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.739, AUC: opp-mlo=0.419\n",
      "Iter=570, avg train loss=0.545, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.535 \n",
      "\tAvg Val Loss: same-mlo=0.749, AUC: same-mlo=0.507 \n",
      "\tAvg Val Loss opp-cc=0.753, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.792, AUC: opp-mlo=0.418\n",
      "Iter=575, avg train loss=0.500, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.747, AUC: same-mlo=0.507 \n",
      "\tAvg Val Loss opp-cc=0.701, AUC: opp-cc=0.474 \n",
      "\tAvg Val Loss: opp-mlo=0.741, AUC: opp-mlo=0.425\n",
      "Iter=580, avg train loss=0.458, \n",
      "\tAvg Val Loss: same-cc=0.636, AUC: same-cc=0.516 \n",
      "\tAvg Val Loss: same-mlo=0.698, AUC: same-mlo=0.516 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.468 \n",
      "\tAvg Val Loss: opp-mlo=0.669, AUC: opp-mlo=0.423\n",
      "Iter=585, avg train loss=0.420, \n",
      "\tAvg Val Loss: same-cc=0.626, AUC: same-cc=0.526 \n",
      "\tAvg Val Loss: same-mlo=0.671, AUC: same-mlo=0.521 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.470 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.422\n",
      "Iter=590, avg train loss=0.430, \n",
      "\tAvg Val Loss: same-cc=0.614, AUC: same-cc=0.541 \n",
      "\tAvg Val Loss: same-mlo=0.654, AUC: same-mlo=0.530 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.436\n",
      "Iter=595, avg train loss=0.589, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.537 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.527 \n",
      "\tAvg Val Loss opp-cc=0.689, AUC: opp-cc=0.480 \n",
      "\tAvg Val Loss: opp-mlo=0.694, AUC: opp-mlo=0.440\n",
      "Iter=600, avg train loss=0.384, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.517 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.714, AUC: opp-mlo=0.460\n",
      "Iter=605, avg train loss=0.502, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.537 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.520 \n",
      "\tAvg Val Loss opp-cc=0.703, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.680, AUC: opp-mlo=0.466\n",
      "Iter=610, avg train loss=0.528, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.527 \n",
      "\tAvg Val Loss opp-cc=0.780, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.744, AUC: opp-mlo=0.494\n",
      "Best opp-cc model saved.\n",
      "Iter=615, avg train loss=0.465, \n",
      "\tAvg Val Loss: same-cc=0.847, AUC: same-cc=0.513 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.537 \n",
      "\tAvg Val Loss opp-cc=0.805, AUC: opp-cc=0.510 \n",
      "\tAvg Val Loss: opp-mlo=0.876, AUC: opp-mlo=0.484\n",
      "Iter=620, avg train loss=0.517, \n",
      "\tAvg Val Loss: same-cc=0.819, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.678, AUC: same-mlo=0.530 \n",
      "\tAvg Val Loss opp-cc=0.768, AUC: opp-cc=0.501 \n",
      "\tAvg Val Loss: opp-mlo=0.929, AUC: opp-mlo=0.460\n",
      "Iter=625, avg train loss=0.579, \n",
      "\tAvg Val Loss: same-cc=0.663, AUC: same-cc=0.508 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.538 \n",
      "\tAvg Val Loss opp-cc=0.713, AUC: opp-cc=0.463 \n",
      "\tAvg Val Loss: opp-mlo=0.841, AUC: opp-mlo=0.451\n",
      "Iter=630, avg train loss=0.454, \n",
      "\tAvg Val Loss: same-cc=0.615, AUC: same-cc=0.532 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.564 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.445 \n",
      "\tAvg Val Loss: opp-mlo=0.717, AUC: opp-mlo=0.453\n",
      "Iter=635, avg train loss=0.468, \n",
      "\tAvg Val Loss: same-cc=0.597, AUC: same-cc=0.553 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.577 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.468 \n",
      "\tAvg Val Loss: opp-mlo=0.667, AUC: opp-mlo=0.419\n",
      "Best same-mlo model saved.\n",
      "Iter=640, avg train loss=0.481, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.538 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.579 \n",
      "\tAvg Val Loss opp-cc=0.707, AUC: opp-cc=0.498 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.414\n",
      "Best same-mlo model saved.\n",
      "Iter=645, avg train loss=0.496, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.570 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.518 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.396\n",
      "Best opp-cc model saved.\n",
      "Iter=650, avg train loss=0.443, \n",
      "\tAvg Val Loss: same-cc=0.703, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.556 \n",
      "\tAvg Val Loss opp-cc=0.737, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.801, AUC: opp-mlo=0.382\n",
      "Best opp-cc model saved.\n",
      "Iter=655, avg train loss=0.511, \n",
      "\tAvg Val Loss: same-cc=0.828, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.558 \n",
      "\tAvg Val Loss opp-cc=0.782, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.985, AUC: opp-mlo=0.387\n",
      "Iter=660, avg train loss=0.453, \n",
      "\tAvg Val Loss: same-cc=0.667, AUC: same-cc=0.534 \n",
      "\tAvg Val Loss: same-mlo=0.749, AUC: same-mlo=0.531 \n",
      "\tAvg Val Loss opp-cc=0.747, AUC: opp-cc=0.510 \n",
      "\tAvg Val Loss: opp-mlo=0.858, AUC: opp-mlo=0.398\n",
      "Iter=665, avg train loss=0.456, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.537 \n",
      "\tAvg Val Loss: same-mlo=0.737, AUC: same-mlo=0.508 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.504 \n",
      "\tAvg Val Loss: opp-mlo=0.699, AUC: opp-mlo=0.403\n",
      "Iter=670, avg train loss=0.427, \n",
      "\tAvg Val Loss: same-cc=0.640, AUC: same-cc=0.512 \n",
      "\tAvg Val Loss: same-mlo=0.673, AUC: same-mlo=0.514 \n",
      "\tAvg Val Loss opp-cc=0.708, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=675, avg train loss=0.612, \n",
      "\tAvg Val Loss: same-cc=0.683, AUC: same-cc=0.518 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.524 \n",
      "\tAvg Val Loss opp-cc=0.719, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.662, AUC: opp-mlo=0.423\n",
      "Iter=680, avg train loss=0.417, \n",
      "\tAvg Val Loss: same-cc=0.885, AUC: same-cc=0.507 \n",
      "\tAvg Val Loss: same-mlo=0.663, AUC: same-mlo=0.522 \n",
      "\tAvg Val Loss opp-cc=0.756, AUC: opp-cc=0.497 \n",
      "\tAvg Val Loss: opp-mlo=0.718, AUC: opp-mlo=0.454\n",
      "Iter=685, avg train loss=0.452, \n",
      "\tAvg Val Loss: same-cc=0.754, AUC: same-cc=0.507 \n",
      "\tAvg Val Loss: same-mlo=0.670, AUC: same-mlo=0.524 \n",
      "\tAvg Val Loss opp-cc=0.783, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.730, AUC: opp-mlo=0.454\n",
      "Iter=690, avg train loss=0.460, \n",
      "\tAvg Val Loss: same-cc=0.674, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.663, AUC: same-mlo=0.521 \n",
      "\tAvg Val Loss opp-cc=0.711, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.459\n",
      "Iter=695, avg train loss=0.428, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.667, AUC: same-mlo=0.532 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.646, AUC: opp-mlo=0.466\n",
      "Best opp-cc model saved.\n",
      "Iter=700, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.744, AUC: same-cc=0.513 \n",
      "\tAvg Val Loss: same-mlo=0.666, AUC: same-mlo=0.528 \n",
      "\tAvg Val Loss opp-cc=0.715, AUC: opp-cc=0.535 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.471\n",
      "Best opp-cc model saved.\n",
      "Iter=705, avg train loss=0.397, \n",
      "\tAvg Val Loss: same-cc=0.806, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.702, AUC: same-mlo=0.524 \n",
      "\tAvg Val Loss opp-cc=0.856, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.742, AUC: opp-mlo=0.497\n",
      "Iter=710, avg train loss=0.557, \n",
      "\tAvg Val Loss: same-cc=0.722, AUC: same-cc=0.532 \n",
      "\tAvg Val Loss: same-mlo=0.754, AUC: same-mlo=0.524 \n",
      "\tAvg Val Loss opp-cc=0.918, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.757, AUC: opp-mlo=0.491\n",
      "Iter=715, avg train loss=0.489, \n",
      "\tAvg Val Loss: same-cc=0.649, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.672, AUC: same-mlo=0.530 \n",
      "\tAvg Val Loss opp-cc=0.741, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.479\n",
      "Iter=720, avg train loss=0.481, \n",
      "\tAvg Val Loss: same-cc=0.668, AUC: same-cc=0.497 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.522 \n",
      "\tAvg Val Loss opp-cc=0.701, AUC: opp-cc=0.494 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.437\n",
      "Iter=725, avg train loss=0.399, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.507 \n",
      "\tAvg Val Loss: same-mlo=0.655, AUC: same-mlo=0.529 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.687, AUC: opp-mlo=0.426\n",
      "Iter=730, avg train loss=0.393, \n",
      "\tAvg Val Loss: same-cc=0.711, AUC: same-cc=0.508 \n",
      "\tAvg Val Loss: same-mlo=0.657, AUC: same-mlo=0.526 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.705, AUC: opp-mlo=0.434\n",
      "Iter=735, avg train loss=0.494, \n",
      "\tAvg Val Loss: same-cc=0.873, AUC: same-cc=0.507 \n",
      "\tAvg Val Loss: same-mlo=0.689, AUC: same-mlo=0.521 \n",
      "\tAvg Val Loss opp-cc=0.730, AUC: opp-cc=0.500 \n",
      "\tAvg Val Loss: opp-mlo=0.852, AUC: opp-mlo=0.419\n",
      "Iter=740, avg train loss=0.371, \n",
      "\tAvg Val Loss: same-cc=0.798, AUC: same-cc=0.517 \n",
      "\tAvg Val Loss: same-mlo=0.738, AUC: same-mlo=0.516 \n",
      "\tAvg Val Loss opp-cc=0.742, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.877, AUC: opp-mlo=0.435\n",
      "Iter=745, avg train loss=0.445, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.526 \n",
      "\tAvg Val Loss: same-mlo=0.724, AUC: same-mlo=0.514 \n",
      "\tAvg Val Loss opp-cc=0.719, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.800, AUC: opp-mlo=0.465\n",
      "Best opp-cc model saved.\n",
      "Iter=750, avg train loss=0.382, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.535 \n",
      "\tAvg Val Loss: same-mlo=0.670, AUC: same-mlo=0.532 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.715, AUC: opp-mlo=0.471\n",
      "Best opp-cc model saved.\n",
      "Iter=755, avg train loss=0.487, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.636, AUC: same-mlo=0.541 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.512 \n",
      "\tAvg Val Loss: opp-mlo=0.658, AUC: opp-mlo=0.475\n",
      "Iter=760, avg train loss=0.409, \n",
      "\tAvg Val Loss: same-cc=0.676, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.544 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.683, AUC: opp-mlo=0.473\n",
      "Best opp-cc model saved.\n",
      "Iter=765, avg train loss=0.472, \n",
      "\tAvg Val Loss: same-cc=0.851, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.683, AUC: same-mlo=0.531 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.599 \n",
      "\tAvg Val Loss: opp-mlo=0.793, AUC: opp-mlo=0.474\n",
      "Best opp-cc model saved.\n",
      "Iter=770, avg train loss=0.394, \n",
      "\tAvg Val Loss: same-cc=0.712, AUC: same-cc=0.535 \n",
      "\tAvg Val Loss: same-mlo=0.704, AUC: same-mlo=0.540 \n",
      "\tAvg Val Loss opp-cc=0.787, AUC: opp-cc=0.599 \n",
      "\tAvg Val Loss: opp-mlo=0.732, AUC: opp-mlo=0.479\n",
      "Iter=775, avg train loss=0.398, \n",
      "\tAvg Val Loss: same-cc=0.637, AUC: same-cc=0.535 \n",
      "\tAvg Val Loss: same-mlo=0.746, AUC: same-mlo=0.532 \n",
      "\tAvg Val Loss opp-cc=0.812, AUC: opp-cc=0.556 \n",
      "\tAvg Val Loss: opp-mlo=0.713, AUC: opp-mlo=0.475\n",
      "Iter=780, avg train loss=0.413, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.699, AUC: same-mlo=0.547 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.543 \n",
      "\tAvg Val Loss: opp-mlo=0.663, AUC: opp-mlo=0.471\n",
      "Iter=785, avg train loss=0.451, \n",
      "\tAvg Val Loss: same-cc=0.646, AUC: same-cc=0.545 \n",
      "\tAvg Val Loss: same-mlo=0.699, AUC: same-mlo=0.534 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.488 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.460\n",
      "Iter=790, avg train loss=0.350, \n",
      "\tAvg Val Loss: same-cc=0.661, AUC: same-cc=0.544 \n",
      "\tAvg Val Loss: same-mlo=0.686, AUC: same-mlo=0.539 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.454\n",
      "Iter=795, avg train loss=0.330, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.547 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.725, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.682, AUC: opp-mlo=0.429\n",
      "Iter=800, avg train loss=0.406, \n",
      "\tAvg Val Loss: same-cc=0.720, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.729, AUC: same-mlo=0.541 \n",
      "\tAvg Val Loss opp-cc=0.761, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.720, AUC: opp-mlo=0.447\n",
      "Iter=805, avg train loss=0.453, \n",
      "\tAvg Val Loss: same-cc=0.944, AUC: same-cc=0.523 \n",
      "\tAvg Val Loss: same-mlo=0.811, AUC: same-mlo=0.532 \n",
      "\tAvg Val Loss opp-cc=0.894, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.842, AUC: opp-mlo=0.452\n",
      "Iter=810, avg train loss=0.446, \n",
      "\tAvg Val Loss: same-cc=0.712, AUC: same-cc=0.506 \n",
      "\tAvg Val Loss: same-mlo=0.789, AUC: same-mlo=0.529 \n",
      "\tAvg Val Loss opp-cc=0.831, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.813, AUC: opp-mlo=0.451\n",
      "Iter=815, avg train loss=0.361, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.499 \n",
      "\tAvg Val Loss: same-mlo=0.688, AUC: same-mlo=0.531 \n",
      "\tAvg Val Loss opp-cc=0.712, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.735, AUC: opp-mlo=0.438\n",
      "Iter=820, avg train loss=0.393, \n",
      "\tAvg Val Loss: same-cc=0.650, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.685, AUC: same-mlo=0.515 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.744, AUC: opp-mlo=0.426\n",
      "Iter=825, avg train loss=0.397, \n",
      "\tAvg Val Loss: same-cc=0.704, AUC: same-cc=0.524 \n",
      "\tAvg Val Loss: same-mlo=0.688, AUC: same-mlo=0.516 \n",
      "\tAvg Val Loss opp-cc=0.681, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.760, AUC: opp-mlo=0.442\n",
      "Iter=830, avg train loss=0.278, \n",
      "\tAvg Val Loss: same-cc=0.772, AUC: same-cc=0.506 \n",
      "\tAvg Val Loss: same-mlo=0.689, AUC: same-mlo=0.497 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.567 \n",
      "\tAvg Val Loss: opp-mlo=0.832, AUC: opp-mlo=0.435\n",
      "Iter=835, avg train loss=0.396, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.494 \n",
      "\tAvg Val Loss: same-mlo=0.724, AUC: same-mlo=0.503 \n",
      "\tAvg Val Loss opp-cc=0.707, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.829, AUC: opp-mlo=0.431\n",
      "Iter=840, avg train loss=0.320, \n",
      "\tAvg Val Loss: same-cc=0.657, AUC: same-cc=0.487 \n",
      "\tAvg Val Loss: same-mlo=0.736, AUC: same-mlo=0.491 \n",
      "\tAvg Val Loss opp-cc=0.696, AUC: opp-cc=0.574 \n",
      "\tAvg Val Loss: opp-mlo=0.756, AUC: opp-mlo=0.448\n",
      "Iter=845, avg train loss=0.245, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.496 \n",
      "\tAvg Val Loss: same-mlo=0.740, AUC: same-mlo=0.482 \n",
      "\tAvg Val Loss opp-cc=0.724, AUC: opp-cc=0.569 \n",
      "\tAvg Val Loss: opp-mlo=0.738, AUC: opp-mlo=0.453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=850, avg train loss=0.450, \n",
      "\tAvg Val Loss: same-cc=0.702, AUC: same-cc=0.494 \n",
      "\tAvg Val Loss: same-mlo=0.756, AUC: same-mlo=0.490 \n",
      "\tAvg Val Loss opp-cc=0.726, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.774, AUC: opp-mlo=0.464\n",
      "Iter=855, avg train loss=0.322, \n",
      "\tAvg Val Loss: same-cc=0.803, AUC: same-cc=0.500 \n",
      "\tAvg Val Loss: same-mlo=0.737, AUC: same-mlo=0.488 \n",
      "\tAvg Val Loss opp-cc=0.704, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.725, AUC: opp-mlo=0.451\n",
      "Iter=860, avg train loss=0.409, \n",
      "\tAvg Val Loss: same-cc=0.821, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.710, AUC: same-mlo=0.496 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.731, AUC: opp-mlo=0.443\n",
      "Iter=865, avg train loss=0.428, \n",
      "\tAvg Val Loss: same-cc=0.743, AUC: same-cc=0.530 \n",
      "\tAvg Val Loss: same-mlo=0.704, AUC: same-mlo=0.513 \n",
      "\tAvg Val Loss opp-cc=0.719, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.741, AUC: opp-mlo=0.450\n",
      "Iter=870, avg train loss=0.352, \n",
      "\tAvg Val Loss: same-cc=0.641, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.713, AUC: same-mlo=0.520 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.574 \n",
      "\tAvg Val Loss: opp-mlo=0.693, AUC: opp-mlo=0.436\n",
      "Iter=875, avg train loss=0.384, \n",
      "\tAvg Val Loss: same-cc=0.632, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.726, AUC: same-mlo=0.518 \n",
      "\tAvg Val Loss opp-cc=0.667, AUC: opp-cc=0.578 \n",
      "\tAvg Val Loss: opp-mlo=0.702, AUC: opp-mlo=0.445\n",
      "Iter=880, avg train loss=0.425, \n",
      "\tAvg Val Loss: same-cc=0.930, AUC: same-cc=0.556 \n",
      "\tAvg Val Loss: same-mlo=0.816, AUC: same-mlo=0.519 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.567 \n",
      "\tAvg Val Loss: opp-mlo=0.864, AUC: opp-mlo=0.434\n",
      "Iter=885, avg train loss=0.370, \n",
      "\tAvg Val Loss: same-cc=0.728, AUC: same-cc=0.566 \n",
      "\tAvg Val Loss: same-mlo=0.798, AUC: same-mlo=0.525 \n",
      "\tAvg Val Loss opp-cc=0.702, AUC: opp-cc=0.574 \n",
      "\tAvg Val Loss: opp-mlo=0.771, AUC: opp-mlo=0.428\n",
      "Iter=890, avg train loss=0.375, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.559 \n",
      "\tAvg Val Loss: same-mlo=0.715, AUC: same-mlo=0.508 \n",
      "\tAvg Val Loss opp-cc=0.731, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.658, AUC: opp-mlo=0.439\n",
      "Iter=895, avg train loss=0.273, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.665, AUC: same-mlo=0.513 \n",
      "\tAvg Val Loss opp-cc=0.739, AUC: opp-cc=0.546 \n",
      "\tAvg Val Loss: opp-mlo=0.663, AUC: opp-mlo=0.422\n",
      "Iter=900, avg train loss=0.491, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.690, AUC: same-mlo=0.528 \n",
      "\tAvg Val Loss opp-cc=0.711, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.656, AUC: opp-mlo=0.459\n",
      "Iter=905, avg train loss=0.385, \n",
      "\tAvg Val Loss: same-cc=0.844, AUC: same-cc=0.556 \n",
      "\tAvg Val Loss: same-mlo=0.725, AUC: same-mlo=0.519 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.713, AUC: opp-mlo=0.463\n",
      "Iter=910, avg train loss=0.356, \n",
      "\tAvg Val Loss: same-cc=0.813, AUC: same-cc=0.559 \n",
      "\tAvg Val Loss: same-mlo=0.779, AUC: same-mlo=0.525 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.838, AUC: opp-mlo=0.496\n",
      "Iter=915, avg train loss=0.381, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.561 \n",
      "\tAvg Val Loss: same-mlo=0.699, AUC: same-mlo=0.534 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.750, AUC: opp-mlo=0.505\n",
      "Iter=920, avg train loss=0.304, \n",
      "\tAvg Val Loss: same-cc=0.646, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.671, AUC: same-mlo=0.540 \n",
      "\tAvg Val Loss opp-cc=0.730, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.664, AUC: opp-mlo=0.486\n",
      "Iter=925, avg train loss=0.275, \n",
      "\tAvg Val Loss: same-cc=0.632, AUC: same-cc=0.550 \n",
      "\tAvg Val Loss: same-mlo=0.673, AUC: same-mlo=0.550 \n",
      "\tAvg Val Loss opp-cc=0.782, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.694, AUC: opp-mlo=0.497\n",
      "Iter=930, avg train loss=0.346, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.689, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.727, AUC: opp-mlo=0.487\n",
      "Iter=935, avg train loss=0.294, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.711, AUC: same-mlo=0.553 \n",
      "\tAvg Val Loss opp-cc=0.707, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.727, AUC: opp-mlo=0.523\n",
      "Best opp-mlo model saved.\n",
      "Iter=940, avg train loss=0.324, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.534 \n",
      "\tAvg Val Loss: same-mlo=0.722, AUC: same-mlo=0.553 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.731, AUC: opp-mlo=0.515\n",
      "Iter=945, avg train loss=0.451, \n",
      "\tAvg Val Loss: same-cc=0.722, AUC: same-cc=0.520 \n",
      "\tAvg Val Loss: same-mlo=0.753, AUC: same-mlo=0.531 \n",
      "\tAvg Val Loss opp-cc=0.773, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.802, AUC: opp-mlo=0.527\n",
      "Best opp-mlo model saved.\n",
      "Iter=950, avg train loss=0.345, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.516 \n",
      "\tAvg Val Loss: same-mlo=0.730, AUC: same-mlo=0.541 \n",
      "\tAvg Val Loss opp-cc=0.773, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.780, AUC: opp-mlo=0.508\n",
      "Iter=955, avg train loss=0.370, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.510 \n",
      "\tAvg Val Loss: same-mlo=0.687, AUC: same-mlo=0.563 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.481\n",
      "Iter=960, avg train loss=0.241, \n",
      "\tAvg Val Loss: same-cc=0.681, AUC: same-cc=0.517 \n",
      "\tAvg Val Loss: same-mlo=0.681, AUC: same-mlo=0.582 \n",
      "\tAvg Val Loss opp-cc=0.710, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.444\n",
      "Best same-mlo model saved.\n",
      "Iter=965, avg train loss=0.396, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.503 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.578 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.440\n",
      "Iter=970, avg train loss=0.317, \n",
      "\tAvg Val Loss: same-cc=0.727, AUC: same-cc=0.493 \n",
      "\tAvg Val Loss: same-mlo=0.693, AUC: same-mlo=0.575 \n",
      "\tAvg Val Loss opp-cc=0.740, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.772, AUC: opp-mlo=0.435\n",
      "Iter=975, avg train loss=0.338, \n",
      "\tAvg Val Loss: same-cc=0.953, AUC: same-cc=0.514 \n",
      "\tAvg Val Loss: same-mlo=0.723, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.842, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.892, AUC: opp-mlo=0.452\n",
      "Iter=980, avg train loss=0.314, \n",
      "\tAvg Val Loss: same-cc=0.783, AUC: same-cc=0.520 \n",
      "\tAvg Val Loss: same-mlo=0.739, AUC: same-mlo=0.558 \n",
      "\tAvg Val Loss opp-cc=0.809, AUC: opp-cc=0.550 \n",
      "\tAvg Val Loss: opp-mlo=0.875, AUC: opp-mlo=0.460\n",
      "Iter=985, avg train loss=0.283, \n",
      "\tAvg Val Loss: same-cc=0.650, AUC: same-cc=0.500 \n",
      "\tAvg Val Loss: same-mlo=0.692, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.756, AUC: opp-cc=0.532 \n",
      "\tAvg Val Loss: opp-mlo=0.731, AUC: opp-mlo=0.461\n",
      "Iter=990, avg train loss=0.277, \n",
      "\tAvg Val Loss: same-cc=0.649, AUC: same-cc=0.491 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.546 \n",
      "\tAvg Val Loss opp-cc=0.735, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.687, AUC: opp-mlo=0.447\n",
      "Iter=995, avg train loss=0.363, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.491 \n",
      "\tAvg Val Loss: same-mlo=0.738, AUC: same-mlo=0.536 \n",
      "\tAvg Val Loss opp-cc=0.739, AUC: opp-cc=0.501 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.445\n",
      "Iter=1000, avg train loss=0.197, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.487 \n",
      "\tAvg Val Loss: same-mlo=0.712, AUC: same-mlo=0.550 \n",
      "\tAvg Val Loss opp-cc=0.771, AUC: opp-cc=0.498 \n",
      "\tAvg Val Loss: opp-mlo=0.689, AUC: opp-mlo=0.443\n",
      "Iter=1005, avg train loss=0.327, \n",
      "\tAvg Val Loss: same-cc=0.700, AUC: same-cc=0.482 \n",
      "\tAvg Val Loss: same-mlo=0.714, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.736, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.698, AUC: opp-mlo=0.471\n",
      "Iter=1010, avg train loss=0.320, \n",
      "\tAvg Val Loss: same-cc=0.734, AUC: same-cc=0.479 \n",
      "\tAvg Val Loss: same-mlo=0.744, AUC: same-mlo=0.551 \n",
      "\tAvg Val Loss opp-cc=0.728, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.710, AUC: opp-mlo=0.471\n",
      "Iter=1015, avg train loss=0.246, \n",
      "\tAvg Val Loss: same-cc=0.726, AUC: same-cc=0.493 \n",
      "\tAvg Val Loss: same-mlo=0.757, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.738, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.725, AUC: opp-mlo=0.485\n",
      "Iter=1020, avg train loss=0.368, \n",
      "\tAvg Val Loss: same-cc=0.790, AUC: same-cc=0.485 \n",
      "\tAvg Val Loss: same-mlo=0.738, AUC: same-mlo=0.558 \n",
      "\tAvg Val Loss opp-cc=0.784, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.841, AUC: opp-mlo=0.480\n",
      "Iter=1025, avg train loss=0.331, \n",
      "\tAvg Val Loss: same-cc=0.778, AUC: same-cc=0.514 \n",
      "\tAvg Val Loss: same-mlo=0.724, AUC: same-mlo=0.534 \n",
      "\tAvg Val Loss opp-cc=0.748, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.836, AUC: opp-mlo=0.474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1030, avg train loss=0.308, \n",
      "\tAvg Val Loss: same-cc=0.736, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.712, AUC: same-mlo=0.542 \n",
      "\tAvg Val Loss opp-cc=0.719, AUC: opp-cc=0.499 \n",
      "\tAvg Val Loss: opp-mlo=0.815, AUC: opp-mlo=0.455\n",
      "Iter=1035, avg train loss=0.324, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.494 \n",
      "\tAvg Val Loss: same-mlo=0.705, AUC: same-mlo=0.544 \n",
      "\tAvg Val Loss opp-cc=0.713, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.755, AUC: opp-mlo=0.433\n",
      "Iter=1040, avg train loss=0.259, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.475 \n",
      "\tAvg Val Loss: same-mlo=0.681, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.702, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.715, AUC: opp-mlo=0.430\n",
      "Best models loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training: same-cc=0.639, same-mlo=0.607, opp-cc=0.508, opp-mlo=0.566\n",
      "Max-Score Based AUC After Training: 0.534, Mean-Score Based AUC After Training: 0.612\n",
      "\n",
      "\n",
      "\n",
      "========== Fold 2 ==========\n",
      "Test AUC at start: same-cc=0.415, same-mlo=0.490, opp-cc=0.412, opp-mlo=0.399\n",
      "Max-Score Based AUC Before Training: 0.393, Mean-Score Based AUC Before Training: 0.418\n",
      "Iter=5, avg train loss=1.100, \n",
      "\tAvg Val Loss: same-cc=0.572, AUC: same-cc=0.545 \n",
      "\tAvg Val Loss: same-mlo=0.575, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.575, AUC: opp-cc=0.550 \n",
      "\tAvg Val Loss: opp-mlo=0.592, AUC: opp-mlo=0.491\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=10, avg train loss=0.709, \n",
      "\tAvg Val Loss: same-cc=0.593, AUC: same-cc=0.522 \n",
      "\tAvg Val Loss: same-mlo=0.576, AUC: same-mlo=0.565 \n",
      "\tAvg Val Loss opp-cc=0.585, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.593, AUC: opp-mlo=0.501\n",
      "Best opp-mlo model saved.\n",
      "Iter=15, avg train loss=0.996, \n",
      "\tAvg Val Loss: same-cc=0.591, AUC: same-cc=0.518 \n",
      "\tAvg Val Loss: same-mlo=0.575, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.584, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.596, AUC: opp-mlo=0.510\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=20, avg train loss=1.145, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.494 \n",
      "\tAvg Val Loss: same-mlo=0.578, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.591, AUC: opp-cc=0.580 \n",
      "\tAvg Val Loss: opp-mlo=0.598, AUC: opp-mlo=0.523\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=25, avg train loss=0.744, \n",
      "\tAvg Val Loss: same-cc=0.605, AUC: same-cc=0.530 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.599 \n",
      "\tAvg Val Loss opp-cc=0.613, AUC: opp-cc=0.610 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.539\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=30, avg train loss=0.988, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.605, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.591, AUC: opp-cc=0.596 \n",
      "\tAvg Val Loss: opp-mlo=0.601, AUC: opp-mlo=0.535\n",
      "Best same-mlo model saved.\n",
      "Iter=35, avg train loss=0.831, \n",
      "\tAvg Val Loss: same-cc=0.616, AUC: same-cc=0.541 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.618, AUC: opp-cc=0.623 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.548\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=40, avg train loss=0.718, \n",
      "\tAvg Val Loss: same-cc=0.602, AUC: same-cc=0.541 \n",
      "\tAvg Val Loss: same-mlo=0.582, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.621, AUC: opp-cc=0.619 \n",
      "\tAvg Val Loss: opp-mlo=0.603, AUC: opp-mlo=0.552\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=45, avg train loss=0.689, \n",
      "\tAvg Val Loss: same-cc=0.605, AUC: same-cc=0.530 \n",
      "\tAvg Val Loss: same-mlo=0.582, AUC: same-mlo=0.650 \n",
      "\tAvg Val Loss opp-cc=0.620, AUC: opp-cc=0.603 \n",
      "\tAvg Val Loss: opp-mlo=0.606, AUC: opp-mlo=0.560\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=50, avg train loss=0.644, \n",
      "\tAvg Val Loss: same-cc=0.606, AUC: same-cc=0.543 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.655 \n",
      "\tAvg Val Loss opp-cc=0.609, AUC: opp-cc=0.591 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.552\n",
      "Best same-mlo model saved.\n",
      "Iter=55, avg train loss=0.707, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.548 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.600, AUC: opp-cc=0.583 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.542\n",
      "Best same-cc model saved.\n",
      "Iter=60, avg train loss=0.723, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.560 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.672 \n",
      "\tAvg Val Loss opp-cc=0.609, AUC: opp-cc=0.597 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.539\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Iter=65, avg train loss=0.704, \n",
      "\tAvg Val Loss: same-cc=0.615, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.679 \n",
      "\tAvg Val Loss opp-cc=0.611, AUC: opp-cc=0.588 \n",
      "\tAvg Val Loss: opp-mlo=0.621, AUC: opp-mlo=0.553\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Iter=70, avg train loss=0.693, \n",
      "\tAvg Val Loss: same-cc=0.621, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.672 \n",
      "\tAvg Val Loss opp-cc=0.601, AUC: opp-cc=0.577 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.561\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=75, avg train loss=0.790, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.602, AUC: same-mlo=0.671 \n",
      "\tAvg Val Loss opp-cc=0.616, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.575\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=80, avg train loss=0.709, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.629, AUC: same-mlo=0.676 \n",
      "\tAvg Val Loss opp-cc=0.628, AUC: opp-cc=0.581 \n",
      "\tAvg Val Loss: opp-mlo=0.621, AUC: opp-mlo=0.574\n",
      "Best same-cc model saved.\n",
      "Iter=85, avg train loss=0.654, \n",
      "\tAvg Val Loss: same-cc=0.662, AUC: same-cc=0.600 \n",
      "\tAvg Val Loss: same-mlo=0.647, AUC: same-mlo=0.669 \n",
      "\tAvg Val Loss opp-cc=0.657, AUC: opp-cc=0.595 \n",
      "\tAvg Val Loss: opp-mlo=0.631, AUC: opp-mlo=0.604\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=90, avg train loss=0.701, \n",
      "\tAvg Val Loss: same-cc=0.671, AUC: same-cc=0.602 \n",
      "\tAvg Val Loss: same-mlo=0.656, AUC: same-mlo=0.660 \n",
      "\tAvg Val Loss opp-cc=0.657, AUC: opp-cc=0.590 \n",
      "\tAvg Val Loss: opp-mlo=0.636, AUC: opp-mlo=0.590\n",
      "Best same-cc model saved.\n",
      "Iter=95, avg train loss=0.687, \n",
      "\tAvg Val Loss: same-cc=0.644, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.653, AUC: same-mlo=0.667 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.588 \n",
      "\tAvg Val Loss: opp-mlo=0.631, AUC: opp-mlo=0.584\n",
      "Best same-cc model saved.\n",
      "Iter=100, avg train loss=0.698, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.655, AUC: same-mlo=0.660 \n",
      "\tAvg Val Loss opp-cc=0.619, AUC: opp-cc=0.592 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.567\n",
      "Best same-cc model saved.\n",
      "Iter=105, avg train loss=0.698, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.595 \n",
      "\tAvg Val Loss: same-mlo=0.658, AUC: same-mlo=0.662 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.588 \n",
      "\tAvg Val Loss: opp-mlo=0.636, AUC: opp-mlo=0.592\n",
      "Iter=110, avg train loss=0.657, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.603 \n",
      "\tAvg Val Loss: same-mlo=0.662, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.593 \n",
      "\tAvg Val Loss: opp-mlo=0.639, AUC: opp-mlo=0.590\n",
      "Iter=115, avg train loss=0.660, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.608 \n",
      "\tAvg Val Loss: same-mlo=0.650, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.583 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.580\n",
      "Iter=120, avg train loss=0.688, \n",
      "\tAvg Val Loss: same-cc=0.651, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.661, AUC: same-mlo=0.649 \n",
      "\tAvg Val Loss opp-cc=0.624, AUC: opp-cc=0.582 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.589\n",
      "Iter=125, avg train loss=0.680, \n",
      "\tAvg Val Loss: same-cc=0.628, AUC: same-cc=0.591 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.660 \n",
      "\tAvg Val Loss opp-cc=0.616, AUC: opp-cc=0.596 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.617\n",
      "Best opp-mlo model saved.\n",
      "Iter=130, avg train loss=0.657, \n",
      "\tAvg Val Loss: same-cc=0.636, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.650 \n",
      "\tAvg Val Loss opp-cc=0.602, AUC: opp-cc=0.582 \n",
      "\tAvg Val Loss: opp-mlo=0.611, AUC: opp-mlo=0.612\n",
      "Iter=135, avg train loss=0.661, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.586 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.664 \n",
      "\tAvg Val Loss opp-cc=0.608, AUC: opp-cc=0.569 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=140, avg train loss=0.639, \n",
      "\tAvg Val Loss: same-cc=0.633, AUC: same-cc=0.587 \n",
      "\tAvg Val Loss: same-mlo=0.636, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.609, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.601\n",
      "Iter=145, avg train loss=0.633, \n",
      "\tAvg Val Loss: same-cc=0.604, AUC: same-cc=0.550 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.591, AUC: opp-cc=0.543 \n",
      "\tAvg Val Loss: opp-mlo=0.601, AUC: opp-mlo=0.606\n",
      "Iter=150, avg train loss=0.720, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.586 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.646 \n",
      "\tAvg Val Loss opp-cc=0.603, AUC: opp-cc=0.537 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.601\n",
      "Iter=155, avg train loss=0.671, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.579 \n",
      "\tAvg Val Loss: same-mlo=0.651, AUC: same-mlo=0.643 \n",
      "\tAvg Val Loss opp-cc=0.611, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.626, AUC: opp-mlo=0.606\n",
      "Iter=160, avg train loss=0.653, \n",
      "\tAvg Val Loss: same-cc=0.602, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.624, AUC: same-mlo=0.645 \n",
      "\tAvg Val Loss opp-cc=0.603, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.608\n",
      "Iter=165, avg train loss=0.624, \n",
      "\tAvg Val Loss: same-cc=0.608, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.610, AUC: opp-cc=0.550 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.596\n",
      "Iter=170, avg train loss=0.615, \n",
      "\tAvg Val Loss: same-cc=0.599, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.591, AUC: opp-cc=0.550 \n",
      "\tAvg Val Loss: opp-mlo=0.584, AUC: opp-mlo=0.573\n",
      "Iter=175, avg train loss=0.663, \n",
      "\tAvg Val Loss: same-cc=0.586, AUC: same-cc=0.566 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.578, AUC: opp-cc=0.534 \n",
      "\tAvg Val Loss: opp-mlo=0.576, AUC: opp-mlo=0.572\n",
      "Iter=180, avg train loss=0.642, \n",
      "\tAvg Val Loss: same-cc=0.599, AUC: same-cc=0.567 \n",
      "\tAvg Val Loss: same-mlo=0.588, AUC: same-mlo=0.621 \n",
      "\tAvg Val Loss opp-cc=0.585, AUC: opp-cc=0.534 \n",
      "\tAvg Val Loss: opp-mlo=0.581, AUC: opp-mlo=0.565\n",
      "Iter=185, avg train loss=0.678, \n",
      "\tAvg Val Loss: same-cc=0.598, AUC: same-cc=0.547 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.621 \n",
      "\tAvg Val Loss opp-cc=0.595, AUC: opp-cc=0.550 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.562\n",
      "Iter=190, avg train loss=0.702, \n",
      "\tAvg Val Loss: same-cc=0.618, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.587, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.626, AUC: opp-cc=0.589 \n",
      "\tAvg Val Loss: opp-mlo=0.592, AUC: opp-mlo=0.579\n",
      "Iter=195, avg train loss=0.770, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.634, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.599 \n",
      "\tAvg Val Loss: opp-mlo=0.635, AUC: opp-mlo=0.592\n",
      "Iter=200, avg train loss=0.658, \n",
      "\tAvg Val Loss: same-cc=0.671, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.700, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.695, AUC: opp-cc=0.614 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.593\n",
      "Iter=205, avg train loss=0.644, \n",
      "\tAvg Val Loss: same-cc=0.671, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.703, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.624 \n",
      "\tAvg Val Loss: opp-mlo=0.646, AUC: opp-mlo=0.596\n",
      "Best opp-cc model saved.\n",
      "Iter=210, avg train loss=0.673, \n",
      "\tAvg Val Loss: same-cc=0.669, AUC: same-cc=0.612 \n",
      "\tAvg Val Loss: same-mlo=0.687, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.611 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.596\n",
      "Best same-cc model saved.\n",
      "Iter=215, avg train loss=0.629, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.691, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.590 \n",
      "\tAvg Val Loss: opp-mlo=0.636, AUC: opp-mlo=0.611\n",
      "Iter=220, avg train loss=0.668, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.704, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.592 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.609\n",
      "Iter=225, avg train loss=0.606, \n",
      "\tAvg Val Loss: same-cc=0.668, AUC: same-cc=0.603 \n",
      "\tAvg Val Loss: same-mlo=0.739, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.581 \n",
      "\tAvg Val Loss: opp-mlo=0.652, AUC: opp-mlo=0.617\n",
      "Iter=230, avg train loss=0.610, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.599 \n",
      "\tAvg Val Loss: same-mlo=0.724, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.586 \n",
      "\tAvg Val Loss: opp-mlo=0.670, AUC: opp-mlo=0.615\n",
      "Iter=235, avg train loss=0.641, \n",
      "\tAvg Val Loss: same-cc=0.662, AUC: same-cc=0.602 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.608 \n",
      "\tAvg Val Loss opp-cc=0.656, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.612\n",
      "Iter=240, avg train loss=0.657, \n",
      "\tAvg Val Loss: same-cc=0.668, AUC: same-cc=0.600 \n",
      "\tAvg Val Loss: same-mlo=0.663, AUC: same-mlo=0.596 \n",
      "\tAvg Val Loss opp-cc=0.664, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.671, AUC: opp-mlo=0.610\n",
      "Iter=245, avg train loss=0.663, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.587 \n",
      "\tAvg Val Loss: same-mlo=0.663, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.658, AUC: opp-mlo=0.602\n",
      "Iter=250, avg train loss=0.674, \n",
      "\tAvg Val Loss: same-cc=0.651, AUC: same-cc=0.599 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.591 \n",
      "\tAvg Val Loss opp-cc=0.612, AUC: opp-cc=0.567 \n",
      "\tAvg Val Loss: opp-mlo=0.655, AUC: opp-mlo=0.600\n",
      "Iter=255, avg train loss=0.602, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.594 \n",
      "\tAvg Val Loss: same-mlo=0.657, AUC: same-mlo=0.596 \n",
      "\tAvg Val Loss opp-cc=0.618, AUC: opp-cc=0.565 \n",
      "\tAvg Val Loss: opp-mlo=0.636, AUC: opp-mlo=0.600\n",
      "Iter=260, avg train loss=0.628, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.631, AUC: same-mlo=0.592 \n",
      "\tAvg Val Loss opp-cc=0.609, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.589\n",
      "Iter=265, avg train loss=0.634, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.591 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.629, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.603, AUC: opp-mlo=0.580\n",
      "Iter=270, avg train loss=0.615, \n",
      "\tAvg Val Loss: same-cc=0.626, AUC: same-cc=0.577 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.607 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.577\n",
      "Iter=275, avg train loss=0.676, \n",
      "\tAvg Val Loss: same-cc=0.633, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=0.631, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.569 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.581\n",
      "Iter=280, avg train loss=0.672, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.586 \n",
      "\tAvg Val Loss: same-mlo=0.634, AUC: same-mlo=0.607 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.567 \n",
      "\tAvg Val Loss: opp-mlo=0.645, AUC: opp-mlo=0.573\n",
      "Iter=285, avg train loss=0.659, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.614 \n",
      "\tAvg Val Loss: same-mlo=0.671, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.688, AUC: opp-mlo=0.570\n",
      "Best same-cc model saved.\n",
      "Iter=290, avg train loss=0.671, \n",
      "\tAvg Val Loss: same-cc=0.706, AUC: same-cc=0.619 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.565 \n",
      "\tAvg Val Loss: opp-mlo=0.690, AUC: opp-mlo=0.568\n",
      "Best same-cc model saved.\n",
      "Iter=295, avg train loss=0.613, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.602 \n",
      "\tAvg Val Loss: same-mlo=0.689, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.551 \n",
      "\tAvg Val Loss: opp-mlo=0.667, AUC: opp-mlo=0.571\n",
      "Iter=300, avg train loss=0.626, \n",
      "\tAvg Val Loss: same-cc=0.731, AUC: same-cc=0.594 \n",
      "\tAvg Val Loss: same-mlo=0.710, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.678, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.575\n",
      "Iter=305, avg train loss=0.676, \n",
      "\tAvg Val Loss: same-cc=0.727, AUC: same-cc=0.594 \n",
      "\tAvg Val Loss: same-mlo=0.713, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.581\n",
      "Iter=310, avg train loss=0.640, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.609 \n",
      "\tAvg Val Loss: same-mlo=0.685, AUC: same-mlo=0.596 \n",
      "\tAvg Val Loss opp-cc=0.707, AUC: opp-cc=0.564 \n",
      "\tAvg Val Loss: opp-mlo=0.658, AUC: opp-mlo=0.578\n",
      "Iter=315, avg train loss=0.626, \n",
      "\tAvg Val Loss: same-cc=0.682, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.685, AUC: same-mlo=0.596 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=320, avg train loss=0.600, \n",
      "\tAvg Val Loss: same-cc=0.661, AUC: same-cc=0.614 \n",
      "\tAvg Val Loss: same-mlo=0.645, AUC: same-mlo=0.585 \n",
      "\tAvg Val Loss opp-cc=0.638, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.575\n",
      "Iter=325, avg train loss=0.584, \n",
      "\tAvg Val Loss: same-cc=0.632, AUC: same-cc=0.607 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.615, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.567\n",
      "Iter=330, avg train loss=0.648, \n",
      "\tAvg Val Loss: same-cc=0.594, AUC: same-cc=0.597 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.595, AUC: opp-cc=0.525 \n",
      "\tAvg Val Loss: opp-mlo=0.588, AUC: opp-mlo=0.579\n",
      "Iter=335, avg train loss=0.690, \n",
      "\tAvg Val Loss: same-cc=0.598, AUC: same-cc=0.571 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.587 \n",
      "\tAvg Val Loss opp-cc=0.604, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.597, AUC: opp-mlo=0.579\n",
      "Iter=340, avg train loss=0.761, \n",
      "\tAvg Val Loss: same-cc=0.611, AUC: same-cc=0.548 \n",
      "\tAvg Val Loss: same-mlo=0.664, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.546 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.569\n",
      "Iter=345, avg train loss=0.625, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.541 \n",
      "\tAvg Val Loss: same-mlo=0.704, AUC: same-mlo=0.581 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.544 \n",
      "\tAvg Val Loss: opp-mlo=0.650, AUC: opp-mlo=0.553\n",
      "Iter=350, avg train loss=0.601, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.694, AUC: same-mlo=0.585 \n",
      "\tAvg Val Loss opp-cc=0.708, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.549\n",
      "Iter=355, avg train loss=0.620, \n",
      "\tAvg Val Loss: same-cc=0.616, AUC: same-cc=0.530 \n",
      "\tAvg Val Loss: same-mlo=0.689, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.557\n",
      "Iter=360, avg train loss=0.618, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.709, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.562 \n",
      "\tAvg Val Loss: opp-mlo=0.691, AUC: opp-mlo=0.567\n",
      "Iter=365, avg train loss=0.581, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.666, AUC: same-mlo=0.616 \n",
      "\tAvg Val Loss opp-cc=0.678, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.682, AUC: opp-mlo=0.589\n",
      "Iter=370, avg train loss=0.639, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.681, AUC: opp-cc=0.574 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.601\n",
      "Iter=375, avg train loss=0.599, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.551 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.616 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.650, AUC: opp-mlo=0.597\n",
      "Iter=380, avg train loss=0.608, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.688, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.592\n",
      "Iter=385, avg train loss=0.587, \n",
      "\tAvg Val Loss: same-cc=0.661, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.716, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.664, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.667, AUC: opp-mlo=0.587\n",
      "Iter=390, avg train loss=0.641, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.528 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.637, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.568\n",
      "Iter=395, avg train loss=0.582, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.626, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.557\n",
      "Iter=400, avg train loss=0.658, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.549 \n",
      "\tAvg Val Loss: same-mlo=0.651, AUC: same-mlo=0.589 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.646, AUC: opp-mlo=0.543\n",
      "Iter=405, avg train loss=0.662, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.630, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.658, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.648, AUC: opp-mlo=0.547\n",
      "Iter=410, avg train loss=0.615, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.544 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.579 \n",
      "\tAvg Val Loss: opp-mlo=0.637, AUC: opp-mlo=0.546\n",
      "Iter=415, avg train loss=0.653, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.635, AUC: opp-cc=0.569 \n",
      "\tAvg Val Loss: opp-mlo=0.636, AUC: opp-mlo=0.539\n",
      "Iter=420, avg train loss=0.628, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.520 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.671, AUC: opp-mlo=0.530\n",
      "Iter=425, avg train loss=0.541, \n",
      "\tAvg Val Loss: same-cc=0.684, AUC: same-cc=0.528 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=0.664, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.680, AUC: opp-mlo=0.526\n",
      "Iter=430, avg train loss=0.563, \n",
      "\tAvg Val Loss: same-cc=0.682, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.682, AUC: opp-mlo=0.516\n",
      "Iter=435, avg train loss=0.528, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.513 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.650 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.643, AUC: opp-mlo=0.535\n",
      "Iter=440, avg train loss=0.549, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.517 \n",
      "\tAvg Val Loss: same-mlo=0.630, AUC: same-mlo=0.642 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.641, AUC: opp-mlo=0.546\n",
      "Iter=445, avg train loss=0.618, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.539 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.646, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.557\n",
      "Iter=450, avg train loss=0.576, \n",
      "\tAvg Val Loss: same-cc=0.669, AUC: same-cc=0.544 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.644, AUC: opp-mlo=0.567\n",
      "Iter=455, avg train loss=0.590, \n",
      "\tAvg Val Loss: same-cc=0.713, AUC: same-cc=0.560 \n",
      "\tAvg Val Loss: same-mlo=0.626, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.733, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.570\n",
      "Iter=460, avg train loss=0.606, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.561 \n",
      "\tAvg Val Loss: same-mlo=0.620, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.565 \n",
      "\tAvg Val Loss: opp-mlo=0.644, AUC: opp-mlo=0.589\n",
      "Iter=465, avg train loss=0.552, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.543 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.626, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.621, AUC: opp-mlo=0.588\n",
      "Iter=470, avg train loss=0.603, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.613, AUC: opp-cc=0.582 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.604\n",
      "Iter=475, avg train loss=0.557, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.534 \n",
      "\tAvg Val Loss: same-mlo=0.608, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.618, AUC: opp-cc=0.574 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.600\n",
      "Iter=480, avg train loss=0.556, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.644, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.641, AUC: opp-mlo=0.608\n",
      "Iter=485, avg train loss=0.534, \n",
      "\tAvg Val Loss: same-cc=0.719, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.723, AUC: same-mlo=0.645 \n",
      "\tAvg Val Loss opp-cc=0.707, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.714, AUC: opp-mlo=0.591\n",
      "Iter=490, avg train loss=0.578, \n",
      "\tAvg Val Loss: same-cc=0.752, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.716, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.742, AUC: opp-cc=0.544 \n",
      "\tAvg Val Loss: opp-mlo=0.720, AUC: opp-mlo=0.588\n",
      "Iter=495, avg train loss=0.552, \n",
      "\tAvg Val Loss: same-cc=0.713, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.684, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.731, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=500, avg train loss=0.667, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.538 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.717, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.687, AUC: opp-mlo=0.586\n",
      "Iter=505, avg train loss=0.601, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.539 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.649 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.640, AUC: opp-mlo=0.584\n",
      "Iter=510, avg train loss=0.537, \n",
      "\tAvg Val Loss: same-cc=0.656, AUC: same-cc=0.541 \n",
      "\tAvg Val Loss: same-mlo=0.636, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.518 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.585\n",
      "Iter=515, avg train loss=0.581, \n",
      "\tAvg Val Loss: same-cc=0.628, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.628, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.588\n",
      "Iter=520, avg train loss=0.476, \n",
      "\tAvg Val Loss: same-cc=0.606, AUC: same-cc=0.512 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.652 \n",
      "\tAvg Val Loss opp-cc=0.632, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.598, AUC: opp-mlo=0.586\n",
      "Iter=525, avg train loss=0.675, \n",
      "\tAvg Val Loss: same-cc=0.606, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.611, AUC: opp-mlo=0.579\n",
      "Iter=530, avg train loss=0.583, \n",
      "\tAvg Val Loss: same-cc=0.649, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.577\n",
      "Iter=535, avg train loss=0.541, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.634 \n",
      "\tAvg Val Loss opp-cc=0.611, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.607\n",
      "Iter=540, avg train loss=0.472, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.601 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.647 \n",
      "\tAvg Val Loss opp-cc=0.610, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.603, AUC: opp-mlo=0.614\n",
      "Iter=545, avg train loss=0.616, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.600 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=0.638, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.580, AUC: opp-mlo=0.630\n",
      "Best opp-mlo model saved.\n",
      "Iter=550, avg train loss=0.501, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.603 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.645 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.624\n",
      "Iter=555, avg train loss=0.517, \n",
      "\tAvg Val Loss: same-cc=0.681, AUC: same-cc=0.614 \n",
      "\tAvg Val Loss: same-mlo=0.686, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.624\n",
      "Iter=560, avg train loss=0.511, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.595 \n",
      "\tAvg Val Loss: same-mlo=0.704, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.689, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.618\n",
      "Iter=565, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.738, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.737, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.615\n",
      "Iter=570, avg train loss=0.439, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.752, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.746, AUC: opp-cc=0.518 \n",
      "\tAvg Val Loss: opp-mlo=0.698, AUC: opp-mlo=0.605\n",
      "Iter=575, avg train loss=0.414, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.719, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.737, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.663, AUC: opp-mlo=0.616\n",
      "Iter=580, avg train loss=0.489, \n",
      "\tAvg Val Loss: same-cc=0.608, AUC: same-cc=0.577 \n",
      "\tAvg Val Loss: same-mlo=0.660, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.615\n",
      "Iter=585, avg train loss=0.579, \n",
      "\tAvg Val Loss: same-cc=0.597, AUC: same-cc=0.560 \n",
      "\tAvg Val Loss: same-mlo=0.628, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.655, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.600\n",
      "Iter=590, avg train loss=0.679, \n",
      "\tAvg Val Loss: same-cc=0.605, AUC: same-cc=0.528 \n",
      "\tAvg Val Loss: same-mlo=0.630, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.659, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.585\n",
      "Iter=595, avg train loss=0.435, \n",
      "\tAvg Val Loss: same-cc=0.672, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.644, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.705, AUC: opp-cc=0.546 \n",
      "\tAvg Val Loss: opp-mlo=0.671, AUC: opp-mlo=0.586\n",
      "Iter=600, avg train loss=0.581, \n",
      "\tAvg Val Loss: same-cc=0.804, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.710, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.817, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.572\n",
      "Iter=605, avg train loss=0.493, \n",
      "\tAvg Val Loss: same-cc=0.887, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.775, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.769, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.578\n",
      "Iter=610, avg train loss=0.507, \n",
      "\tAvg Val Loss: same-cc=0.755, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.683, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.592\n",
      "Iter=615, avg train loss=0.447, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.654, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.638, AUC: opp-cc=0.537 \n",
      "\tAvg Val Loss: opp-mlo=0.631, AUC: opp-mlo=0.588\n",
      "Iter=620, avg train loss=0.637, \n",
      "\tAvg Val Loss: same-cc=0.644, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.665, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.689, AUC: opp-mlo=0.594\n",
      "Iter=625, avg train loss=0.465, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.645, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=0.777, AUC: opp-cc=0.525 \n",
      "\tAvg Val Loss: opp-mlo=0.691, AUC: opp-mlo=0.595\n",
      "Iter=630, avg train loss=0.543, \n",
      "\tAvg Val Loss: same-cc=0.633, AUC: same-cc=0.557 \n",
      "\tAvg Val Loss: same-mlo=0.628, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.733, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.612\n",
      "Iter=635, avg train loss=0.476, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.577, AUC: opp-mlo=0.619\n",
      "Iter=640, avg train loss=0.470, \n",
      "\tAvg Val Loss: same-cc=0.680, AUC: same-cc=0.586 \n",
      "\tAvg Val Loss: same-mlo=0.576, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.563, AUC: opp-mlo=0.607\n",
      "Iter=645, avg train loss=0.588, \n",
      "\tAvg Val Loss: same-cc=0.644, AUC: same-cc=0.560 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.655, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.574, AUC: opp-mlo=0.603\n",
      "Iter=650, avg train loss=0.449, \n",
      "\tAvg Val Loss: same-cc=0.681, AUC: same-cc=0.564 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.664, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.585\n",
      "Iter=655, avg train loss=0.410, \n",
      "\tAvg Val Loss: same-cc=0.639, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.659, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.596, AUC: opp-mlo=0.612\n",
      "Iter=660, avg train loss=0.561, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.534 \n",
      "\tAvg Val Loss: same-mlo=0.580, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.577, AUC: opp-mlo=0.618\n",
      "Iter=665, avg train loss=0.457, \n",
      "\tAvg Val Loss: same-cc=0.742, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.652 \n",
      "\tAvg Val Loss opp-cc=0.783, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.631\n",
      "Best opp-mlo model saved.\n",
      "Iter=670, avg train loss=0.580, \n",
      "\tAvg Val Loss: same-cc=0.764, AUC: same-cc=0.566 \n",
      "\tAvg Val Loss: same-mlo=0.693, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.740, AUC: opp-cc=0.508 \n",
      "\tAvg Val Loss: opp-mlo=0.748, AUC: opp-mlo=0.610\n",
      "Iter=675, avg train loss=0.511, \n",
      "\tAvg Val Loss: same-cc=0.922, AUC: same-cc=0.594 \n",
      "\tAvg Val Loss: same-mlo=0.793, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.724, AUC: opp-cc=0.497 \n",
      "\tAvg Val Loss: opp-mlo=0.853, AUC: opp-mlo=0.589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=680, avg train loss=0.518, \n",
      "\tAvg Val Loss: same-cc=0.820, AUC: same-cc=0.607 \n",
      "\tAvg Val Loss: same-mlo=0.787, AUC: same-mlo=0.660 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.796, AUC: opp-mlo=0.602\n",
      "Iter=685, avg train loss=0.513, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.595 \n",
      "\tAvg Val Loss: same-mlo=0.755, AUC: same-mlo=0.647 \n",
      "\tAvg Val Loss opp-cc=0.643, AUC: opp-cc=0.510 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.606\n",
      "Iter=690, avg train loss=0.506, \n",
      "\tAvg Val Loss: same-cc=0.584, AUC: same-cc=0.564 \n",
      "\tAvg Val Loss: same-mlo=0.626, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.599, AUC: opp-mlo=0.617\n",
      "Iter=695, avg train loss=0.610, \n",
      "\tAvg Val Loss: same-cc=0.598, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.659, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.597, AUC: opp-mlo=0.604\n",
      "Iter=700, avg train loss=0.393, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.564, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.582, AUC: opp-mlo=0.585\n",
      "Iter=705, avg train loss=0.403, \n",
      "\tAvg Val Loss: same-cc=0.610, AUC: same-cc=0.495 \n",
      "\tAvg Val Loss: same-mlo=0.580, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.726, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.578\n",
      "Iter=710, avg train loss=0.405, \n",
      "\tAvg Val Loss: same-cc=0.601, AUC: same-cc=0.503 \n",
      "\tAvg Val Loss: same-mlo=0.570, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.584, AUC: opp-mlo=0.589\n",
      "Iter=715, avg train loss=0.572, \n",
      "\tAvg Val Loss: same-cc=0.628, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.639, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.795, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.693, AUC: opp-mlo=0.605\n",
      "Iter=720, avg train loss=0.534, \n",
      "\tAvg Val Loss: same-cc=0.795, AUC: same-cc=0.564 \n",
      "\tAvg Val Loss: same-mlo=0.792, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.902, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.819, AUC: opp-mlo=0.597\n",
      "Iter=725, avg train loss=0.424, \n",
      "\tAvg Val Loss: same-cc=0.851, AUC: same-cc=0.584 \n",
      "\tAvg Val Loss: same-mlo=0.842, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=0.720, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.709, AUC: opp-mlo=0.587\n",
      "Iter=730, avg train loss=0.490, \n",
      "\tAvg Val Loss: same-cc=0.810, AUC: same-cc=0.592 \n",
      "\tAvg Val Loss: same-mlo=0.790, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.712, AUC: opp-cc=0.528 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.583\n",
      "Iter=735, avg train loss=0.370, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.565 \n",
      "\tAvg Val Loss: same-mlo=0.675, AUC: same-mlo=0.652 \n",
      "\tAvg Val Loss opp-cc=0.722, AUC: opp-cc=0.532 \n",
      "\tAvg Val Loss: opp-mlo=0.588, AUC: opp-mlo=0.589\n",
      "Iter=740, avg train loss=0.579, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.568 \n",
      "\tAvg Val Loss: same-mlo=0.654, AUC: same-mlo=0.649 \n",
      "\tAvg Val Loss opp-cc=0.746, AUC: opp-cc=0.528 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.596\n",
      "Iter=745, avg train loss=0.491, \n",
      "\tAvg Val Loss: same-cc=0.704, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.750, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.828, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.738, AUC: opp-mlo=0.585\n",
      "Iter=750, avg train loss=0.420, \n",
      "\tAvg Val Loss: same-cc=0.742, AUC: same-cc=0.622 \n",
      "\tAvg Val Loss: same-mlo=0.755, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.785, AUC: opp-cc=0.532 \n",
      "\tAvg Val Loss: opp-mlo=0.773, AUC: opp-mlo=0.599\n",
      "Best same-cc model saved.\n",
      "Iter=755, avg train loss=0.412, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.621 \n",
      "\tAvg Val Loss: same-mlo=0.630, AUC: same-mlo=0.660 \n",
      "\tAvg Val Loss opp-cc=0.782, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.610\n",
      "Iter=760, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.621 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.649 \n",
      "\tAvg Val Loss opp-cc=0.711, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.632\n",
      "Best opp-mlo model saved.\n",
      "Iter=765, avg train loss=0.469, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.658 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.804, AUC: opp-cc=0.537 \n",
      "\tAvg Val Loss: opp-mlo=0.596, AUC: opp-mlo=0.648\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=770, avg train loss=0.477, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.674 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.836, AUC: opp-cc=0.546 \n",
      "\tAvg Val Loss: opp-mlo=0.619, AUC: opp-mlo=0.628\n",
      "Best same-cc model saved.\n",
      "Iter=775, avg train loss=0.430, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.651 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.619\n",
      "Iter=780, avg train loss=0.467, \n",
      "\tAvg Val Loss: same-cc=0.684, AUC: same-cc=0.664 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.655 \n",
      "\tAvg Val Loss opp-cc=0.725, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.692, AUC: opp-mlo=0.610\n",
      "Iter=785, avg train loss=0.390, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.637 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.640 \n",
      "\tAvg Val Loss opp-cc=0.667, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.612\n",
      "Iter=790, avg train loss=0.419, \n",
      "\tAvg Val Loss: same-cc=0.594, AUC: same-cc=0.621 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.546 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.604\n",
      "Iter=795, avg train loss=0.410, \n",
      "\tAvg Val Loss: same-cc=0.600, AUC: same-cc=0.610 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.635 \n",
      "\tAvg Val Loss opp-cc=0.702, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.607\n",
      "Iter=800, avg train loss=0.340, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.609 \n",
      "\tAvg Val Loss: same-mlo=0.647, AUC: same-mlo=0.639 \n",
      "\tAvg Val Loss opp-cc=0.797, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.633, AUC: opp-mlo=0.610\n",
      "Iter=805, avg train loss=0.406, \n",
      "\tAvg Val Loss: same-cc=0.596, AUC: same-cc=0.591 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.645 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.534 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.623\n",
      "Iter=810, avg train loss=0.329, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.571 \n",
      "\tAvg Val Loss: same-mlo=0.585, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.544 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.628\n",
      "Iter=815, avg train loss=0.502, \n",
      "\tAvg Val Loss: same-cc=0.621, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.602, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.750, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.663, AUC: opp-mlo=0.628\n",
      "Iter=820, avg train loss=0.460, \n",
      "\tAvg Val Loss: same-cc=0.700, AUC: same-cc=0.629 \n",
      "\tAvg Val Loss: same-mlo=0.736, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=1.229, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.912, AUC: opp-mlo=0.607\n",
      "Iter=825, avg train loss=0.350, \n",
      "\tAvg Val Loss: same-cc=0.945, AUC: same-cc=0.654 \n",
      "\tAvg Val Loss: same-mlo=0.922, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=1.280, AUC: opp-cc=0.565 \n",
      "\tAvg Val Loss: opp-mlo=0.998, AUC: opp-mlo=0.602\n",
      "Iter=830, avg train loss=0.343, \n",
      "\tAvg Val Loss: same-cc=0.814, AUC: same-cc=0.649 \n",
      "\tAvg Val Loss: same-mlo=0.981, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.894, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.857, AUC: opp-mlo=0.596\n",
      "Iter=835, avg train loss=0.473, \n",
      "\tAvg Val Loss: same-cc=0.586, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.768, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.534 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.618\n",
      "Iter=840, avg train loss=0.429, \n",
      "\tAvg Val Loss: same-cc=0.586, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.643 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.528 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.616\n",
      "Iter=845, avg train loss=0.397, \n",
      "\tAvg Val Loss: same-cc=0.598, AUC: same-cc=0.610 \n",
      "\tAvg Val Loss: same-mlo=0.699, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.737, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.599\n",
      "Iter=850, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.626, AUC: same-cc=0.630 \n",
      "\tAvg Val Loss: same-mlo=0.781, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.780, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.658, AUC: opp-mlo=0.585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=855, avg train loss=0.349, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.615 \n",
      "\tAvg Val Loss: same-mlo=0.754, AUC: same-mlo=0.639 \n",
      "\tAvg Val Loss opp-cc=0.717, AUC: opp-cc=0.518 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.582\n",
      "Iter=860, avg train loss=0.418, \n",
      "\tAvg Val Loss: same-cc=0.644, AUC: same-cc=0.649 \n",
      "\tAvg Val Loss: same-mlo=0.753, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.727, AUC: opp-cc=0.546 \n",
      "\tAvg Val Loss: opp-mlo=0.683, AUC: opp-mlo=0.567\n",
      "Iter=865, avg train loss=0.320, \n",
      "\tAvg Val Loss: same-cc=0.603, AUC: same-cc=0.644 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.639 \n",
      "\tAvg Val Loss opp-cc=0.667, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.588\n",
      "Iter=870, avg train loss=0.375, \n",
      "\tAvg Val Loss: same-cc=0.576, AUC: same-cc=0.637 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.671, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.600\n",
      "Iter=875, avg train loss=0.435, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.658 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.700, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.645, AUC: opp-mlo=0.593\n",
      "Iter=880, avg train loss=0.436, \n",
      "\tAvg Val Loss: same-cc=0.633, AUC: same-cc=0.640 \n",
      "\tAvg Val Loss: same-mlo=0.653, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.783, AUC: opp-cc=0.567 \n",
      "\tAvg Val Loss: opp-mlo=0.720, AUC: opp-mlo=0.579\n",
      "Iter=885, avg train loss=0.313, \n",
      "\tAvg Val Loss: same-cc=0.608, AUC: same-cc=0.571 \n",
      "\tAvg Val Loss: same-mlo=0.656, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.755, AUC: opp-cc=0.573 \n",
      "\tAvg Val Loss: opp-mlo=0.698, AUC: opp-mlo=0.595\n",
      "Iter=890, avg train loss=0.429, \n",
      "\tAvg Val Loss: same-cc=0.711, AUC: same-cc=0.534 \n",
      "\tAvg Val Loss: same-mlo=0.627, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.637, AUC: opp-mlo=0.587\n",
      "Iter=895, avg train loss=0.465, \n",
      "\tAvg Val Loss: same-cc=0.679, AUC: same-cc=0.516 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.635 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.564 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.607\n",
      "Iter=900, avg train loss=0.491, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.553 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.640 \n",
      "\tAvg Val Loss opp-cc=0.773, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.685, AUC: opp-mlo=0.618\n",
      "Iter=905, avg train loss=0.402, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.698, AUC: same-mlo=0.652 \n",
      "\tAvg Val Loss opp-cc=0.828, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.809, AUC: opp-mlo=0.623\n",
      "Iter=910, avg train loss=0.367, \n",
      "\tAvg Val Loss: same-cc=0.674, AUC: same-cc=0.639 \n",
      "\tAvg Val Loss: same-mlo=0.640, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.695, AUC: opp-cc=0.535 \n",
      "\tAvg Val Loss: opp-mlo=0.643, AUC: opp-mlo=0.642\n",
      "Iter=915, avg train loss=0.335, \n",
      "\tAvg Val Loss: same-cc=0.690, AUC: same-cc=0.645 \n",
      "\tAvg Val Loss: same-mlo=0.602, AUC: same-mlo=0.657 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.594, AUC: opp-mlo=0.628\n",
      "Iter=920, avg train loss=0.306, \n",
      "\tAvg Val Loss: same-cc=0.672, AUC: same-cc=0.640 \n",
      "\tAvg Val Loss: same-mlo=0.577, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.596\n",
      "Iter=925, avg train loss=0.316, \n",
      "\tAvg Val Loss: same-cc=0.744, AUC: same-cc=0.679 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.606\n",
      "Best same-cc model saved.\n",
      "Iter=930, avg train loss=0.389, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.676 \n",
      "\tAvg Val Loss: same-mlo=0.664, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.639, AUC: opp-mlo=0.599\n",
      "Iter=935, avg train loss=0.332, \n",
      "\tAvg Val Loss: same-cc=0.643, AUC: same-cc=0.655 \n",
      "\tAvg Val Loss: same-mlo=0.655, AUC: same-mlo=0.648 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.573 \n",
      "\tAvg Val Loss: opp-mlo=0.692, AUC: opp-mlo=0.603\n",
      "Iter=940, avg train loss=0.361, \n",
      "\tAvg Val Loss: same-cc=0.679, AUC: same-cc=0.653 \n",
      "\tAvg Val Loss: same-mlo=0.660, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.885, AUC: opp-cc=0.583 \n",
      "\tAvg Val Loss: opp-mlo=0.792, AUC: opp-mlo=0.600\n",
      "Iter=945, avg train loss=0.328, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.649 \n",
      "\tAvg Val Loss: same-mlo=0.713, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.838, AUC: opp-cc=0.586 \n",
      "\tAvg Val Loss: opp-mlo=0.815, AUC: opp-mlo=0.570\n",
      "Iter=950, avg train loss=0.290, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.645 \n",
      "\tAvg Val Loss: same-mlo=0.684, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.768, AUC: opp-cc=0.588 \n",
      "\tAvg Val Loss: opp-mlo=0.719, AUC: opp-mlo=0.572\n",
      "Iter=955, avg train loss=0.453, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.649, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.595 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.559\n",
      "Iter=960, avg train loss=0.328, \n",
      "\tAvg Val Loss: same-cc=0.592, AUC: same-cc=0.556 \n",
      "\tAvg Val Loss: same-mlo=0.625, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.607 \n",
      "\tAvg Val Loss: opp-mlo=0.640, AUC: opp-mlo=0.566\n",
      "Iter=965, avg train loss=0.498, \n",
      "\tAvg Val Loss: same-cc=0.606, AUC: same-cc=0.547 \n",
      "\tAvg Val Loss: same-mlo=0.624, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.703, AUC: opp-cc=0.602 \n",
      "\tAvg Val Loss: opp-mlo=0.624, AUC: opp-mlo=0.564\n",
      "Iter=970, avg train loss=0.256, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=0.700, AUC: same-mlo=0.639 \n",
      "\tAvg Val Loss opp-cc=0.760, AUC: opp-cc=0.578 \n",
      "\tAvg Val Loss: opp-mlo=0.698, AUC: opp-mlo=0.551\n",
      "Iter=975, avg train loss=0.303, \n",
      "\tAvg Val Loss: same-cc=0.741, AUC: same-cc=0.627 \n",
      "\tAvg Val Loss: same-mlo=0.712, AUC: same-mlo=0.648 \n",
      "\tAvg Val Loss opp-cc=0.719, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.686, AUC: opp-mlo=0.566\n",
      "Iter=980, avg train loss=0.317, \n",
      "\tAvg Val Loss: same-cc=0.711, AUC: same-cc=0.633 \n",
      "\tAvg Val Loss: same-mlo=0.737, AUC: same-mlo=0.649 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.587\n",
      "Iter=985, avg train loss=0.399, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.659 \n",
      "\tAvg Val Loss: same-mlo=0.823, AUC: same-mlo=0.655 \n",
      "\tAvg Val Loss opp-cc=0.723, AUC: opp-cc=0.551 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.610\n",
      "Iter=990, avg train loss=0.389, \n",
      "\tAvg Val Loss: same-cc=0.739, AUC: same-cc=0.671 \n",
      "\tAvg Val Loss: same-mlo=0.779, AUC: same-mlo=0.642 \n",
      "\tAvg Val Loss opp-cc=0.840, AUC: opp-cc=0.577 \n",
      "\tAvg Val Loss: opp-mlo=0.689, AUC: opp-mlo=0.586\n",
      "Iter=995, avg train loss=0.404, \n",
      "\tAvg Val Loss: same-cc=0.597, AUC: same-cc=0.602 \n",
      "\tAvg Val Loss: same-mlo=0.636, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.785, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.577\n",
      "Iter=1000, avg train loss=0.325, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.771, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.593\n",
      "Iter=1005, avg train loss=0.332, \n",
      "\tAvg Val Loss: same-cc=0.633, AUC: same-cc=0.608 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.802, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.718, AUC: opp-mlo=0.575\n",
      "Iter=1010, avg train loss=0.306, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.556 \n",
      "\tAvg Val Loss: same-mlo=0.602, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.713, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.577\n",
      "Iter=1015, avg train loss=0.351, \n",
      "\tAvg Val Loss: same-cc=0.612, AUC: same-cc=0.579 \n",
      "\tAvg Val Loss: same-mlo=0.640, AUC: same-mlo=0.635 \n",
      "\tAvg Val Loss opp-cc=0.736, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.565\n",
      "Iter=1020, avg train loss=0.404, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.608 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.837, AUC: opp-cc=0.553 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.573\n",
      "Iter=1025, avg train loss=0.313, \n",
      "\tAvg Val Loss: same-cc=0.829, AUC: same-cc=0.665 \n",
      "\tAvg Val Loss: same-mlo=0.785, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.809, AUC: opp-cc=0.551 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.574\n",
      "Iter=1030, avg train loss=0.366, \n",
      "\tAvg Val Loss: same-cc=0.728, AUC: same-cc=0.655 \n",
      "\tAvg Val Loss: same-mlo=0.818, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.725, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.674, AUC: opp-mlo=0.573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1035, avg train loss=0.340, \n",
      "\tAvg Val Loss: same-cc=0.611, AUC: same-cc=0.626 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.639 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.594\n",
      "Iter=1040, avg train loss=0.304, \n",
      "\tAvg Val Loss: same-cc=0.621, AUC: same-cc=0.659 \n",
      "\tAvg Val Loss: same-mlo=0.584, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.599, AUC: opp-mlo=0.602\n",
      "Best models loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training: same-cc=0.727, same-mlo=0.607, opp-cc=0.494, opp-mlo=0.441\n",
      "Max-Score Based AUC After Training: 0.632, Mean-Score Based AUC After Training: 0.628\n",
      "\n",
      "\n",
      "\n",
      "========== Fold 3 ==========\n",
      "Test AUC at start: same-cc=0.497, same-mlo=0.471, opp-cc=0.495, opp-mlo=0.471\n",
      "Max-Score Based AUC Before Training: 0.486, Mean-Score Based AUC Before Training: 0.483\n",
      "Iter=5, avg train loss=0.810, \n",
      "\tAvg Val Loss: same-cc=0.584, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.573, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.580, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.621, AUC: opp-mlo=0.442\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=10, avg train loss=0.905, \n",
      "\tAvg Val Loss: same-cc=0.598, AUC: same-cc=0.513 \n",
      "\tAvg Val Loss: same-mlo=0.564, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.577, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.477\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=15, avg train loss=0.839, \n",
      "\tAvg Val Loss: same-cc=0.608, AUC: same-cc=0.491 \n",
      "\tAvg Val Loss: same-mlo=0.568, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.585, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.496\n",
      "Best opp-mlo model saved.\n",
      "Iter=20, avg train loss=0.892, \n",
      "\tAvg Val Loss: same-cc=0.614, AUC: same-cc=0.500 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.589, AUC: opp-cc=0.506 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.508\n",
      "Best opp-mlo model saved.\n",
      "Iter=25, avg train loss=0.831, \n",
      "\tAvg Val Loss: same-cc=0.621, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.599, AUC: opp-cc=0.498 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.517\n",
      "Best opp-mlo model saved.\n",
      "Iter=30, avg train loss=0.688, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.635, AUC: opp-cc=0.488 \n",
      "\tAvg Val Loss: opp-mlo=0.610, AUC: opp-mlo=0.501\n",
      "Iter=35, avg train loss=0.843, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.526 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.642 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.468 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.514\n",
      "Best same-mlo model saved.\n",
      "Iter=40, avg train loss=0.812, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.659 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.535\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=45, avg train loss=0.712, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.551 \n",
      "\tAvg Val Loss: same-mlo=0.588, AUC: same-mlo=0.662 \n",
      "\tAvg Val Loss opp-cc=0.646, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.603, AUC: opp-mlo=0.543\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=50, avg train loss=0.687, \n",
      "\tAvg Val Loss: same-cc=0.640, AUC: same-cc=0.566 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.662 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.559\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=55, avg train loss=0.792, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.665 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.478 \n",
      "\tAvg Val Loss: opp-mlo=0.596, AUC: opp-mlo=0.561\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=60, avg train loss=0.685, \n",
      "\tAvg Val Loss: same-cc=0.618, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.672 \n",
      "\tAvg Val Loss opp-cc=0.658, AUC: opp-cc=0.470 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.570\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=65, avg train loss=0.704, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.575 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.665 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.463 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.580\n",
      "Best opp-mlo model saved.\n",
      "Iter=70, avg train loss=0.634, \n",
      "\tAvg Val Loss: same-cc=0.614, AUC: same-cc=0.587 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.668 \n",
      "\tAvg Val Loss opp-cc=0.658, AUC: opp-cc=0.468 \n",
      "\tAvg Val Loss: opp-mlo=0.617, AUC: opp-mlo=0.588\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=75, avg train loss=0.738, \n",
      "\tAvg Val Loss: same-cc=0.621, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=0.606, AUC: same-mlo=0.667 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.470 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.573\n",
      "Iter=80, avg train loss=0.759, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.627, AUC: opp-mlo=0.581\n",
      "Iter=85, avg train loss=0.599, \n",
      "\tAvg Val Loss: same-cc=0.622, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.664 \n",
      "\tAvg Val Loss opp-cc=0.697, AUC: opp-cc=0.470 \n",
      "\tAvg Val Loss: opp-mlo=0.624, AUC: opp-mlo=0.584\n",
      "Iter=90, avg train loss=0.698, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.594 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.665 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.582\n",
      "Best same-cc model saved.\n",
      "Iter=95, avg train loss=0.677, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.606 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.668 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.480 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.608\n",
      "Best same-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=100, avg train loss=0.780, \n",
      "\tAvg Val Loss: same-cc=0.639, AUC: same-cc=0.599 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.666 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.482 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.616\n",
      "Best opp-mlo model saved.\n",
      "Iter=105, avg train loss=0.720, \n",
      "\tAvg Val Loss: same-cc=0.650, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.626, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.490 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.617\n",
      "Best opp-mlo model saved.\n",
      "Iter=110, avg train loss=0.661, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.661 \n",
      "\tAvg Val Loss opp-cc=0.651, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.621\n",
      "Best opp-mlo model saved.\n",
      "Iter=115, avg train loss=0.649, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.591 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.494 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.630\n",
      "Best opp-mlo model saved.\n",
      "Iter=120, avg train loss=0.606, \n",
      "\tAvg Val Loss: same-cc=0.639, AUC: same-cc=0.595 \n",
      "\tAvg Val Loss: same-mlo=0.608, AUC: same-mlo=0.664 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.637\n",
      "Best opp-mlo model saved.\n",
      "Iter=125, avg train loss=0.656, \n",
      "\tAvg Val Loss: same-cc=0.638, AUC: same-cc=0.592 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.642\n",
      "Best opp-mlo model saved.\n",
      "Iter=130, avg train loss=0.681, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.597 \n",
      "\tAvg Val Loss: same-mlo=0.572, AUC: same-mlo=0.677 \n",
      "\tAvg Val Loss opp-cc=0.656, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.655\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=135, avg train loss=0.695, \n",
      "\tAvg Val Loss: same-cc=0.636, AUC: same-cc=0.591 \n",
      "\tAvg Val Loss: same-mlo=0.572, AUC: same-mlo=0.684 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.602, AUC: opp-mlo=0.673\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=140, avg train loss=0.692, \n",
      "\tAvg Val Loss: same-cc=0.651, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.681 \n",
      "\tAvg Val Loss opp-cc=0.703, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.676\n",
      "Best opp-mlo model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=145, avg train loss=0.672, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.594 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.669 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.532 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.675\n",
      "Best opp-cc model saved.\n",
      "Iter=150, avg train loss=0.638, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.675 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.543 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.673\n",
      "Best opp-cc model saved.\n",
      "Iter=155, avg train loss=0.661, \n",
      "\tAvg Val Loss: same-cc=0.644, AUC: same-cc=0.610 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.672 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.658\n",
      "Best same-cc model saved.\n",
      "Iter=160, avg train loss=0.659, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.621 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.675 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.597, AUC: opp-mlo=0.645\n",
      "Best same-cc model saved.\n",
      "Best opp-cc model saved.\n",
      "Iter=165, avg train loss=0.667, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.613 \n",
      "\tAvg Val Loss: same-mlo=0.561, AUC: same-mlo=0.686 \n",
      "\tAvg Val Loss opp-cc=0.646, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.587, AUC: opp-mlo=0.633\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Iter=170, avg train loss=0.679, \n",
      "\tAvg Val Loss: same-cc=0.618, AUC: same-cc=0.617 \n",
      "\tAvg Val Loss: same-mlo=0.552, AUC: same-mlo=0.697 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.573 \n",
      "\tAvg Val Loss: opp-mlo=0.584, AUC: opp-mlo=0.631\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Iter=175, avg train loss=0.642, \n",
      "\tAvg Val Loss: same-cc=0.600, AUC: same-cc=0.616 \n",
      "\tAvg Val Loss: same-mlo=0.541, AUC: same-mlo=0.700 \n",
      "\tAvg Val Loss opp-cc=0.622, AUC: opp-cc=0.562 \n",
      "\tAvg Val Loss: opp-mlo=0.571, AUC: opp-mlo=0.636\n",
      "Best same-mlo model saved.\n",
      "Iter=180, avg train loss=0.667, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.603 \n",
      "\tAvg Val Loss: same-mlo=0.553, AUC: same-mlo=0.696 \n",
      "\tAvg Val Loss opp-cc=0.633, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.576, AUC: opp-mlo=0.638\n",
      "Iter=185, avg train loss=0.691, \n",
      "\tAvg Val Loss: same-cc=0.668, AUC: same-cc=0.597 \n",
      "\tAvg Val Loss: same-mlo=0.620, AUC: same-mlo=0.681 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.535 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.621\n",
      "Iter=190, avg train loss=0.620, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.645, AUC: same-mlo=0.675 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.619\n",
      "Iter=195, avg train loss=0.647, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.605 \n",
      "\tAvg Val Loss: same-mlo=0.670, AUC: same-mlo=0.655 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.512 \n",
      "\tAvg Val Loss: opp-mlo=0.648, AUC: opp-mlo=0.606\n",
      "Iter=200, avg train loss=0.688, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.614 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.516 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.565\n",
      "Iter=205, avg train loss=0.650, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.628 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.659 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.535 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.578\n",
      "Best same-cc model saved.\n",
      "Iter=210, avg train loss=0.614, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.622 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.670 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.534 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.582\n",
      "Iter=215, avg train loss=0.690, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.628 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.669 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.636, AUC: opp-mlo=0.591\n",
      "Iter=220, avg train loss=0.646, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.633 \n",
      "\tAvg Val Loss: same-mlo=0.624, AUC: same-mlo=0.665 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.595\n",
      "Best same-cc model saved.\n",
      "Iter=225, avg train loss=0.655, \n",
      "\tAvg Val Loss: same-cc=0.639, AUC: same-cc=0.608 \n",
      "\tAvg Val Loss: same-mlo=0.675, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.648, AUC: opp-mlo=0.617\n",
      "Iter=230, avg train loss=0.633, \n",
      "\tAvg Val Loss: same-cc=0.663, AUC: same-cc=0.613 \n",
      "\tAvg Val Loss: same-mlo=0.719, AUC: same-mlo=0.635 \n",
      "\tAvg Val Loss opp-cc=0.702, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.664, AUC: opp-mlo=0.628\n",
      "Iter=235, avg train loss=0.657, \n",
      "\tAvg Val Loss: same-cc=0.663, AUC: same-cc=0.614 \n",
      "\tAvg Val Loss: same-mlo=0.721, AUC: same-mlo=0.646 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.635, AUC: opp-mlo=0.644\n",
      "Iter=240, avg train loss=0.627, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.624 \n",
      "\tAvg Val Loss: same-mlo=0.725, AUC: same-mlo=0.652 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.535 \n",
      "\tAvg Val Loss: opp-mlo=0.639, AUC: opp-mlo=0.637\n",
      "Iter=245, avg train loss=0.602, \n",
      "\tAvg Val Loss: same-cc=0.636, AUC: same-cc=0.626 \n",
      "\tAvg Val Loss: same-mlo=0.688, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.633, AUC: opp-mlo=0.628\n",
      "Iter=250, avg train loss=0.670, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.632 \n",
      "\tAvg Val Loss: same-mlo=0.655, AUC: same-mlo=0.669 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.622\n",
      "Iter=255, avg train loss=0.613, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.626 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.675 \n",
      "\tAvg Val Loss opp-cc=0.653, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.593, AUC: opp-mlo=0.621\n",
      "Iter=260, avg train loss=0.679, \n",
      "\tAvg Val Loss: same-cc=0.606, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.586, AUC: same-mlo=0.683 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.551 \n",
      "\tAvg Val Loss: opp-mlo=0.567, AUC: opp-mlo=0.640\n",
      "Iter=265, avg train loss=0.680, \n",
      "\tAvg Val Loss: same-cc=0.612, AUC: same-cc=0.626 \n",
      "\tAvg Val Loss: same-mlo=0.567, AUC: same-mlo=0.687 \n",
      "\tAvg Val Loss opp-cc=0.655, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.575, AUC: opp-mlo=0.644\n",
      "Iter=270, avg train loss=0.722, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.619 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.675 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.624, AUC: opp-mlo=0.639\n",
      "Iter=275, avg train loss=0.635, \n",
      "\tAvg Val Loss: same-cc=0.663, AUC: same-cc=0.609 \n",
      "\tAvg Val Loss: same-mlo=0.617, AUC: same-mlo=0.675 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.645, AUC: opp-mlo=0.636\n",
      "Iter=280, avg train loss=0.605, \n",
      "\tAvg Val Loss: same-cc=0.678, AUC: same-cc=0.601 \n",
      "\tAvg Val Loss: same-mlo=0.625, AUC: same-mlo=0.677 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.577 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.648\n",
      "Best opp-cc model saved.\n",
      "Iter=285, avg train loss=0.644, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.595 \n",
      "\tAvg Val Loss: same-mlo=0.654, AUC: same-mlo=0.677 \n",
      "\tAvg Val Loss opp-cc=0.632, AUC: opp-cc=0.574 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.638\n",
      "Iter=290, avg train loss=0.599, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.679 \n",
      "\tAvg Val Loss opp-cc=0.623, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.642\n",
      "Iter=295, avg train loss=0.693, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.579 \n",
      "\tAvg Val Loss: same-mlo=0.585, AUC: same-mlo=0.692 \n",
      "\tAvg Val Loss opp-cc=0.630, AUC: opp-cc=0.579 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.640\n",
      "Best opp-cc model saved.\n",
      "Iter=300, avg train loss=0.663, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.584, AUC: same-mlo=0.696 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.577 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.643\n",
      "Iter=305, avg train loss=0.589, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.566 \n",
      "\tAvg Val Loss: same-mlo=0.608, AUC: same-mlo=0.697 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.619\n",
      "Iter=310, avg train loss=0.627, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.661, AUC: same-mlo=0.688 \n",
      "\tAvg Val Loss opp-cc=0.721, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=315, avg train loss=0.606, \n",
      "\tAvg Val Loss: same-cc=0.729, AUC: same-cc=0.579 \n",
      "\tAvg Val Loss: same-mlo=0.665, AUC: same-mlo=0.673 \n",
      "\tAvg Val Loss opp-cc=0.782, AUC: opp-cc=0.500 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.616\n",
      "Iter=320, avg train loss=0.608, \n",
      "\tAvg Val Loss: same-cc=0.766, AUC: same-cc=0.579 \n",
      "\tAvg Val Loss: same-mlo=0.698, AUC: same-mlo=0.666 \n",
      "\tAvg Val Loss opp-cc=0.829, AUC: opp-cc=0.492 \n",
      "\tAvg Val Loss: opp-mlo=0.717, AUC: opp-mlo=0.600\n",
      "Iter=325, avg train loss=0.589, \n",
      "\tAvg Val Loss: same-cc=0.817, AUC: same-cc=0.565 \n",
      "\tAvg Val Loss: same-mlo=0.773, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.860, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.758, AUC: opp-mlo=0.574\n",
      "Iter=330, avg train loss=0.652, \n",
      "\tAvg Val Loss: same-cc=0.775, AUC: same-cc=0.594 \n",
      "\tAvg Val Loss: same-mlo=0.751, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.817, AUC: opp-cc=0.478 \n",
      "\tAvg Val Loss: opp-mlo=0.748, AUC: opp-mlo=0.562\n",
      "Iter=335, avg train loss=0.595, \n",
      "\tAvg Val Loss: same-cc=0.743, AUC: same-cc=0.605 \n",
      "\tAvg Val Loss: same-mlo=0.695, AUC: same-mlo=0.645 \n",
      "\tAvg Val Loss opp-cc=0.780, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.719, AUC: opp-mlo=0.567\n",
      "Iter=340, avg train loss=0.600, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.616 \n",
      "\tAvg Val Loss: same-mlo=0.661, AUC: same-mlo=0.648 \n",
      "\tAvg Val Loss opp-cc=0.722, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.540\n",
      "Iter=345, avg train loss=0.614, \n",
      "\tAvg Val Loss: same-cc=0.662, AUC: same-cc=0.619 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.650 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.633, AUC: opp-mlo=0.529\n",
      "Iter=350, avg train loss=0.726, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.630 \n",
      "\tAvg Val Loss: same-mlo=0.587, AUC: same-mlo=0.649 \n",
      "\tAvg Val Loss opp-cc=0.664, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.518\n",
      "Iter=355, avg train loss=0.616, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.625 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.645, AUC: opp-mlo=0.525\n",
      "Iter=360, avg train loss=0.629, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.625 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.664 \n",
      "\tAvg Val Loss opp-cc=0.708, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.546\n",
      "Iter=365, avg train loss=0.591, \n",
      "\tAvg Val Loss: same-cc=0.695, AUC: same-cc=0.622 \n",
      "\tAvg Val Loss: same-mlo=0.692, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.718, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.686, AUC: opp-mlo=0.535\n",
      "Iter=370, avg train loss=0.642, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.621 \n",
      "\tAvg Val Loss: same-mlo=0.693, AUC: same-mlo=0.666 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.524\n",
      "Iter=375, avg train loss=0.614, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.625 \n",
      "\tAvg Val Loss: same-mlo=0.650, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.686, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.531\n",
      "Iter=380, avg train loss=0.627, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.623 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.641, AUC: opp-mlo=0.560\n",
      "Iter=385, avg train loss=0.612, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.632 \n",
      "\tAvg Val Loss: same-mlo=0.586, AUC: same-mlo=0.652 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.532 \n",
      "\tAvg Val Loss: opp-mlo=0.689, AUC: opp-mlo=0.577\n",
      "Iter=390, avg train loss=0.585, \n",
      "\tAvg Val Loss: same-cc=0.663, AUC: same-cc=0.648 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.732, AUC: opp-cc=0.532 \n",
      "\tAvg Val Loss: opp-mlo=0.689, AUC: opp-mlo=0.585\n",
      "Best same-cc model saved.\n",
      "Iter=395, avg train loss=0.574, \n",
      "\tAvg Val Loss: same-cc=0.646, AUC: same-cc=0.656 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.652 \n",
      "\tAvg Val Loss opp-cc=0.733, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.710, AUC: opp-mlo=0.564\n",
      "Best same-cc model saved.\n",
      "Iter=400, avg train loss=0.582, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.644 \n",
      "\tAvg Val Loss: same-mlo=0.639, AUC: same-mlo=0.657 \n",
      "\tAvg Val Loss opp-cc=0.737, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.696, AUC: opp-mlo=0.557\n",
      "Iter=405, avg train loss=0.594, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.625 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=0.745, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.544\n",
      "Iter=410, avg train loss=0.631, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.619 \n",
      "\tAvg Val Loss: same-mlo=0.702, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.765, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.714, AUC: opp-mlo=0.538\n",
      "Iter=415, avg train loss=0.558, \n",
      "\tAvg Val Loss: same-cc=0.669, AUC: same-cc=0.624 \n",
      "\tAvg Val Loss: same-mlo=0.718, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.745, AUC: opp-cc=0.518 \n",
      "\tAvg Val Loss: opp-mlo=0.718, AUC: opp-mlo=0.523\n",
      "Iter=420, avg train loss=0.493, \n",
      "\tAvg Val Loss: same-cc=0.726, AUC: same-cc=0.623 \n",
      "\tAvg Val Loss: same-mlo=0.694, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.766, AUC: opp-cc=0.504 \n",
      "\tAvg Val Loss: opp-mlo=0.685, AUC: opp-mlo=0.534\n",
      "Iter=425, avg train loss=0.639, \n",
      "\tAvg Val Loss: same-cc=0.733, AUC: same-cc=0.633 \n",
      "\tAvg Val Loss: same-mlo=0.717, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.778, AUC: opp-cc=0.516 \n",
      "\tAvg Val Loss: opp-mlo=0.746, AUC: opp-mlo=0.539\n",
      "Iter=430, avg train loss=0.644, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.634 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.736, AUC: opp-mlo=0.548\n",
      "Iter=435, avg train loss=0.506, \n",
      "\tAvg Val Loss: same-cc=0.668, AUC: same-cc=0.631 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.782, AUC: opp-mlo=0.577\n",
      "Iter=440, avg train loss=0.516, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.624 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.528 \n",
      "\tAvg Val Loss: opp-mlo=0.733, AUC: opp-mlo=0.589\n",
      "Iter=445, avg train loss=0.641, \n",
      "\tAvg Val Loss: same-cc=0.681, AUC: same-cc=0.624 \n",
      "\tAvg Val Loss: same-mlo=0.645, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.725, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.697, AUC: opp-mlo=0.593\n",
      "Iter=450, avg train loss=0.582, \n",
      "\tAvg Val Loss: same-cc=0.608, AUC: same-cc=0.637 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.659 \n",
      "\tAvg Val Loss opp-cc=0.681, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.591\n",
      "Iter=455, avg train loss=0.610, \n",
      "\tAvg Val Loss: same-cc=0.583, AUC: same-cc=0.640 \n",
      "\tAvg Val Loss: same-mlo=0.586, AUC: same-mlo=0.664 \n",
      "\tAvg Val Loss opp-cc=0.657, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.580\n",
      "Iter=460, avg train loss=0.500, \n",
      "\tAvg Val Loss: same-cc=0.557, AUC: same-cc=0.644 \n",
      "\tAvg Val Loss: same-mlo=0.582, AUC: same-mlo=0.657 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.566\n",
      "Iter=465, avg train loss=0.448, \n",
      "\tAvg Val Loss: same-cc=0.557, AUC: same-cc=0.659 \n",
      "\tAvg Val Loss: same-mlo=0.554, AUC: same-mlo=0.655 \n",
      "\tAvg Val Loss opp-cc=0.627, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.556\n",
      "Best same-cc model saved.\n",
      "Iter=470, avg train loss=0.757, \n",
      "\tAvg Val Loss: same-cc=0.577, AUC: same-cc=0.646 \n",
      "\tAvg Val Loss: same-mlo=0.576, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.664, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.632, AUC: opp-mlo=0.551\n",
      "Iter=475, avg train loss=0.517, \n",
      "\tAvg Val Loss: same-cc=0.639, AUC: same-cc=0.614 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.745, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.729, AUC: opp-mlo=0.542\n",
      "Iter=480, avg train loss=0.604, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.612 \n",
      "\tAvg Val Loss: same-mlo=0.649, AUC: same-mlo=0.616 \n",
      "\tAvg Val Loss opp-cc=0.743, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.788, AUC: opp-mlo=0.531\n",
      "Iter=485, avg train loss=0.578, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.586 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.528 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.544\n",
      "Iter=490, avg train loss=0.581, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.585 \n",
      "\tAvg Val Loss: same-mlo=0.629, AUC: same-mlo=0.621 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=495, avg train loss=0.530, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.567 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.689, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.534\n",
      "Iter=500, avg train loss=0.610, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=0.620, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.629, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.611, AUC: opp-mlo=0.531\n",
      "Iter=505, avg train loss=0.542, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.645, AUC: opp-cc=0.551 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.543\n",
      "Iter=510, avg train loss=0.526, \n",
      "\tAvg Val Loss: same-cc=0.642, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.608 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.611, AUC: opp-mlo=0.550\n",
      "Iter=515, avg train loss=0.603, \n",
      "\tAvg Val Loss: same-cc=0.597, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.545\n",
      "Iter=520, avg train loss=0.514, \n",
      "\tAvg Val Loss: same-cc=0.618, AUC: same-cc=0.610 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.539\n",
      "Iter=525, avg train loss=0.518, \n",
      "\tAvg Val Loss: same-cc=0.604, AUC: same-cc=0.631 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.646, AUC: opp-cc=0.556 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.539\n",
      "Iter=530, avg train loss=0.550, \n",
      "\tAvg Val Loss: same-cc=0.579, AUC: same-cc=0.647 \n",
      "\tAvg Val Loss: same-mlo=0.606, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.620, AUC: opp-cc=0.557 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.541\n",
      "Iter=535, avg train loss=0.629, \n",
      "\tAvg Val Loss: same-cc=0.569, AUC: same-cc=0.666 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.633, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.666, AUC: opp-mlo=0.540\n",
      "Best same-cc model saved.\n",
      "Iter=540, avg train loss=0.510, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.634 \n",
      "\tAvg Val Loss: same-mlo=0.693, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.735, AUC: opp-mlo=0.560\n",
      "Iter=545, avg train loss=0.472, \n",
      "\tAvg Val Loss: same-cc=0.651, AUC: same-cc=0.631 \n",
      "\tAvg Val Loss: same-mlo=0.651, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.665, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.712, AUC: opp-mlo=0.578\n",
      "Iter=550, avg train loss=0.611, \n",
      "\tAvg Val Loss: same-cc=0.660, AUC: same-cc=0.635 \n",
      "\tAvg Val Loss: same-mlo=0.640, AUC: same-mlo=0.646 \n",
      "\tAvg Val Loss opp-cc=0.635, AUC: opp-cc=0.550 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.547\n",
      "Iter=555, avg train loss=0.541, \n",
      "\tAvg Val Loss: same-cc=0.627, AUC: same-cc=0.629 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.634, AUC: opp-cc=0.557 \n",
      "\tAvg Val Loss: opp-mlo=0.671, AUC: opp-mlo=0.545\n",
      "Iter=560, avg train loss=0.542, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.632 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.641, AUC: opp-mlo=0.529\n",
      "Iter=565, avg train loss=0.521, \n",
      "\tAvg Val Loss: same-cc=0.616, AUC: same-cc=0.639 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.666 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.679, AUC: opp-mlo=0.545\n",
      "Iter=570, avg train loss=0.596, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.622 \n",
      "\tAvg Val Loss: same-mlo=0.636, AUC: same-mlo=0.669 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.528 \n",
      "\tAvg Val Loss: opp-mlo=0.751, AUC: opp-mlo=0.546\n",
      "Iter=575, avg train loss=0.505, \n",
      "\tAvg Val Loss: same-cc=0.672, AUC: same-cc=0.629 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.659 \n",
      "\tAvg Val Loss opp-cc=0.708, AUC: opp-cc=0.525 \n",
      "\tAvg Val Loss: opp-mlo=0.752, AUC: opp-mlo=0.523\n",
      "Iter=580, avg train loss=0.479, \n",
      "\tAvg Val Loss: same-cc=0.583, AUC: same-cc=0.652 \n",
      "\tAvg Val Loss: same-mlo=0.578, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.694, AUC: opp-mlo=0.521\n",
      "Iter=585, avg train loss=0.490, \n",
      "\tAvg Val Loss: same-cc=0.537, AUC: same-cc=0.681 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.543 \n",
      "\tAvg Val Loss: opp-mlo=0.634, AUC: opp-mlo=0.529\n",
      "Best same-cc model saved.\n",
      "Iter=590, avg train loss=0.603, \n",
      "\tAvg Val Loss: same-cc=0.550, AUC: same-cc=0.664 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.642 \n",
      "\tAvg Val Loss opp-cc=0.624, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.645, AUC: opp-mlo=0.505\n",
      "Iter=595, avg train loss=0.549, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.629, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.564 \n",
      "\tAvg Val Loss: opp-mlo=0.702, AUC: opp-mlo=0.512\n",
      "Iter=600, avg train loss=0.463, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.706, AUC: opp-cc=0.564 \n",
      "\tAvg Val Loss: opp-mlo=0.698, AUC: opp-mlo=0.516\n",
      "Iter=605, avg train loss=0.546, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.621 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.639 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.569 \n",
      "\tAvg Val Loss: opp-mlo=0.637, AUC: opp-mlo=0.541\n",
      "Iter=610, avg train loss=0.520, \n",
      "\tAvg Val Loss: same-cc=0.624, AUC: same-cc=0.614 \n",
      "\tAvg Val Loss: same-mlo=0.584, AUC: same-mlo=0.640 \n",
      "\tAvg Val Loss opp-cc=0.637, AUC: opp-cc=0.571 \n",
      "\tAvg Val Loss: opp-mlo=0.619, AUC: opp-mlo=0.548\n",
      "Iter=615, avg train loss=0.473, \n",
      "\tAvg Val Loss: same-cc=0.641, AUC: same-cc=0.638 \n",
      "\tAvg Val Loss: same-mlo=0.605, AUC: same-mlo=0.635 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.562 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.551\n",
      "Iter=620, avg train loss=0.447, \n",
      "\tAvg Val Loss: same-cc=0.615, AUC: same-cc=0.636 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.626, AUC: opp-mlo=0.562\n",
      "Iter=625, avg train loss=0.454, \n",
      "\tAvg Val Loss: same-cc=0.541, AUC: same-cc=0.659 \n",
      "\tAvg Val Loss: same-mlo=0.738, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.556\n",
      "Iter=630, avg train loss=0.509, \n",
      "\tAvg Val Loss: same-cc=0.528, AUC: same-cc=0.665 \n",
      "\tAvg Val Loss: same-mlo=0.757, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.662, AUC: opp-mlo=0.551\n",
      "Iter=635, avg train loss=0.479, \n",
      "\tAvg Val Loss: same-cc=0.545, AUC: same-cc=0.657 \n",
      "\tAvg Val Loss: same-mlo=0.801, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.722, AUC: opp-cc=0.528 \n",
      "\tAvg Val Loss: opp-mlo=0.739, AUC: opp-mlo=0.540\n",
      "Iter=640, avg train loss=0.465, \n",
      "\tAvg Val Loss: same-cc=0.577, AUC: same-cc=0.640 \n",
      "\tAvg Val Loss: same-mlo=0.758, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.753, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.771, AUC: opp-mlo=0.521\n",
      "Iter=645, avg train loss=0.443, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.628 \n",
      "\tAvg Val Loss: same-mlo=0.727, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.738, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.721, AUC: opp-mlo=0.523\n",
      "Iter=650, avg train loss=0.543, \n",
      "\tAvg Val Loss: same-cc=0.599, AUC: same-cc=0.649 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.697, AUC: opp-cc=0.504 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.526\n",
      "Iter=655, avg train loss=0.529, \n",
      "\tAvg Val Loss: same-cc=0.550, AUC: same-cc=0.656 \n",
      "\tAvg Val Loss: same-mlo=0.588, AUC: same-mlo=0.592 \n",
      "\tAvg Val Loss opp-cc=0.699, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.637, AUC: opp-mlo=0.532\n",
      "Iter=660, avg train loss=0.536, \n",
      "\tAvg Val Loss: same-cc=0.555, AUC: same-cc=0.680 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.654, AUC: opp-mlo=0.536\n",
      "Iter=665, avg train loss=0.520, \n",
      "\tAvg Val Loss: same-cc=0.583, AUC: same-cc=0.646 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.524\n",
      "Iter=670, avg train loss=0.499, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.609 \n",
      "\tAvg Val Loss: same-mlo=0.654, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.502 \n",
      "\tAvg Val Loss: opp-mlo=0.715, AUC: opp-mlo=0.516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=675, avg train loss=0.454, \n",
      "\tAvg Val Loss: same-cc=0.746, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.679, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.730, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.718, AUC: opp-mlo=0.525\n",
      "Iter=680, avg train loss=0.468, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.602 \n",
      "\tAvg Val Loss: same-mlo=0.665, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.659, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.695, AUC: opp-mlo=0.526\n",
      "Iter=685, avg train loss=0.478, \n",
      "\tAvg Val Loss: same-cc=0.588, AUC: same-cc=0.605 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.646, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.658, AUC: opp-mlo=0.539\n",
      "Iter=690, avg train loss=0.434, \n",
      "\tAvg Val Loss: same-cc=0.561, AUC: same-cc=0.613 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.588 \n",
      "\tAvg Val Loss opp-cc=0.646, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.649, AUC: opp-mlo=0.542\n",
      "Iter=695, avg train loss=0.470, \n",
      "\tAvg Val Loss: same-cc=0.558, AUC: same-cc=0.626 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.543 \n",
      "\tAvg Val Loss: opp-mlo=0.656, AUC: opp-mlo=0.530\n",
      "Iter=700, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.591, AUC: same-cc=0.629 \n",
      "\tAvg Val Loss: same-mlo=0.606, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.706, AUC: opp-cc=0.531 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.528\n",
      "Iter=705, avg train loss=0.389, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.626 \n",
      "\tAvg Val Loss: same-mlo=0.691, AUC: same-mlo=0.581 \n",
      "\tAvg Val Loss opp-cc=0.772, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.708, AUC: opp-mlo=0.527\n",
      "Iter=710, avg train loss=0.508, \n",
      "\tAvg Val Loss: same-cc=0.638, AUC: same-cc=0.631 \n",
      "\tAvg Val Loss: same-mlo=0.719, AUC: same-mlo=0.578 \n",
      "\tAvg Val Loss opp-cc=0.730, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.715, AUC: opp-mlo=0.541\n",
      "Iter=715, avg train loss=0.527, \n",
      "\tAvg Val Loss: same-cc=0.584, AUC: same-cc=0.650 \n",
      "\tAvg Val Loss: same-mlo=0.657, AUC: same-mlo=0.587 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.689, AUC: opp-mlo=0.528\n",
      "Iter=720, avg train loss=0.415, \n",
      "\tAvg Val Loss: same-cc=0.551, AUC: same-cc=0.666 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.590 \n",
      "\tAvg Val Loss opp-cc=0.657, AUC: opp-cc=0.551 \n",
      "\tAvg Val Loss: opp-mlo=0.669, AUC: opp-mlo=0.525\n",
      "Iter=725, avg train loss=0.627, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.651 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.588 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.498\n",
      "Iter=730, avg train loss=0.392, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.634 \n",
      "\tAvg Val Loss: same-mlo=0.631, AUC: same-mlo=0.593 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.729, AUC: opp-mlo=0.498\n",
      "Iter=735, avg train loss=0.478, \n",
      "\tAvg Val Loss: same-cc=0.592, AUC: same-cc=0.655 \n",
      "\tAvg Val Loss: same-mlo=0.655, AUC: same-mlo=0.616 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.712, AUC: opp-mlo=0.514\n",
      "Iter=740, avg train loss=0.463, \n",
      "\tAvg Val Loss: same-cc=0.541, AUC: same-cc=0.692 \n",
      "\tAvg Val Loss: same-mlo=0.667, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.705, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.683, AUC: opp-mlo=0.512\n",
      "Best same-cc model saved.\n",
      "Iter=745, avg train loss=0.485, \n",
      "\tAvg Val Loss: same-cc=0.539, AUC: same-cc=0.684 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.605 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.692, AUC: opp-mlo=0.492\n",
      "Iter=750, avg train loss=0.438, \n",
      "\tAvg Val Loss: same-cc=0.572, AUC: same-cc=0.674 \n",
      "\tAvg Val Loss: same-mlo=0.648, AUC: same-mlo=0.607 \n",
      "\tAvg Val Loss opp-cc=0.681, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.742, AUC: opp-mlo=0.487\n",
      "Iter=755, avg train loss=0.375, \n",
      "\tAvg Val Loss: same-cc=0.597, AUC: same-cc=0.678 \n",
      "\tAvg Val Loss: same-mlo=0.623, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.774, AUC: opp-mlo=0.495\n",
      "Iter=760, avg train loss=0.441, \n",
      "\tAvg Val Loss: same-cc=0.553, AUC: same-cc=0.684 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.607 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.796, AUC: opp-mlo=0.504\n",
      "Iter=765, avg train loss=0.501, \n",
      "\tAvg Val Loss: same-cc=0.512, AUC: same-cc=0.701 \n",
      "\tAvg Val Loss: same-mlo=0.567, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.639, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.745, AUC: opp-mlo=0.502\n",
      "Best same-cc model saved.\n",
      "Iter=770, avg train loss=0.418, \n",
      "\tAvg Val Loss: same-cc=0.551, AUC: same-cc=0.647 \n",
      "\tAvg Val Loss: same-mlo=0.582, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.537 \n",
      "\tAvg Val Loss: opp-mlo=0.742, AUC: opp-mlo=0.501\n",
      "Iter=775, avg train loss=0.438, \n",
      "\tAvg Val Loss: same-cc=0.671, AUC: same-cc=0.619 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.749, AUC: opp-mlo=0.501\n",
      "Iter=780, avg train loss=0.462, \n",
      "\tAvg Val Loss: same-cc=0.749, AUC: same-cc=0.599 \n",
      "\tAvg Val Loss: same-mlo=0.708, AUC: same-mlo=0.587 \n",
      "\tAvg Val Loss opp-cc=0.751, AUC: opp-cc=0.525 \n",
      "\tAvg Val Loss: opp-mlo=0.789, AUC: opp-mlo=0.508\n",
      "Iter=785, avg train loss=0.526, \n",
      "\tAvg Val Loss: same-cc=0.549, AUC: same-cc=0.626 \n",
      "\tAvg Val Loss: same-mlo=0.608, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.643, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.540\n",
      "Iter=790, avg train loss=0.352, \n",
      "\tAvg Val Loss: same-cc=0.568, AUC: same-cc=0.625 \n",
      "\tAvg Val Loss: same-mlo=0.565, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.634, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.652, AUC: opp-mlo=0.535\n",
      "Iter=795, avg train loss=0.483, \n",
      "\tAvg Val Loss: same-cc=0.555, AUC: same-cc=0.624 \n",
      "\tAvg Val Loss: same-mlo=0.563, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.556 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.528\n",
      "Iter=800, avg train loss=0.571, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.613 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.762, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.739, AUC: opp-mlo=0.506\n",
      "Iter=805, avg train loss=0.390, \n",
      "\tAvg Val Loss: same-cc=0.791, AUC: same-cc=0.597 \n",
      "\tAvg Val Loss: same-mlo=0.701, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.824, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.864, AUC: opp-mlo=0.518\n",
      "Iter=810, avg train loss=0.415, \n",
      "\tAvg Val Loss: same-cc=0.641, AUC: same-cc=0.622 \n",
      "\tAvg Val Loss: same-mlo=0.605, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.706, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.556\n",
      "Iter=815, avg train loss=0.366, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.623 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.556 \n",
      "\tAvg Val Loss: opp-mlo=0.655, AUC: opp-mlo=0.579\n",
      "Iter=820, avg train loss=0.449, \n",
      "\tAvg Val Loss: same-cc=0.600, AUC: same-cc=0.627 \n",
      "\tAvg Val Loss: same-mlo=0.567, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.543 \n",
      "\tAvg Val Loss: opp-mlo=0.666, AUC: opp-mlo=0.577\n",
      "Iter=825, avg train loss=0.450, \n",
      "\tAvg Val Loss: same-cc=0.577, AUC: same-cc=0.616 \n",
      "\tAvg Val Loss: same-mlo=0.560, AUC: same-mlo=0.640 \n",
      "\tAvg Val Loss opp-cc=0.635, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.682, AUC: opp-mlo=0.545\n",
      "Iter=830, avg train loss=0.493, \n",
      "\tAvg Val Loss: same-cc=0.679, AUC: same-cc=0.616 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.639 \n",
      "\tAvg Val Loss opp-cc=0.659, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.823, AUC: opp-mlo=0.496\n",
      "Iter=835, avg train loss=0.471, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.615 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.553 \n",
      "\tAvg Val Loss: opp-mlo=0.852, AUC: opp-mlo=0.490\n",
      "Iter=840, avg train loss=0.460, \n",
      "\tAvg Val Loss: same-cc=0.626, AUC: same-cc=0.623 \n",
      "\tAvg Val Loss: same-mlo=0.627, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.655, AUC: opp-cc=0.579 \n",
      "\tAvg Val Loss: opp-mlo=0.836, AUC: opp-mlo=0.492\n",
      "Iter=845, avg train loss=0.394, \n",
      "\tAvg Val Loss: same-cc=0.569, AUC: same-cc=0.645 \n",
      "\tAvg Val Loss: same-mlo=0.579, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.583 \n",
      "\tAvg Val Loss: opp-mlo=0.719, AUC: opp-mlo=0.500\n",
      "Best opp-cc model saved.\n",
      "Iter=850, avg train loss=0.470, \n",
      "\tAvg Val Loss: same-cc=0.598, AUC: same-cc=0.639 \n",
      "\tAvg Val Loss: same-mlo=0.580, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.569 \n",
      "\tAvg Val Loss: opp-mlo=0.674, AUC: opp-mlo=0.545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=855, avg train loss=0.367, \n",
      "\tAvg Val Loss: same-cc=0.706, AUC: same-cc=0.618 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.573 \n",
      "\tAvg Val Loss: opp-mlo=0.666, AUC: opp-mlo=0.558\n",
      "Iter=860, avg train loss=0.470, \n",
      "\tAvg Val Loss: same-cc=0.869, AUC: same-cc=0.587 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.730, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.704, AUC: opp-mlo=0.575\n",
      "Iter=865, avg train loss=0.338, \n",
      "\tAvg Val Loss: same-cc=0.582, AUC: same-cc=0.631 \n",
      "\tAvg Val Loss: same-mlo=0.608, AUC: same-mlo=0.608 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.562 \n",
      "\tAvg Val Loss: opp-mlo=0.669, AUC: opp-mlo=0.592\n",
      "Iter=870, avg train loss=0.413, \n",
      "\tAvg Val Loss: same-cc=0.529, AUC: same-cc=0.675 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.670, AUC: opp-mlo=0.579\n",
      "Iter=875, avg train loss=0.430, \n",
      "\tAvg Val Loss: same-cc=0.532, AUC: same-cc=0.652 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.639, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.588\n",
      "Iter=880, avg train loss=0.319, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.628 \n",
      "\tAvg Val Loss: same-mlo=0.601, AUC: same-mlo=0.634 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.592\n",
      "Iter=885, avg train loss=0.344, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.616 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.557 \n",
      "\tAvg Val Loss: opp-mlo=0.705, AUC: opp-mlo=0.562\n",
      "Iter=890, avg train loss=0.388, \n",
      "\tAvg Val Loss: same-cc=0.594, AUC: same-cc=0.636 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.794, AUC: opp-mlo=0.505\n",
      "Iter=895, avg train loss=0.292, \n",
      "\tAvg Val Loss: same-cc=0.605, AUC: same-cc=0.630 \n",
      "\tAvg Val Loss: same-mlo=0.614, AUC: same-mlo=0.628 \n",
      "\tAvg Val Loss opp-cc=0.708, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.776, AUC: opp-mlo=0.492\n",
      "Iter=900, avg train loss=0.425, \n",
      "\tAvg Val Loss: same-cc=0.588, AUC: same-cc=0.642 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.715, AUC: opp-mlo=0.477\n",
      "Iter=905, avg train loss=0.389, \n",
      "\tAvg Val Loss: same-cc=0.560, AUC: same-cc=0.661 \n",
      "\tAvg Val Loss: same-mlo=0.650, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.546 \n",
      "\tAvg Val Loss: opp-mlo=0.699, AUC: opp-mlo=0.485\n",
      "Iter=910, avg train loss=0.352, \n",
      "\tAvg Val Loss: same-cc=0.572, AUC: same-cc=0.669 \n",
      "\tAvg Val Loss: same-mlo=0.640, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.695, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.746, AUC: opp-mlo=0.514\n",
      "Iter=915, avg train loss=0.346, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.670 \n",
      "\tAvg Val Loss: same-mlo=0.649, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.790, AUC: opp-mlo=0.522\n",
      "Iter=920, avg train loss=0.377, \n",
      "\tAvg Val Loss: same-cc=0.679, AUC: same-cc=0.653 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.608 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.750, AUC: opp-mlo=0.525\n",
      "Iter=925, avg train loss=0.386, \n",
      "\tAvg Val Loss: same-cc=0.599, AUC: same-cc=0.666 \n",
      "\tAvg Val Loss: same-mlo=0.657, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.726, AUC: opp-mlo=0.500\n",
      "Iter=930, avg train loss=0.386, \n",
      "\tAvg Val Loss: same-cc=0.545, AUC: same-cc=0.679 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.595 \n",
      "\tAvg Val Loss opp-cc=0.703, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.729, AUC: opp-mlo=0.487\n",
      "Iter=935, avg train loss=0.380, \n",
      "\tAvg Val Loss: same-cc=0.563, AUC: same-cc=0.658 \n",
      "\tAvg Val Loss: same-mlo=0.648, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.727, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.796, AUC: opp-mlo=0.465\n",
      "Iter=940, avg train loss=0.386, \n",
      "\tAvg Val Loss: same-cc=0.560, AUC: same-cc=0.671 \n",
      "\tAvg Val Loss: same-mlo=0.627, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.671, AUC: opp-cc=0.532 \n",
      "\tAvg Val Loss: opp-mlo=0.769, AUC: opp-mlo=0.457\n",
      "Iter=945, avg train loss=0.265, \n",
      "\tAvg Val Loss: same-cc=0.581, AUC: same-cc=0.645 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.595 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.776, AUC: opp-mlo=0.445\n",
      "Iter=950, avg train loss=0.308, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.628 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.534 \n",
      "\tAvg Val Loss: opp-mlo=0.777, AUC: opp-mlo=0.462\n",
      "Iter=955, avg train loss=0.387, \n",
      "\tAvg Val Loss: same-cc=0.756, AUC: same-cc=0.615 \n",
      "\tAvg Val Loss: same-mlo=0.649, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.718, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.844, AUC: opp-mlo=0.447\n",
      "Iter=960, avg train loss=0.308, \n",
      "\tAvg Val Loss: same-cc=0.704, AUC: same-cc=0.626 \n",
      "\tAvg Val Loss: same-mlo=0.679, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.730, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.809, AUC: opp-mlo=0.451\n",
      "Iter=965, avg train loss=0.248, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.634 \n",
      "\tAvg Val Loss: same-mlo=0.650, AUC: same-mlo=0.590 \n",
      "\tAvg Val Loss opp-cc=0.695, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.814, AUC: opp-mlo=0.464\n",
      "Iter=970, avg train loss=0.306, \n",
      "\tAvg Val Loss: same-cc=0.601, AUC: same-cc=0.653 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.581 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.556 \n",
      "\tAvg Val Loss: opp-mlo=0.760, AUC: opp-mlo=0.490\n",
      "Iter=975, avg train loss=0.322, \n",
      "\tAvg Val Loss: same-cc=0.630, AUC: same-cc=0.651 \n",
      "\tAvg Val Loss: same-mlo=0.626, AUC: same-mlo=0.573 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.551 \n",
      "\tAvg Val Loss: opp-mlo=0.869, AUC: opp-mlo=0.505\n",
      "Iter=980, avg train loss=0.341, \n",
      "\tAvg Val Loss: same-cc=0.628, AUC: same-cc=0.649 \n",
      "\tAvg Val Loss: same-mlo=0.690, AUC: same-mlo=0.566 \n",
      "\tAvg Val Loss opp-cc=0.712, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.888, AUC: opp-mlo=0.507\n",
      "Iter=985, avg train loss=0.372, \n",
      "\tAvg Val Loss: same-cc=0.612, AUC: same-cc=0.638 \n",
      "\tAvg Val Loss: same-mlo=0.758, AUC: same-mlo=0.565 \n",
      "\tAvg Val Loss opp-cc=0.706, AUC: opp-cc=0.553 \n",
      "\tAvg Val Loss: opp-mlo=0.757, AUC: opp-mlo=0.518\n",
      "Iter=990, avg train loss=0.311, \n",
      "\tAvg Val Loss: same-cc=0.593, AUC: same-cc=0.630 \n",
      "\tAvg Val Loss: same-mlo=0.754, AUC: same-mlo=0.571 \n",
      "\tAvg Val Loss opp-cc=0.712, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.728, AUC: opp-mlo=0.531\n",
      "Iter=995, avg train loss=0.271, \n",
      "\tAvg Val Loss: same-cc=0.592, AUC: same-cc=0.628 \n",
      "\tAvg Val Loss: same-mlo=0.695, AUC: same-mlo=0.565 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.748, AUC: opp-mlo=0.518\n",
      "Iter=1000, avg train loss=0.326, \n",
      "\tAvg Val Loss: same-cc=0.562, AUC: same-cc=0.642 \n",
      "\tAvg Val Loss: same-mlo=0.666, AUC: same-mlo=0.556 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.551 \n",
      "\tAvg Val Loss: opp-mlo=0.758, AUC: opp-mlo=0.513\n",
      "Iter=1005, avg train loss=0.363, \n",
      "\tAvg Val Loss: same-cc=0.588, AUC: same-cc=0.637 \n",
      "\tAvg Val Loss: same-mlo=0.709, AUC: same-mlo=0.575 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.816, AUC: opp-mlo=0.503\n",
      "Iter=1010, avg train loss=0.394, \n",
      "\tAvg Val Loss: same-cc=0.732, AUC: same-cc=0.637 \n",
      "\tAvg Val Loss: same-mlo=0.825, AUC: same-mlo=0.577 \n",
      "\tAvg Val Loss opp-cc=0.745, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.847, AUC: opp-mlo=0.472\n",
      "Iter=1015, avg train loss=0.231, \n",
      "\tAvg Val Loss: same-cc=0.659, AUC: same-cc=0.646 \n",
      "\tAvg Val Loss: same-mlo=0.807, AUC: same-mlo=0.584 \n",
      "\tAvg Val Loss opp-cc=0.723, AUC: opp-cc=0.543 \n",
      "\tAvg Val Loss: opp-mlo=0.778, AUC: opp-mlo=0.452\n",
      "Iter=1020, avg train loss=0.318, \n",
      "\tAvg Val Loss: same-cc=0.541, AUC: same-cc=0.671 \n",
      "\tAvg Val Loss: same-mlo=0.697, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.721, AUC: opp-cc=0.548 \n",
      "\tAvg Val Loss: opp-mlo=0.739, AUC: opp-mlo=0.478\n",
      "Iter=1025, avg train loss=0.308, \n",
      "\tAvg Val Loss: same-cc=0.543, AUC: same-cc=0.671 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.565 \n",
      "\tAvg Val Loss opp-cc=0.701, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.740, AUC: opp-mlo=0.484\n",
      "Iter=1030, avg train loss=0.326, \n",
      "\tAvg Val Loss: same-cc=0.587, AUC: same-cc=0.656 \n",
      "\tAvg Val Loss: same-mlo=0.716, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.728, AUC: opp-cc=0.553 \n",
      "\tAvg Val Loss: opp-mlo=0.747, AUC: opp-mlo=0.507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1035, avg train loss=0.265, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.644 \n",
      "\tAvg Val Loss: same-mlo=0.722, AUC: same-mlo=0.551 \n",
      "\tAvg Val Loss opp-cc=0.728, AUC: opp-cc=0.534 \n",
      "\tAvg Val Loss: opp-mlo=0.739, AUC: opp-mlo=0.529\n",
      "Iter=1040, avg train loss=0.353, \n",
      "\tAvg Val Loss: same-cc=0.753, AUC: same-cc=0.632 \n",
      "\tAvg Val Loss: same-mlo=0.719, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.732, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.730, AUC: opp-mlo=0.529\n",
      "Best models loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training: same-cc=0.600, same-mlo=0.554, opp-cc=0.527, opp-mlo=0.612\n",
      "Max-Score Based AUC After Training: 0.606, Mean-Score Based AUC After Training: 0.595\n",
      "\n",
      "\n",
      "\n",
      "========== Fold 4 ==========\n",
      "Test AUC at start: same-cc=0.582, same-mlo=0.573, opp-cc=0.579, opp-mlo=0.565\n",
      "Max-Score Based AUC Before Training: 0.587, Mean-Score Based AUC Before Training: 0.577\n",
      "Iter=5, avg train loss=1.096, \n",
      "\tAvg Val Loss: same-cc=0.579, AUC: same-cc=0.541 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.508 \n",
      "\tAvg Val Loss opp-cc=0.578, AUC: opp-cc=0.553 \n",
      "\tAvg Val Loss: opp-mlo=0.596, AUC: opp-mlo=0.506\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=10, avg train loss=0.836, \n",
      "\tAvg Val Loss: same-cc=0.580, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.582, AUC: same-mlo=0.539 \n",
      "\tAvg Val Loss opp-cc=0.577, AUC: opp-cc=0.580 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.520\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=15, avg train loss=0.765, \n",
      "\tAvg Val Loss: same-cc=0.580, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.587, AUC: same-mlo=0.530 \n",
      "\tAvg Val Loss opp-cc=0.588, AUC: opp-cc=0.597 \n",
      "\tAvg Val Loss: opp-mlo=0.596, AUC: opp-mlo=0.501\n",
      "Best same-cc model saved.\n",
      "Best opp-cc model saved.\n",
      "Iter=20, avg train loss=0.750, \n",
      "\tAvg Val Loss: same-cc=0.578, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.575, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.583, AUC: opp-cc=0.586 \n",
      "\tAvg Val Loss: opp-mlo=0.588, AUC: opp-mlo=0.505\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Iter=25, avg train loss=0.918, \n",
      "\tAvg Val Loss: same-cc=0.579, AUC: same-cc=0.573 \n",
      "\tAvg Val Loss: same-mlo=0.573, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.590, AUC: opp-cc=0.588 \n",
      "\tAvg Val Loss: opp-mlo=0.585, AUC: opp-mlo=0.520\n",
      "Best same-mlo model saved.\n",
      "Iter=30, avg train loss=0.740, \n",
      "\tAvg Val Loss: same-cc=0.579, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.568, AUC: same-mlo=0.605 \n",
      "\tAvg Val Loss opp-cc=0.595, AUC: opp-cc=0.579 \n",
      "\tAvg Val Loss: opp-mlo=0.584, AUC: opp-mlo=0.516\n",
      "Best same-mlo model saved.\n",
      "Iter=35, avg train loss=0.656, \n",
      "\tAvg Val Loss: same-cc=0.577, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.560, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.593, AUC: opp-cc=0.582 \n",
      "\tAvg Val Loss: opp-mlo=0.587, AUC: opp-mlo=0.520\n",
      "Iter=40, avg train loss=0.714, \n",
      "\tAvg Val Loss: same-cc=0.590, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.556, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.597, AUC: opp-cc=0.579 \n",
      "\tAvg Val Loss: opp-mlo=0.580, AUC: opp-mlo=0.543\n",
      "Best opp-mlo model saved.\n",
      "Iter=45, avg train loss=0.724, \n",
      "\tAvg Val Loss: same-cc=0.577, AUC: same-cc=0.550 \n",
      "\tAvg Val Loss: same-mlo=0.553, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.594, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.576, AUC: opp-mlo=0.563\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=50, avg train loss=0.892, \n",
      "\tAvg Val Loss: same-cc=0.599, AUC: same-cc=0.557 \n",
      "\tAvg Val Loss: same-mlo=0.565, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.614, AUC: opp-cc=0.564 \n",
      "\tAvg Val Loss: opp-mlo=0.597, AUC: opp-mlo=0.560\n",
      "Best same-mlo model saved.\n",
      "Iter=55, avg train loss=0.614, \n",
      "\tAvg Val Loss: same-cc=0.599, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.578, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.614, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.569\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=60, avg train loss=0.672, \n",
      "\tAvg Val Loss: same-cc=0.585, AUC: same-cc=0.563 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.635 \n",
      "\tAvg Val Loss opp-cc=0.602, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.609, AUC: opp-mlo=0.566\n",
      "Best same-mlo model saved.\n",
      "Iter=65, avg train loss=0.663, \n",
      "\tAvg Val Loss: same-cc=0.586, AUC: same-cc=0.567 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.605, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.596, AUC: opp-mlo=0.564\n",
      "Best same-mlo model saved.\n",
      "Iter=70, avg train loss=0.690, \n",
      "\tAvg Val Loss: same-cc=0.582, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=0.565, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.603, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.560\n",
      "Best same-cc model saved.\n",
      "Iter=75, avg train loss=0.734, \n",
      "\tAvg Val Loss: same-cc=0.580, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.564, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.612, AUC: opp-cc=0.543 \n",
      "\tAvg Val Loss: opp-mlo=0.589, AUC: opp-mlo=0.570\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=80, avg train loss=0.647, \n",
      "\tAvg Val Loss: same-cc=0.582, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.566, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.617, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.562\n",
      "Best same-cc model saved.\n",
      "Iter=85, avg train loss=0.617, \n",
      "\tAvg Val Loss: same-cc=0.570, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.568, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.607, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.603, AUC: opp-mlo=0.545\n",
      "Iter=90, avg train loss=0.657, \n",
      "\tAvg Val Loss: same-cc=0.574, AUC: same-cc=0.592 \n",
      "\tAvg Val Loss: same-mlo=0.565, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.605, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.530\n",
      "Best same-cc model saved.\n",
      "Iter=95, avg train loss=0.556, \n",
      "\tAvg Val Loss: same-cc=0.581, AUC: same-cc=0.577 \n",
      "\tAvg Val Loss: same-mlo=0.565, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.597, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.590, AUC: opp-mlo=0.526\n",
      "Iter=100, avg train loss=0.745, \n",
      "\tAvg Val Loss: same-cc=0.584, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.561, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.607, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.593, AUC: opp-mlo=0.539\n",
      "Iter=105, avg train loss=0.777, \n",
      "\tAvg Val Loss: same-cc=0.576, AUC: same-cc=0.595 \n",
      "\tAvg Val Loss: same-mlo=0.564, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.612, AUC: opp-cc=0.512 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.547\n",
      "Best same-cc model saved.\n",
      "Iter=110, avg train loss=0.673, \n",
      "\tAvg Val Loss: same-cc=0.579, AUC: same-cc=0.601 \n",
      "\tAvg Val Loss: same-mlo=0.566, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.619, AUC: opp-cc=0.508 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.540\n",
      "Best same-cc model saved.\n",
      "Iter=115, avg train loss=0.663, \n",
      "\tAvg Val Loss: same-cc=0.595, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.576, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.621, AUC: opp-cc=0.504 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.529\n",
      "Best same-cc model saved.\n",
      "Iter=120, avg train loss=0.727, \n",
      "\tAvg Val Loss: same-cc=0.602, AUC: same-cc=0.621 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.620, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.613, AUC: opp-mlo=0.520\n",
      "Best same-cc model saved.\n",
      "Iter=125, avg train loss=0.627, \n",
      "\tAvg Val Loss: same-cc=0.604, AUC: same-cc=0.632 \n",
      "\tAvg Val Loss: same-mlo=0.591, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.628, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.514\n",
      "Best same-cc model saved.\n",
      "Iter=130, avg train loss=0.627, \n",
      "\tAvg Val Loss: same-cc=0.612, AUC: same-cc=0.638 \n",
      "\tAvg Val Loss: same-mlo=0.596, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.621, AUC: opp-mlo=0.499\n",
      "Best same-cc model saved.\n",
      "Iter=135, avg train loss=0.703, \n",
      "\tAvg Val Loss: same-cc=0.585, AUC: same-cc=0.640 \n",
      "\tAvg Val Loss: same-mlo=0.588, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.627, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.607, AUC: opp-mlo=0.497\n",
      "Best same-cc model saved.\n",
      "Iter=140, avg train loss=0.721, \n",
      "\tAvg Val Loss: same-cc=0.597, AUC: same-cc=0.639 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.637, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.604, AUC: opp-mlo=0.486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=145, avg train loss=0.741, \n",
      "\tAvg Val Loss: same-cc=0.598, AUC: same-cc=0.648 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.461 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.494\n",
      "Best same-cc model saved.\n",
      "Iter=150, avg train loss=0.617, \n",
      "\tAvg Val Loss: same-cc=0.588, AUC: same-cc=0.656 \n",
      "\tAvg Val Loss: same-mlo=0.606, AUC: same-mlo=0.608 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.451 \n",
      "\tAvg Val Loss: opp-mlo=0.633, AUC: opp-mlo=0.513\n",
      "Best same-cc model saved.\n",
      "Iter=155, avg train loss=0.686, \n",
      "\tAvg Val Loss: same-cc=0.596, AUC: same-cc=0.667 \n",
      "\tAvg Val Loss: same-mlo=0.617, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.659, AUC: opp-cc=0.458 \n",
      "\tAvg Val Loss: opp-mlo=0.650, AUC: opp-mlo=0.504\n",
      "Best same-cc model saved.\n",
      "Iter=160, avg train loss=0.680, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.669 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.664, AUC: opp-cc=0.455 \n",
      "\tAvg Val Loss: opp-mlo=0.647, AUC: opp-mlo=0.506\n",
      "Best same-cc model saved.\n",
      "Iter=165, avg train loss=0.696, \n",
      "\tAvg Val Loss: same-cc=0.640, AUC: same-cc=0.669 \n",
      "\tAvg Val Loss: same-mlo=0.640, AUC: same-mlo=0.608 \n",
      "\tAvg Val Loss opp-cc=0.678, AUC: opp-cc=0.458 \n",
      "\tAvg Val Loss: opp-mlo=0.693, AUC: opp-mlo=0.520\n",
      "Iter=170, avg train loss=0.624, \n",
      "\tAvg Val Loss: same-cc=0.617, AUC: same-cc=0.682 \n",
      "\tAvg Val Loss: same-mlo=0.624, AUC: same-mlo=0.599 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.457 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.512\n",
      "Best same-cc model saved.\n",
      "Iter=175, avg train loss=0.621, \n",
      "\tAvg Val Loss: same-cc=0.598, AUC: same-cc=0.657 \n",
      "\tAvg Val Loss: same-mlo=0.608, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.434 \n",
      "\tAvg Val Loss: opp-mlo=0.681, AUC: opp-mlo=0.506\n",
      "Iter=180, avg train loss=0.639, \n",
      "\tAvg Val Loss: same-cc=0.593, AUC: same-cc=0.643 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.411 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.504\n",
      "Iter=185, avg train loss=0.676, \n",
      "\tAvg Val Loss: same-cc=0.589, AUC: same-cc=0.640 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.654, AUC: opp-cc=0.415 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.504\n",
      "Iter=190, avg train loss=0.702, \n",
      "\tAvg Val Loss: same-cc=0.616, AUC: same-cc=0.638 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.651, AUC: opp-cc=0.420 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.500\n",
      "Iter=195, avg train loss=0.577, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.646 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.573 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.450 \n",
      "\tAvg Val Loss: opp-mlo=0.683, AUC: opp-mlo=0.504\n",
      "Iter=200, avg train loss=0.654, \n",
      "\tAvg Val Loss: same-cc=0.628, AUC: same-cc=0.640 \n",
      "\tAvg Val Loss: same-mlo=0.626, AUC: same-mlo=0.566 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.455 \n",
      "\tAvg Val Loss: opp-mlo=0.670, AUC: opp-mlo=0.498\n",
      "Iter=205, avg train loss=0.671, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.639 \n",
      "\tAvg Val Loss: same-mlo=0.634, AUC: same-mlo=0.563 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.459 \n",
      "\tAvg Val Loss: opp-mlo=0.670, AUC: opp-mlo=0.501\n",
      "Iter=210, avg train loss=0.667, \n",
      "\tAvg Val Loss: same-cc=0.592, AUC: same-cc=0.635 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.561 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.451 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.498\n",
      "Iter=215, avg train loss=0.602, \n",
      "\tAvg Val Loss: same-cc=0.578, AUC: same-cc=0.642 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.558 \n",
      "\tAvg Val Loss opp-cc=0.632, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.652, AUC: opp-mlo=0.496\n",
      "Iter=220, avg train loss=0.605, \n",
      "\tAvg Val Loss: same-cc=0.589, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.556 \n",
      "\tAvg Val Loss opp-cc=0.633, AUC: opp-cc=0.457 \n",
      "\tAvg Val Loss: opp-mlo=0.663, AUC: opp-mlo=0.492\n",
      "Iter=225, avg train loss=0.642, \n",
      "\tAvg Val Loss: same-cc=0.588, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.639, AUC: opp-cc=0.452 \n",
      "\tAvg Val Loss: opp-mlo=0.687, AUC: opp-mlo=0.491\n",
      "Iter=230, avg train loss=0.608, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.642, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.440 \n",
      "\tAvg Val Loss: opp-mlo=0.720, AUC: opp-mlo=0.479\n",
      "Iter=235, avg train loss=0.616, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.595 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.455 \n",
      "\tAvg Val Loss: opp-mlo=0.714, AUC: opp-mlo=0.485\n",
      "Iter=240, avg train loss=0.634, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.549 \n",
      "\tAvg Val Loss opp-cc=0.656, AUC: opp-cc=0.451 \n",
      "\tAvg Val Loss: opp-mlo=0.688, AUC: opp-mlo=0.491\n",
      "Iter=245, avg train loss=0.677, \n",
      "\tAvg Val Loss: same-cc=0.617, AUC: same-cc=0.603 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.544 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.441 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.491\n",
      "Iter=250, avg train loss=0.634, \n",
      "\tAvg Val Loss: same-cc=0.611, AUC: same-cc=0.609 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.456 \n",
      "\tAvg Val Loss: opp-mlo=0.670, AUC: opp-mlo=0.493\n",
      "Iter=255, avg train loss=0.656, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.607 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.553 \n",
      "\tAvg Val Loss opp-cc=0.680, AUC: opp-cc=0.458 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.491\n",
      "Iter=260, avg train loss=0.616, \n",
      "\tAvg Val Loss: same-cc=0.594, AUC: same-cc=0.613 \n",
      "\tAvg Val Loss: same-mlo=0.619, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.456 \n",
      "\tAvg Val Loss: opp-mlo=0.670, AUC: opp-mlo=0.492\n",
      "Iter=265, avg train loss=0.653, \n",
      "\tAvg Val Loss: same-cc=0.583, AUC: same-cc=0.605 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.448 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.479\n",
      "Iter=270, avg train loss=0.633, \n",
      "\tAvg Val Loss: same-cc=0.580, AUC: same-cc=0.617 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.461 \n",
      "\tAvg Val Loss: opp-mlo=0.677, AUC: opp-mlo=0.481\n",
      "Iter=275, avg train loss=0.546, \n",
      "\tAvg Val Loss: same-cc=0.567, AUC: same-cc=0.615 \n",
      "\tAvg Val Loss: same-mlo=0.607, AUC: same-mlo=0.574 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.461 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.472\n",
      "Iter=280, avg train loss=0.676, \n",
      "\tAvg Val Loss: same-cc=0.569, AUC: same-cc=0.615 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.573 \n",
      "\tAvg Val Loss opp-cc=0.655, AUC: opp-cc=0.460 \n",
      "\tAvg Val Loss: opp-mlo=0.689, AUC: opp-mlo=0.476\n",
      "Iter=285, avg train loss=0.614, \n",
      "\tAvg Val Loss: same-cc=0.576, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.633, AUC: same-mlo=0.573 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.452 \n",
      "\tAvg Val Loss: opp-mlo=0.707, AUC: opp-mlo=0.485\n",
      "Iter=290, avg train loss=0.557, \n",
      "\tAvg Val Loss: same-cc=0.585, AUC: same-cc=0.601 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.468 \n",
      "\tAvg Val Loss: opp-mlo=0.716, AUC: opp-mlo=0.484\n",
      "Iter=295, avg train loss=0.716, \n",
      "\tAvg Val Loss: same-cc=0.608, AUC: same-cc=0.619 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.471 \n",
      "\tAvg Val Loss: opp-mlo=0.701, AUC: opp-mlo=0.493\n",
      "Iter=300, avg train loss=0.670, \n",
      "\tAvg Val Loss: same-cc=0.616, AUC: same-cc=0.625 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.691, AUC: opp-mlo=0.485\n",
      "Iter=305, avg train loss=0.586, \n",
      "\tAvg Val Loss: same-cc=0.628, AUC: same-cc=0.649 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.586 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.480\n",
      "Iter=310, avg train loss=0.620, \n",
      "\tAvg Val Loss: same-cc=0.650, AUC: same-cc=0.665 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.588 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.460 \n",
      "\tAvg Val Loss: opp-mlo=0.714, AUC: opp-mlo=0.478\n",
      "Iter=315, avg train loss=0.664, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.680 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.686, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.701, AUC: opp-mlo=0.466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=320, avg train loss=0.623, \n",
      "\tAvg Val Loss: same-cc=0.603, AUC: same-cc=0.706 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.451 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.476\n",
      "Best same-cc model saved.\n",
      "Iter=325, avg train loss=0.575, \n",
      "\tAvg Val Loss: same-cc=0.579, AUC: same-cc=0.716 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.658, AUC: opp-cc=0.454 \n",
      "\tAvg Val Loss: opp-mlo=0.667, AUC: opp-mlo=0.495\n",
      "Best same-cc model saved.\n",
      "Iter=330, avg train loss=0.648, \n",
      "\tAvg Val Loss: same-cc=0.523, AUC: same-cc=0.724 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.630, AUC: opp-cc=0.456 \n",
      "\tAvg Val Loss: opp-mlo=0.674, AUC: opp-mlo=0.518\n",
      "Best same-cc model saved.\n",
      "Iter=335, avg train loss=0.550, \n",
      "\tAvg Val Loss: same-cc=0.521, AUC: same-cc=0.704 \n",
      "\tAvg Val Loss: same-mlo=0.585, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.639, AUC: opp-cc=0.458 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.516\n",
      "Iter=340, avg train loss=0.676, \n",
      "\tAvg Val Loss: same-cc=0.535, AUC: same-cc=0.677 \n",
      "\tAvg Val Loss: same-mlo=0.608, AUC: same-mlo=0.591 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.459 \n",
      "\tAvg Val Loss: opp-mlo=0.663, AUC: opp-mlo=0.504\n",
      "Iter=345, avg train loss=0.602, \n",
      "\tAvg Val Loss: same-cc=0.540, AUC: same-cc=0.664 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.458 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.508\n",
      "Iter=350, avg train loss=0.613, \n",
      "\tAvg Val Loss: same-cc=0.549, AUC: same-cc=0.637 \n",
      "\tAvg Val Loss: same-mlo=0.598, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.453 \n",
      "\tAvg Val Loss: opp-mlo=0.674, AUC: opp-mlo=0.516\n",
      "Iter=355, avg train loss=0.551, \n",
      "\tAvg Val Loss: same-cc=0.544, AUC: same-cc=0.655 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.685, AUC: opp-mlo=0.514\n",
      "Iter=360, avg train loss=0.611, \n",
      "\tAvg Val Loss: same-cc=0.549, AUC: same-cc=0.642 \n",
      "\tAvg Val Loss: same-mlo=0.608, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.664, AUC: opp-mlo=0.508\n",
      "Iter=365, avg train loss=0.547, \n",
      "\tAvg Val Loss: same-cc=0.541, AUC: same-cc=0.673 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.472 \n",
      "\tAvg Val Loss: opp-mlo=0.680, AUC: opp-mlo=0.494\n",
      "Iter=370, avg train loss=0.616, \n",
      "\tAvg Val Loss: same-cc=0.543, AUC: same-cc=0.676 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.471 \n",
      "\tAvg Val Loss: opp-mlo=0.686, AUC: opp-mlo=0.498\n",
      "Iter=375, avg train loss=0.612, \n",
      "\tAvg Val Loss: same-cc=0.541, AUC: same-cc=0.681 \n",
      "\tAvg Val Loss: same-mlo=0.645, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.482 \n",
      "\tAvg Val Loss: opp-mlo=0.724, AUC: opp-mlo=0.491\n",
      "Iter=380, avg train loss=0.631, \n",
      "\tAvg Val Loss: same-cc=0.547, AUC: same-cc=0.693 \n",
      "\tAvg Val Loss: same-mlo=0.646, AUC: same-mlo=0.616 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.490 \n",
      "\tAvg Val Loss: opp-mlo=0.733, AUC: opp-mlo=0.493\n",
      "Iter=385, avg train loss=0.601, \n",
      "\tAvg Val Loss: same-cc=0.540, AUC: same-cc=0.673 \n",
      "\tAvg Val Loss: same-mlo=0.602, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.700, AUC: opp-mlo=0.492\n",
      "Iter=390, avg train loss=0.575, \n",
      "\tAvg Val Loss: same-cc=0.542, AUC: same-cc=0.681 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.708, AUC: opp-mlo=0.482\n",
      "Iter=395, avg train loss=0.529, \n",
      "\tAvg Val Loss: same-cc=0.525, AUC: same-cc=0.683 \n",
      "\tAvg Val Loss: same-mlo=0.613, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.498 \n",
      "\tAvg Val Loss: opp-mlo=0.690, AUC: opp-mlo=0.483\n",
      "Iter=400, avg train loss=0.562, \n",
      "\tAvg Val Loss: same-cc=0.524, AUC: same-cc=0.694 \n",
      "\tAvg Val Loss: same-mlo=0.583, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.637, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.493\n",
      "Iter=405, avg train loss=0.587, \n",
      "\tAvg Val Loss: same-cc=0.511, AUC: same-cc=0.725 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.645, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.488\n",
      "Best same-cc model saved.\n",
      "Iter=410, avg train loss=0.546, \n",
      "\tAvg Val Loss: same-cc=0.509, AUC: same-cc=0.700 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.635, AUC: opp-cc=0.512 \n",
      "\tAvg Val Loss: opp-mlo=0.669, AUC: opp-mlo=0.488\n",
      "Iter=415, avg train loss=0.636, \n",
      "\tAvg Val Loss: same-cc=0.524, AUC: same-cc=0.696 \n",
      "\tAvg Val Loss: same-mlo=0.575, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.617, AUC: opp-cc=0.536 \n",
      "\tAvg Val Loss: opp-mlo=0.669, AUC: opp-mlo=0.482\n",
      "Iter=420, avg train loss=0.572, \n",
      "\tAvg Val Loss: same-cc=0.571, AUC: same-cc=0.705 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.639, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.690, AUC: opp-mlo=0.483\n",
      "Iter=425, avg train loss=0.612, \n",
      "\tAvg Val Loss: same-cc=0.647, AUC: same-cc=0.708 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.710, AUC: opp-mlo=0.492\n",
      "Iter=430, avg train loss=0.563, \n",
      "\tAvg Val Loss: same-cc=0.634, AUC: same-cc=0.725 \n",
      "\tAvg Val Loss: same-mlo=0.627, AUC: same-mlo=0.645 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.702, AUC: opp-mlo=0.480\n",
      "Iter=435, avg train loss=0.552, \n",
      "\tAvg Val Loss: same-cc=0.569, AUC: same-cc=0.704 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.643 \n",
      "\tAvg Val Loss opp-cc=0.717, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.471\n",
      "Iter=440, avg train loss=0.610, \n",
      "\tAvg Val Loss: same-cc=0.536, AUC: same-cc=0.690 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.518 \n",
      "\tAvg Val Loss: opp-mlo=0.668, AUC: opp-mlo=0.475\n",
      "Iter=445, avg train loss=0.536, \n",
      "\tAvg Val Loss: same-cc=0.533, AUC: same-cc=0.675 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.682, AUC: opp-cc=0.504 \n",
      "\tAvg Val Loss: opp-mlo=0.665, AUC: opp-mlo=0.468\n",
      "Iter=450, avg train loss=0.516, \n",
      "\tAvg Val Loss: same-cc=0.546, AUC: same-cc=0.652 \n",
      "\tAvg Val Loss: same-mlo=0.602, AUC: same-mlo=0.638 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.502 \n",
      "\tAvg Val Loss: opp-mlo=0.655, AUC: opp-mlo=0.461\n",
      "Iter=455, avg train loss=0.491, \n",
      "\tAvg Val Loss: same-cc=0.533, AUC: same-cc=0.677 \n",
      "\tAvg Val Loss: same-mlo=0.592, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.449\n",
      "Iter=460, avg train loss=0.663, \n",
      "\tAvg Val Loss: same-cc=0.538, AUC: same-cc=0.681 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.649, AUC: opp-cc=0.501 \n",
      "\tAvg Val Loss: opp-mlo=0.657, AUC: opp-mlo=0.456\n",
      "Iter=465, avg train loss=0.574, \n",
      "\tAvg Val Loss: same-cc=0.527, AUC: same-cc=0.696 \n",
      "\tAvg Val Loss: same-mlo=0.684, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.476 \n",
      "\tAvg Val Loss: opp-mlo=0.679, AUC: opp-mlo=0.462\n",
      "Iter=470, avg train loss=0.598, \n",
      "\tAvg Val Loss: same-cc=0.545, AUC: same-cc=0.708 \n",
      "\tAvg Val Loss: same-mlo=0.636, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.484 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.469\n",
      "Iter=475, avg train loss=0.486, \n",
      "\tAvg Val Loss: same-cc=0.535, AUC: same-cc=0.690 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.466\n",
      "Iter=480, avg train loss=0.588, \n",
      "\tAvg Val Loss: same-cc=0.528, AUC: same-cc=0.709 \n",
      "\tAvg Val Loss: same-mlo=0.582, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.659, AUC: opp-cc=0.470 \n",
      "\tAvg Val Loss: opp-mlo=0.662, AUC: opp-mlo=0.466\n",
      "Iter=485, avg train loss=0.541, \n",
      "\tAvg Val Loss: same-cc=0.547, AUC: same-cc=0.705 \n",
      "\tAvg Val Loss: same-mlo=0.602, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.702, AUC: opp-cc=0.450 \n",
      "\tAvg Val Loss: opp-mlo=0.664, AUC: opp-mlo=0.449\n",
      "Iter=490, avg train loss=0.608, \n",
      "\tAvg Val Loss: same-cc=0.568, AUC: same-cc=0.709 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.439 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.448\n",
      "Iter=495, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.564, AUC: same-cc=0.704 \n",
      "\tAvg Val Loss: same-mlo=0.679, AUC: same-mlo=0.616 \n",
      "\tAvg Val Loss opp-cc=0.703, AUC: opp-cc=0.459 \n",
      "\tAvg Val Loss: opp-mlo=0.776, AUC: opp-mlo=0.458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=500, avg train loss=0.498, \n",
      "\tAvg Val Loss: same-cc=0.536, AUC: same-cc=0.699 \n",
      "\tAvg Val Loss: same-mlo=0.647, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.478 \n",
      "\tAvg Val Loss: opp-mlo=0.770, AUC: opp-mlo=0.465\n",
      "Iter=505, avg train loss=0.563, \n",
      "\tAvg Val Loss: same-cc=0.515, AUC: same-cc=0.711 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.480 \n",
      "\tAvg Val Loss: opp-mlo=0.743, AUC: opp-mlo=0.465\n",
      "Iter=510, avg train loss=0.539, \n",
      "\tAvg Val Loss: same-cc=0.514, AUC: same-cc=0.708 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.681, AUC: opp-cc=0.472 \n",
      "\tAvg Val Loss: opp-mlo=0.723, AUC: opp-mlo=0.464\n",
      "Iter=515, avg train loss=0.501, \n",
      "\tAvg Val Loss: same-cc=0.521, AUC: same-cc=0.718 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.711, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.708, AUC: opp-mlo=0.472\n",
      "Iter=520, avg train loss=0.505, \n",
      "\tAvg Val Loss: same-cc=0.524, AUC: same-cc=0.721 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.689, AUC: opp-cc=0.500 \n",
      "\tAvg Val Loss: opp-mlo=0.673, AUC: opp-mlo=0.471\n",
      "Iter=525, avg train loss=0.433, \n",
      "\tAvg Val Loss: same-cc=0.520, AUC: same-cc=0.702 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.500 \n",
      "\tAvg Val Loss: opp-mlo=0.669, AUC: opp-mlo=0.470\n",
      "Iter=530, avg train loss=0.510, \n",
      "\tAvg Val Loss: same-cc=0.524, AUC: same-cc=0.705 \n",
      "\tAvg Val Loss: same-mlo=0.644, AUC: same-mlo=0.621 \n",
      "\tAvg Val Loss opp-cc=0.724, AUC: opp-cc=0.516 \n",
      "\tAvg Val Loss: opp-mlo=0.696, AUC: opp-mlo=0.486\n",
      "Iter=535, avg train loss=0.563, \n",
      "\tAvg Val Loss: same-cc=0.565, AUC: same-cc=0.712 \n",
      "\tAvg Val Loss: same-mlo=0.707, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.740, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.772, AUC: opp-mlo=0.491\n",
      "Iter=540, avg train loss=0.513, \n",
      "\tAvg Val Loss: same-cc=0.584, AUC: same-cc=0.704 \n",
      "\tAvg Val Loss: same-mlo=0.683, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.719, AUC: opp-cc=0.508 \n",
      "\tAvg Val Loss: opp-mlo=0.786, AUC: opp-mlo=0.490\n",
      "Iter=545, avg train loss=0.504, \n",
      "\tAvg Val Loss: same-cc=0.616, AUC: same-cc=0.693 \n",
      "\tAvg Val Loss: same-mlo=0.663, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.681, AUC: opp-cc=0.504 \n",
      "\tAvg Val Loss: opp-mlo=0.737, AUC: opp-mlo=0.512\n",
      "Iter=550, avg train loss=0.566, \n",
      "\tAvg Val Loss: same-cc=0.544, AUC: same-cc=0.669 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.499\n",
      "Iter=555, avg train loss=0.505, \n",
      "\tAvg Val Loss: same-cc=0.557, AUC: same-cc=0.695 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.501 \n",
      "\tAvg Val Loss: opp-mlo=0.653, AUC: opp-mlo=0.488\n",
      "Iter=560, avg train loss=0.488, \n",
      "\tAvg Val Loss: same-cc=0.523, AUC: same-cc=0.692 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.710, AUC: opp-mlo=0.474\n",
      "Iter=565, avg train loss=0.475, \n",
      "\tAvg Val Loss: same-cc=0.524, AUC: same-cc=0.700 \n",
      "\tAvg Val Loss: same-mlo=0.620, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.692, AUC: opp-mlo=0.461\n",
      "Iter=570, avg train loss=0.537, \n",
      "\tAvg Val Loss: same-cc=0.533, AUC: same-cc=0.696 \n",
      "\tAvg Val Loss: same-mlo=0.660, AUC: same-mlo=0.608 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.476 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.453\n",
      "Iter=575, avg train loss=0.431, \n",
      "\tAvg Val Loss: same-cc=0.545, AUC: same-cc=0.709 \n",
      "\tAvg Val Loss: same-mlo=0.661, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.678, AUC: opp-cc=0.488 \n",
      "\tAvg Val Loss: opp-mlo=0.711, AUC: opp-mlo=0.460\n",
      "Iter=580, avg train loss=0.458, \n",
      "\tAvg Val Loss: same-cc=0.581, AUC: same-cc=0.713 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.697, AUC: opp-cc=0.482 \n",
      "\tAvg Val Loss: opp-mlo=0.796, AUC: opp-mlo=0.476\n",
      "Iter=585, avg train loss=0.467, \n",
      "\tAvg Val Loss: same-cc=0.530, AUC: same-cc=0.708 \n",
      "\tAvg Val Loss: same-mlo=0.653, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.695, AUC: opp-cc=0.480 \n",
      "\tAvg Val Loss: opp-mlo=0.880, AUC: opp-mlo=0.455\n",
      "Iter=590, avg train loss=0.594, \n",
      "\tAvg Val Loss: same-cc=0.573, AUC: same-cc=0.645 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.687, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.768, AUC: opp-mlo=0.464\n",
      "Iter=595, avg train loss=0.438, \n",
      "\tAvg Val Loss: same-cc=0.580, AUC: same-cc=0.631 \n",
      "\tAvg Val Loss: same-mlo=0.599, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.667, AUC: opp-cc=0.510 \n",
      "\tAvg Val Loss: opp-mlo=0.706, AUC: opp-mlo=0.476\n",
      "Iter=600, avg train loss=0.588, \n",
      "\tAvg Val Loss: same-cc=0.558, AUC: same-cc=0.638 \n",
      "\tAvg Val Loss: same-mlo=0.618, AUC: same-mlo=0.599 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.705, AUC: opp-mlo=0.478\n",
      "Iter=605, avg train loss=0.515, \n",
      "\tAvg Val Loss: same-cc=0.581, AUC: same-cc=0.639 \n",
      "\tAvg Val Loss: same-mlo=0.765, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.778, AUC: opp-cc=0.492 \n",
      "\tAvg Val Loss: opp-mlo=0.730, AUC: opp-mlo=0.490\n",
      "Iter=610, avg train loss=0.448, \n",
      "\tAvg Val Loss: same-cc=0.592, AUC: same-cc=0.651 \n",
      "\tAvg Val Loss: same-mlo=0.767, AUC: same-mlo=0.592 \n",
      "\tAvg Val Loss opp-cc=0.775, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.712, AUC: opp-mlo=0.481\n",
      "Iter=615, avg train loss=0.397, \n",
      "\tAvg Val Loss: same-cc=0.571, AUC: same-cc=0.660 \n",
      "\tAvg Val Loss: same-mlo=0.724, AUC: same-mlo=0.592 \n",
      "\tAvg Val Loss opp-cc=0.753, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.700, AUC: opp-mlo=0.488\n",
      "Iter=620, avg train loss=0.554, \n",
      "\tAvg Val Loss: same-cc=0.633, AUC: same-cc=0.698 \n",
      "\tAvg Val Loss: same-mlo=0.719, AUC: same-mlo=0.599 \n",
      "\tAvg Val Loss opp-cc=0.782, AUC: opp-cc=0.480 \n",
      "\tAvg Val Loss: opp-mlo=0.763, AUC: opp-mlo=0.493\n",
      "Iter=625, avg train loss=0.431, \n",
      "\tAvg Val Loss: same-cc=0.700, AUC: same-cc=0.700 \n",
      "\tAvg Val Loss: same-mlo=0.785, AUC: same-mlo=0.599 \n",
      "\tAvg Val Loss opp-cc=0.820, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.891, AUC: opp-mlo=0.483\n",
      "Iter=630, avg train loss=0.488, \n",
      "\tAvg Val Loss: same-cc=0.662, AUC: same-cc=0.709 \n",
      "\tAvg Val Loss: same-mlo=0.793, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.800, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.901, AUC: opp-mlo=0.494\n",
      "Iter=635, avg train loss=0.424, \n",
      "\tAvg Val Loss: same-cc=0.601, AUC: same-cc=0.693 \n",
      "\tAvg Val Loss: same-mlo=0.708, AUC: same-mlo=0.599 \n",
      "\tAvg Val Loss opp-cc=0.781, AUC: opp-cc=0.508 \n",
      "\tAvg Val Loss: opp-mlo=0.871, AUC: opp-mlo=0.493\n",
      "Iter=640, avg train loss=0.499, \n",
      "\tAvg Val Loss: same-cc=0.586, AUC: same-cc=0.624 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.590 \n",
      "\tAvg Val Loss opp-cc=0.722, AUC: opp-cc=0.512 \n",
      "\tAvg Val Loss: opp-mlo=0.784, AUC: opp-mlo=0.494\n",
      "Iter=645, avg train loss=0.447, \n",
      "\tAvg Val Loss: same-cc=0.606, AUC: same-cc=0.607 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.595 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.707, AUC: opp-mlo=0.483\n",
      "Iter=650, avg train loss=0.466, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.627 \n",
      "\tAvg Val Loss: same-mlo=0.625, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.702, AUC: opp-mlo=0.483\n",
      "Iter=655, avg train loss=0.447, \n",
      "\tAvg Val Loss: same-cc=0.550, AUC: same-cc=0.714 \n",
      "\tAvg Val Loss: same-mlo=0.689, AUC: same-mlo=0.613 \n",
      "\tAvg Val Loss opp-cc=0.727, AUC: opp-cc=0.516 \n",
      "\tAvg Val Loss: opp-mlo=0.757, AUC: opp-mlo=0.472\n",
      "Iter=660, avg train loss=0.458, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.724 \n",
      "\tAvg Val Loss: same-mlo=0.757, AUC: same-mlo=0.608 \n",
      "\tAvg Val Loss opp-cc=0.761, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.779, AUC: opp-mlo=0.473\n",
      "Iter=665, avg train loss=0.447, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.709 \n",
      "\tAvg Val Loss: same-mlo=0.707, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.785, AUC: opp-cc=0.502 \n",
      "\tAvg Val Loss: opp-mlo=0.797, AUC: opp-mlo=0.476\n",
      "Iter=670, avg train loss=0.346, \n",
      "\tAvg Val Loss: same-cc=0.556, AUC: same-cc=0.700 \n",
      "\tAvg Val Loss: same-mlo=0.674, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.738, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.767, AUC: opp-mlo=0.473\n",
      "Iter=675, avg train loss=0.439, \n",
      "\tAvg Val Loss: same-cc=0.536, AUC: same-cc=0.692 \n",
      "\tAvg Val Loss: same-mlo=0.645, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.760, AUC: opp-cc=0.512 \n",
      "\tAvg Val Loss: opp-mlo=0.805, AUC: opp-mlo=0.472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=680, avg train loss=0.354, \n",
      "\tAvg Val Loss: same-cc=0.576, AUC: same-cc=0.687 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.647 \n",
      "\tAvg Val Loss opp-cc=0.781, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.824, AUC: opp-mlo=0.473\n",
      "Iter=685, avg train loss=0.461, \n",
      "\tAvg Val Loss: same-cc=0.563, AUC: same-cc=0.695 \n",
      "\tAvg Val Loss: same-mlo=0.630, AUC: same-mlo=0.649 \n",
      "\tAvg Val Loss opp-cc=0.752, AUC: opp-cc=0.508 \n",
      "\tAvg Val Loss: opp-mlo=0.810, AUC: opp-mlo=0.478\n",
      "Iter=690, avg train loss=0.361, \n",
      "\tAvg Val Loss: same-cc=0.532, AUC: same-cc=0.710 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.703, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.770, AUC: opp-mlo=0.481\n",
      "Iter=695, avg train loss=0.441, \n",
      "\tAvg Val Loss: same-cc=0.520, AUC: same-cc=0.713 \n",
      "\tAvg Val Loss: same-mlo=0.628, AUC: same-mlo=0.621 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.512 \n",
      "\tAvg Val Loss: opp-mlo=0.744, AUC: opp-mlo=0.476\n",
      "Iter=700, avg train loss=0.391, \n",
      "\tAvg Val Loss: same-cc=0.521, AUC: same-cc=0.716 \n",
      "\tAvg Val Loss: same-mlo=0.685, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.728, AUC: opp-cc=0.501 \n",
      "\tAvg Val Loss: opp-mlo=0.743, AUC: opp-mlo=0.474\n",
      "Iter=705, avg train loss=0.443, \n",
      "\tAvg Val Loss: same-cc=0.550, AUC: same-cc=0.713 \n",
      "\tAvg Val Loss: same-mlo=0.719, AUC: same-mlo=0.648 \n",
      "\tAvg Val Loss opp-cc=0.804, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.757, AUC: opp-mlo=0.469\n",
      "Iter=710, avg train loss=0.403, \n",
      "\tAvg Val Loss: same-cc=0.554, AUC: same-cc=0.698 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.634 \n",
      "\tAvg Val Loss opp-cc=0.835, AUC: opp-cc=0.506 \n",
      "\tAvg Val Loss: opp-mlo=0.795, AUC: opp-mlo=0.478\n",
      "Iter=715, avg train loss=0.446, \n",
      "\tAvg Val Loss: same-cc=0.563, AUC: same-cc=0.667 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.784, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.770, AUC: opp-mlo=0.469\n",
      "Iter=720, avg train loss=0.402, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.655 \n",
      "\tAvg Val Loss: same-mlo=0.710, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.736, AUC: opp-mlo=0.468\n",
      "Iter=725, avg train loss=0.422, \n",
      "\tAvg Val Loss: same-cc=0.615, AUC: same-cc=0.683 \n",
      "\tAvg Val Loss: same-mlo=0.694, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.518 \n",
      "\tAvg Val Loss: opp-mlo=0.719, AUC: opp-mlo=0.463\n",
      "Iter=730, avg train loss=0.524, \n",
      "\tAvg Val Loss: same-cc=0.552, AUC: same-cc=0.714 \n",
      "\tAvg Val Loss: same-mlo=0.768, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.709, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.753, AUC: opp-mlo=0.470\n",
      "Iter=735, avg train loss=0.421, \n",
      "\tAvg Val Loss: same-cc=0.714, AUC: same-cc=0.721 \n",
      "\tAvg Val Loss: same-mlo=0.885, AUC: same-mlo=0.616 \n",
      "\tAvg Val Loss opp-cc=0.819, AUC: opp-cc=0.516 \n",
      "\tAvg Val Loss: opp-mlo=0.817, AUC: opp-mlo=0.465\n",
      "Iter=740, avg train loss=0.420, \n",
      "\tAvg Val Loss: same-cc=0.580, AUC: same-cc=0.699 \n",
      "\tAvg Val Loss: same-mlo=0.727, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.815, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.802, AUC: opp-mlo=0.474\n",
      "Iter=745, avg train loss=0.470, \n",
      "\tAvg Val Loss: same-cc=0.559, AUC: same-cc=0.675 \n",
      "\tAvg Val Loss: same-mlo=0.673, AUC: same-mlo=0.626 \n",
      "\tAvg Val Loss opp-cc=0.808, AUC: opp-cc=0.492 \n",
      "\tAvg Val Loss: opp-mlo=0.788, AUC: opp-mlo=0.471\n",
      "Iter=750, avg train loss=0.369, \n",
      "\tAvg Val Loss: same-cc=0.558, AUC: same-cc=0.675 \n",
      "\tAvg Val Loss: same-mlo=0.686, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.772, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.789, AUC: opp-mlo=0.479\n",
      "Iter=755, avg train loss=0.312, \n",
      "\tAvg Val Loss: same-cc=0.600, AUC: same-cc=0.662 \n",
      "\tAvg Val Loss: same-mlo=0.671, AUC: same-mlo=0.605 \n",
      "\tAvg Val Loss opp-cc=0.744, AUC: opp-cc=0.492 \n",
      "\tAvg Val Loss: opp-mlo=0.756, AUC: opp-mlo=0.464\n",
      "Iter=760, avg train loss=0.485, \n",
      "\tAvg Val Loss: same-cc=0.608, AUC: same-cc=0.652 \n",
      "\tAvg Val Loss: same-mlo=0.672, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.737, AUC: opp-cc=0.492 \n",
      "\tAvg Val Loss: opp-mlo=0.752, AUC: opp-mlo=0.466\n",
      "Iter=765, avg train loss=0.432, \n",
      "\tAvg Val Loss: same-cc=0.559, AUC: same-cc=0.665 \n",
      "\tAvg Val Loss: same-mlo=0.703, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.736, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.762, AUC: opp-mlo=0.463\n",
      "Iter=770, avg train loss=0.432, \n",
      "\tAvg Val Loss: same-cc=0.578, AUC: same-cc=0.676 \n",
      "\tAvg Val Loss: same-mlo=0.665, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.746, AUC: opp-cc=0.464 \n",
      "\tAvg Val Loss: opp-mlo=0.766, AUC: opp-mlo=0.464\n",
      "Iter=775, avg train loss=0.405, \n",
      "\tAvg Val Loss: same-cc=0.576, AUC: same-cc=0.667 \n",
      "\tAvg Val Loss: same-mlo=0.649, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.764, AUC: opp-cc=0.474 \n",
      "\tAvg Val Loss: opp-mlo=0.792, AUC: opp-mlo=0.468\n",
      "Iter=780, avg train loss=0.365, \n",
      "\tAvg Val Loss: same-cc=0.559, AUC: same-cc=0.640 \n",
      "\tAvg Val Loss: same-mlo=0.630, AUC: same-mlo=0.591 \n",
      "\tAvg Val Loss opp-cc=0.802, AUC: opp-cc=0.463 \n",
      "\tAvg Val Loss: opp-mlo=0.759, AUC: opp-mlo=0.461\n",
      "Iter=785, avg train loss=0.403, \n",
      "\tAvg Val Loss: same-cc=0.621, AUC: same-cc=0.638 \n",
      "\tAvg Val Loss: same-mlo=0.650, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.741, AUC: opp-cc=0.473 \n",
      "\tAvg Val Loss: opp-mlo=0.738, AUC: opp-mlo=0.447\n",
      "Iter=790, avg train loss=0.498, \n",
      "\tAvg Val Loss: same-cc=0.586, AUC: same-cc=0.658 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.607 \n",
      "\tAvg Val Loss opp-cc=0.809, AUC: opp-cc=0.502 \n",
      "\tAvg Val Loss: opp-mlo=0.760, AUC: opp-mlo=0.455\n",
      "Iter=795, avg train loss=0.441, \n",
      "\tAvg Val Loss: same-cc=0.576, AUC: same-cc=0.683 \n",
      "\tAvg Val Loss: same-mlo=0.664, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.870, AUC: opp-cc=0.491 \n",
      "\tAvg Val Loss: opp-mlo=0.804, AUC: opp-mlo=0.458\n",
      "Iter=800, avg train loss=0.385, \n",
      "\tAvg Val Loss: same-cc=0.560, AUC: same-cc=0.692 \n",
      "\tAvg Val Loss: same-mlo=0.677, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.804, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.778, AUC: opp-mlo=0.445\n",
      "Iter=805, avg train loss=0.458, \n",
      "\tAvg Val Loss: same-cc=0.558, AUC: same-cc=0.669 \n",
      "\tAvg Val Loss: same-mlo=0.708, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.880, AUC: opp-cc=0.465 \n",
      "\tAvg Val Loss: opp-mlo=0.859, AUC: opp-mlo=0.452\n",
      "Iter=810, avg train loss=0.436, \n",
      "\tAvg Val Loss: same-cc=0.550, AUC: same-cc=0.651 \n",
      "\tAvg Val Loss: same-mlo=0.682, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.812, AUC: opp-cc=0.461 \n",
      "\tAvg Val Loss: opp-mlo=0.870, AUC: opp-mlo=0.448\n",
      "Iter=815, avg train loss=0.398, \n",
      "\tAvg Val Loss: same-cc=0.543, AUC: same-cc=0.657 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.785, AUC: opp-cc=0.448 \n",
      "\tAvg Val Loss: opp-mlo=0.878, AUC: opp-mlo=0.443\n",
      "Iter=820, avg train loss=0.392, \n",
      "\tAvg Val Loss: same-cc=0.548, AUC: same-cc=0.664 \n",
      "\tAvg Val Loss: same-mlo=0.690, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.785, AUC: opp-cc=0.456 \n",
      "\tAvg Val Loss: opp-mlo=0.883, AUC: opp-mlo=0.436\n",
      "Iter=825, avg train loss=0.388, \n",
      "\tAvg Val Loss: same-cc=0.555, AUC: same-cc=0.667 \n",
      "\tAvg Val Loss: same-mlo=0.687, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.757, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.828, AUC: opp-mlo=0.442\n",
      "Iter=830, avg train loss=0.356, \n",
      "\tAvg Val Loss: same-cc=0.555, AUC: same-cc=0.673 \n",
      "\tAvg Val Loss: same-mlo=0.689, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.715, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.748, AUC: opp-mlo=0.445\n",
      "Iter=835, avg train loss=0.348, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.653 \n",
      "\tAvg Val Loss: same-mlo=0.667, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.721, AUC: opp-cc=0.512 \n",
      "\tAvg Val Loss: opp-mlo=0.757, AUC: opp-mlo=0.444\n",
      "Iter=840, avg train loss=0.269, \n",
      "\tAvg Val Loss: same-cc=0.585, AUC: same-cc=0.684 \n",
      "\tAvg Val Loss: same-mlo=0.675, AUC: same-mlo=0.592 \n",
      "\tAvg Val Loss opp-cc=0.763, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.753, AUC: opp-mlo=0.448\n",
      "Iter=845, avg train loss=0.316, \n",
      "\tAvg Val Loss: same-cc=0.564, AUC: same-cc=0.731 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.571 \n",
      "\tAvg Val Loss opp-cc=0.822, AUC: opp-cc=0.500 \n",
      "\tAvg Val Loss: opp-mlo=0.768, AUC: opp-mlo=0.451\n",
      "Best same-cc model saved.\n",
      "Iter=850, avg train loss=0.371, \n",
      "\tAvg Val Loss: same-cc=0.573, AUC: same-cc=0.737 \n",
      "\tAvg Val Loss: same-mlo=0.681, AUC: same-mlo=0.556 \n",
      "\tAvg Val Loss opp-cc=0.852, AUC: opp-cc=0.498 \n",
      "\tAvg Val Loss: opp-mlo=0.816, AUC: opp-mlo=0.461\n",
      "Best same-cc model saved.\n",
      "Iter=855, avg train loss=0.467, \n",
      "\tAvg Val Loss: same-cc=0.593, AUC: same-cc=0.751 \n",
      "\tAvg Val Loss: same-mlo=0.864, AUC: same-mlo=0.559 \n",
      "\tAvg Val Loss opp-cc=0.987, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.865, AUC: opp-mlo=0.468\n",
      "Best same-cc model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=860, avg train loss=0.373, \n",
      "\tAvg Val Loss: same-cc=0.567, AUC: same-cc=0.717 \n",
      "\tAvg Val Loss: same-mlo=0.930, AUC: same-mlo=0.564 \n",
      "\tAvg Val Loss opp-cc=0.952, AUC: opp-cc=0.488 \n",
      "\tAvg Val Loss: opp-mlo=1.077, AUC: opp-mlo=0.471\n",
      "Iter=865, avg train loss=0.342, \n",
      "\tAvg Val Loss: same-cc=0.548, AUC: same-cc=0.689 \n",
      "\tAvg Val Loss: same-mlo=0.745, AUC: same-mlo=0.577 \n",
      "\tAvg Val Loss opp-cc=0.856, AUC: opp-cc=0.499 \n",
      "\tAvg Val Loss: opp-mlo=0.954, AUC: opp-mlo=0.472\n",
      "Iter=870, avg train loss=0.298, \n",
      "\tAvg Val Loss: same-cc=0.602, AUC: same-cc=0.668 \n",
      "\tAvg Val Loss: same-mlo=0.690, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.765, AUC: opp-cc=0.497 \n",
      "\tAvg Val Loss: opp-mlo=0.798, AUC: opp-mlo=0.462\n",
      "Iter=875, avg train loss=0.456, \n",
      "\tAvg Val Loss: same-cc=0.559, AUC: same-cc=0.686 \n",
      "\tAvg Val Loss: same-mlo=0.727, AUC: same-mlo=0.582 \n",
      "\tAvg Val Loss opp-cc=0.784, AUC: opp-cc=0.494 \n",
      "\tAvg Val Loss: opp-mlo=0.845, AUC: opp-mlo=0.461\n",
      "Iter=880, avg train loss=0.279, \n",
      "\tAvg Val Loss: same-cc=0.609, AUC: same-cc=0.697 \n",
      "\tAvg Val Loss: same-mlo=0.838, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.842, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.964, AUC: opp-mlo=0.451\n",
      "Iter=885, avg train loss=0.422, \n",
      "\tAvg Val Loss: same-cc=0.583, AUC: same-cc=0.671 \n",
      "\tAvg Val Loss: same-mlo=0.843, AUC: same-mlo=0.605 \n",
      "\tAvg Val Loss opp-cc=0.839, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.940, AUC: opp-mlo=0.453\n",
      "Iter=890, avg train loss=0.319, \n",
      "\tAvg Val Loss: same-cc=0.581, AUC: same-cc=0.639 \n",
      "\tAvg Val Loss: same-mlo=0.745, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.763, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.920, AUC: opp-mlo=0.462\n",
      "Iter=895, avg train loss=0.303, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.621 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.718, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.891, AUC: opp-mlo=0.462\n",
      "Iter=900, avg train loss=0.350, \n",
      "\tAvg Val Loss: same-cc=0.588, AUC: same-cc=0.625 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.616 \n",
      "\tAvg Val Loss opp-cc=0.727, AUC: opp-cc=0.504 \n",
      "\tAvg Val Loss: opp-mlo=0.866, AUC: opp-mlo=0.464\n",
      "Iter=905, avg train loss=0.271, \n",
      "\tAvg Val Loss: same-cc=0.555, AUC: same-cc=0.662 \n",
      "\tAvg Val Loss: same-mlo=0.687, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.506 \n",
      "\tAvg Val Loss: opp-mlo=0.802, AUC: opp-mlo=0.463\n",
      "Iter=910, avg train loss=0.353, \n",
      "\tAvg Val Loss: same-cc=0.570, AUC: same-cc=0.719 \n",
      "\tAvg Val Loss: same-mlo=0.730, AUC: same-mlo=0.605 \n",
      "\tAvg Val Loss opp-cc=0.717, AUC: opp-cc=0.493 \n",
      "\tAvg Val Loss: opp-mlo=0.756, AUC: opp-mlo=0.455\n",
      "Iter=915, avg train loss=0.335, \n",
      "\tAvg Val Loss: same-cc=0.555, AUC: same-cc=0.733 \n",
      "\tAvg Val Loss: same-mlo=0.733, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.728, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.776, AUC: opp-mlo=0.452\n",
      "Iter=920, avg train loss=0.355, \n",
      "\tAvg Val Loss: same-cc=0.566, AUC: same-cc=0.733 \n",
      "\tAvg Val Loss: same-mlo=0.768, AUC: same-mlo=0.605 \n",
      "\tAvg Val Loss opp-cc=0.858, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.804, AUC: opp-mlo=0.453\n",
      "Iter=925, avg train loss=0.379, \n",
      "\tAvg Val Loss: same-cc=0.537, AUC: same-cc=0.723 \n",
      "\tAvg Val Loss: same-mlo=0.726, AUC: same-mlo=0.600 \n",
      "\tAvg Val Loss opp-cc=0.851, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.839, AUC: opp-mlo=0.449\n",
      "Iter=930, avg train loss=0.350, \n",
      "\tAvg Val Loss: same-cc=0.540, AUC: same-cc=0.693 \n",
      "\tAvg Val Loss: same-mlo=0.707, AUC: same-mlo=0.599 \n",
      "\tAvg Val Loss opp-cc=0.757, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.907, AUC: opp-mlo=0.436\n",
      "Iter=935, avg train loss=0.308, \n",
      "\tAvg Val Loss: same-cc=0.574, AUC: same-cc=0.655 \n",
      "\tAvg Val Loss: same-mlo=0.725, AUC: same-mlo=0.605 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.542 \n",
      "\tAvg Val Loss: opp-mlo=0.845, AUC: opp-mlo=0.431\n",
      "Iter=940, avg train loss=0.266, \n",
      "\tAvg Val Loss: same-cc=0.575, AUC: same-cc=0.651 \n",
      "\tAvg Val Loss: same-mlo=0.722, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.720, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.786, AUC: opp-mlo=0.428\n",
      "Iter=945, avg train loss=0.310, \n",
      "\tAvg Val Loss: same-cc=0.565, AUC: same-cc=0.669 \n",
      "\tAvg Val Loss: same-mlo=0.729, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.768, AUC: opp-cc=0.505 \n",
      "\tAvg Val Loss: opp-mlo=0.781, AUC: opp-mlo=0.426\n",
      "Iter=950, avg train loss=0.302, \n",
      "\tAvg Val Loss: same-cc=0.548, AUC: same-cc=0.692 \n",
      "\tAvg Val Loss: same-mlo=0.720, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.791, AUC: opp-cc=0.499 \n",
      "\tAvg Val Loss: opp-mlo=0.805, AUC: opp-mlo=0.427\n",
      "Iter=955, avg train loss=0.306, \n",
      "\tAvg Val Loss: same-cc=0.546, AUC: same-cc=0.695 \n",
      "\tAvg Val Loss: same-mlo=0.703, AUC: same-mlo=0.586 \n",
      "\tAvg Val Loss opp-cc=0.816, AUC: opp-cc=0.508 \n",
      "\tAvg Val Loss: opp-mlo=0.807, AUC: opp-mlo=0.437\n",
      "Iter=960, avg train loss=0.321, \n",
      "\tAvg Val Loss: same-cc=0.571, AUC: same-cc=0.675 \n",
      "\tAvg Val Loss: same-mlo=0.700, AUC: same-mlo=0.588 \n",
      "\tAvg Val Loss opp-cc=0.786, AUC: opp-cc=0.507 \n",
      "\tAvg Val Loss: opp-mlo=0.786, AUC: opp-mlo=0.448\n",
      "Iter=965, avg train loss=0.269, \n",
      "\tAvg Val Loss: same-cc=0.585, AUC: same-cc=0.669 \n",
      "\tAvg Val Loss: same-mlo=0.752, AUC: same-mlo=0.592 \n",
      "\tAvg Val Loss opp-cc=0.781, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.817, AUC: opp-mlo=0.457\n",
      "Iter=970, avg train loss=0.324, \n",
      "\tAvg Val Loss: same-cc=0.551, AUC: same-cc=0.704 \n",
      "\tAvg Val Loss: same-mlo=0.767, AUC: same-mlo=0.591 \n",
      "\tAvg Val Loss opp-cc=0.781, AUC: opp-cc=0.508 \n",
      "\tAvg Val Loss: opp-mlo=0.837, AUC: opp-mlo=0.455\n",
      "Iter=975, avg train loss=0.242, \n",
      "\tAvg Val Loss: same-cc=0.558, AUC: same-cc=0.687 \n",
      "\tAvg Val Loss: same-mlo=0.784, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.796, AUC: opp-cc=0.510 \n",
      "\tAvg Val Loss: opp-mlo=0.828, AUC: opp-mlo=0.451\n",
      "Iter=980, avg train loss=0.321, \n",
      "\tAvg Val Loss: same-cc=0.599, AUC: same-cc=0.667 \n",
      "\tAvg Val Loss: same-mlo=0.719, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.769, AUC: opp-cc=0.525 \n",
      "\tAvg Val Loss: opp-mlo=0.752, AUC: opp-mlo=0.452\n",
      "Iter=985, avg train loss=0.275, \n",
      "\tAvg Val Loss: same-cc=0.575, AUC: same-cc=0.665 \n",
      "\tAvg Val Loss: same-mlo=0.716, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.806, AUC: opp-cc=0.500 \n",
      "\tAvg Val Loss: opp-mlo=0.798, AUC: opp-mlo=0.451\n",
      "Iter=990, avg train loss=0.366, \n",
      "\tAvg Val Loss: same-cc=0.587, AUC: same-cc=0.652 \n",
      "\tAvg Val Loss: same-mlo=0.733, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.800, AUC: opp-cc=0.494 \n",
      "\tAvg Val Loss: opp-mlo=0.867, AUC: opp-mlo=0.470\n",
      "Iter=995, avg train loss=0.327, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.657 \n",
      "\tAvg Val Loss: same-mlo=0.738, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.712, AUC: opp-cc=0.516 \n",
      "\tAvg Val Loss: opp-mlo=0.824, AUC: opp-mlo=0.464\n",
      "Iter=1000, avg train loss=0.272, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.654 \n",
      "\tAvg Val Loss: same-mlo=0.766, AUC: same-mlo=0.593 \n",
      "\tAvg Val Loss opp-cc=0.726, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.815, AUC: opp-mlo=0.456\n",
      "Iter=1005, avg train loss=0.396, \n",
      "\tAvg Val Loss: same-cc=0.605, AUC: same-cc=0.689 \n",
      "\tAvg Val Loss: same-mlo=0.831, AUC: same-mlo=0.599 \n",
      "\tAvg Val Loss opp-cc=0.714, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.807, AUC: opp-mlo=0.463\n",
      "Iter=1010, avg train loss=0.327, \n",
      "\tAvg Val Loss: same-cc=0.594, AUC: same-cc=0.698 \n",
      "\tAvg Val Loss: same-mlo=0.872, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.799, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.820, AUC: opp-mlo=0.472\n",
      "Iter=1015, avg train loss=0.283, \n",
      "\tAvg Val Loss: same-cc=0.580, AUC: same-cc=0.658 \n",
      "\tAvg Val Loss: same-mlo=0.744, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.832, AUC: opp-cc=0.544 \n",
      "\tAvg Val Loss: opp-mlo=0.804, AUC: opp-mlo=0.465\n",
      "Iter=1020, avg train loss=0.230, \n",
      "\tAvg Val Loss: same-cc=0.588, AUC: same-cc=0.645 \n",
      "\tAvg Val Loss: same-mlo=0.722, AUC: same-mlo=0.588 \n",
      "\tAvg Val Loss opp-cc=0.838, AUC: opp-cc=0.543 \n",
      "\tAvg Val Loss: opp-mlo=0.876, AUC: opp-mlo=0.479\n",
      "Iter=1025, avg train loss=0.301, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.612 \n",
      "\tAvg Val Loss: same-mlo=0.720, AUC: same-mlo=0.582 \n",
      "\tAvg Val Loss opp-cc=0.753, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.901, AUC: opp-mlo=0.497\n",
      "Iter=1030, avg train loss=0.291, \n",
      "\tAvg Val Loss: same-cc=0.574, AUC: same-cc=0.638 \n",
      "\tAvg Val Loss: same-mlo=0.713, AUC: same-mlo=0.578 \n",
      "\tAvg Val Loss opp-cc=0.704, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.786, AUC: opp-mlo=0.474\n",
      "Iter=1035, avg train loss=0.350, \n",
      "\tAvg Val Loss: same-cc=0.562, AUC: same-cc=0.674 \n",
      "\tAvg Val Loss: same-mlo=0.777, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.725, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.784, AUC: opp-mlo=0.478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1040, avg train loss=0.381, \n",
      "\tAvg Val Loss: same-cc=0.576, AUC: same-cc=0.679 \n",
      "\tAvg Val Loss: same-mlo=0.962, AUC: same-mlo=0.590 \n",
      "\tAvg Val Loss opp-cc=0.849, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.994, AUC: opp-mlo=0.477\n",
      "Best models loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training: same-cc=0.483, same-mlo=0.668, opp-cc=0.629, opp-mlo=0.637\n",
      "Max-Score Based AUC After Training: 0.527, Mean-Score Based AUC After Training: 0.583\n",
      "\n",
      "\n",
      "\n",
      "========== Fold 5 ==========\n",
      "Test AUC at start: same-cc=0.554, same-mlo=0.484, opp-cc=0.485, opp-mlo=0.444\n",
      "Max-Score Based AUC Before Training: 0.533, Mean-Score Based AUC Before Training: 0.503\n",
      "Iter=5, avg train loss=0.919, \n",
      "\tAvg Val Loss: same-cc=0.662, AUC: same-cc=0.412 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.462 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.461 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.388\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=10, avg train loss=1.037, \n",
      "\tAvg Val Loss: same-cc=0.663, AUC: same-cc=0.430 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.483 \n",
      "\tAvg Val Loss opp-cc=0.637, AUC: opp-cc=0.466 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.390\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=15, avg train loss=1.111, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.438 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.491 \n",
      "\tAvg Val Loss opp-cc=0.646, AUC: opp-cc=0.463 \n",
      "\tAvg Val Loss: opp-mlo=0.643, AUC: opp-mlo=0.395\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=20, avg train loss=0.953, \n",
      "\tAvg Val Loss: same-cc=0.667, AUC: same-cc=0.435 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.492 \n",
      "\tAvg Val Loss opp-cc=0.653, AUC: opp-cc=0.455 \n",
      "\tAvg Val Loss: opp-mlo=0.633, AUC: opp-mlo=0.409\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=25, avg train loss=0.876, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.437 \n",
      "\tAvg Val Loss: same-mlo=0.637, AUC: same-mlo=0.494 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.630, AUC: opp-mlo=0.416\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=30, avg train loss=0.846, \n",
      "\tAvg Val Loss: same-cc=0.661, AUC: same-cc=0.457 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.520 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.468 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.449\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=35, avg train loss=0.698, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.468 \n",
      "\tAvg Val Loss: same-mlo=0.624, AUC: same-mlo=0.555 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.454 \n",
      "\tAvg Val Loss: opp-mlo=0.627, AUC: opp-mlo=0.490\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=40, avg train loss=0.821, \n",
      "\tAvg Val Loss: same-cc=0.646, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.625, AUC: same-mlo=0.581 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.455 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.519\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=45, avg train loss=0.678, \n",
      "\tAvg Val Loss: same-cc=0.632, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.609, AUC: same-mlo=0.591 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.471 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.543\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=50, avg train loss=0.639, \n",
      "\tAvg Val Loss: same-cc=0.638, AUC: same-cc=0.539 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.667, AUC: opp-cc=0.482 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.540\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Iter=55, avg train loss=0.678, \n",
      "\tAvg Val Loss: same-cc=0.615, AUC: same-cc=0.538 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.625, AUC: opp-cc=0.492 \n",
      "\tAvg Val Loss: opp-mlo=0.589, AUC: opp-mlo=0.564\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=60, avg train loss=0.722, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.532 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.629, AUC: opp-cc=0.480 \n",
      "\tAvg Val Loss: opp-mlo=0.593, AUC: opp-mlo=0.568\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=65, avg train loss=0.669, \n",
      "\tAvg Val Loss: same-cc=0.632, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.623 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.477 \n",
      "\tAvg Val Loss: opp-mlo=0.596, AUC: opp-mlo=0.573\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=70, avg train loss=0.715, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.526 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.484 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.561\n",
      "Iter=75, avg train loss=0.673, \n",
      "\tAvg Val Loss: same-cc=0.686, AUC: same-cc=0.513 \n",
      "\tAvg Val Loss: same-mlo=0.610, AUC: same-mlo=0.629 \n",
      "\tAvg Val Loss opp-cc=0.665, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.574\n",
      "Best same-mlo model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=80, avg train loss=0.671, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.484 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.571\n",
      "Best opp-cc model saved.\n",
      "Iter=85, avg train loss=0.662, \n",
      "\tAvg Val Loss: same-cc=0.710, AUC: same-cc=0.485 \n",
      "\tAvg Val Loss: same-mlo=0.622, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.695, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.626, AUC: opp-mlo=0.544\n",
      "Iter=90, avg train loss=0.679, \n",
      "\tAvg Val Loss: same-cc=0.699, AUC: same-cc=0.490 \n",
      "\tAvg Val Loss: same-mlo=0.611, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.538\n",
      "Iter=95, avg train loss=0.668, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.495 \n",
      "\tAvg Val Loss: same-mlo=0.616, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.626, AUC: opp-mlo=0.545\n",
      "Iter=100, avg train loss=0.652, \n",
      "\tAvg Val Loss: same-cc=0.658, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.603, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.619, AUC: opp-mlo=0.558\n",
      "Iter=105, avg train loss=0.638, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.514 \n",
      "\tAvg Val Loss: same-mlo=0.604, AUC: same-mlo=0.593 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.484 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.545\n",
      "Iter=110, avg train loss=0.622, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.501 \n",
      "\tAvg Val Loss: same-mlo=0.608, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.480 \n",
      "\tAvg Val Loss: opp-mlo=0.619, AUC: opp-mlo=0.544\n",
      "Iter=115, avg train loss=0.625, \n",
      "\tAvg Val Loss: same-cc=0.632, AUC: same-cc=0.506 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.599 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.483 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.546\n",
      "Iter=120, avg train loss=0.662, \n",
      "\tAvg Val Loss: same-cc=0.622, AUC: same-cc=0.506 \n",
      "\tAvg Val Loss: same-mlo=0.589, AUC: same-mlo=0.591 \n",
      "\tAvg Val Loss opp-cc=0.629, AUC: opp-cc=0.490 \n",
      "\tAvg Val Loss: opp-mlo=0.592, AUC: opp-mlo=0.549\n",
      "Iter=125, avg train loss=0.601, \n",
      "\tAvg Val Loss: same-cc=0.616, AUC: same-cc=0.508 \n",
      "\tAvg Val Loss: same-mlo=0.572, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.621, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.583, AUC: opp-mlo=0.562\n",
      "Iter=130, avg train loss=0.716, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.571, AUC: same-mlo=0.596 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.490 \n",
      "\tAvg Val Loss: opp-mlo=0.588, AUC: opp-mlo=0.557\n",
      "Iter=135, avg train loss=0.624, \n",
      "\tAvg Val Loss: same-cc=0.635, AUC: same-cc=0.521 \n",
      "\tAvg Val Loss: same-mlo=0.564, AUC: same-mlo=0.615 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.582, AUC: opp-mlo=0.582\n",
      "Best opp-mlo model saved.\n",
      "Iter=140, avg train loss=0.651, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.516 \n",
      "\tAvg Val Loss: same-mlo=0.561, AUC: same-mlo=0.621 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.487 \n",
      "\tAvg Val Loss: opp-mlo=0.578, AUC: opp-mlo=0.563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=145, avg train loss=0.715, \n",
      "\tAvg Val Loss: same-cc=0.622, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.561, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.482 \n",
      "\tAvg Val Loss: opp-mlo=0.584, AUC: opp-mlo=0.568\n",
      "Best same-mlo model saved.\n",
      "Iter=150, avg train loss=0.603, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.553, AUC: same-mlo=0.640 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.492 \n",
      "\tAvg Val Loss: opp-mlo=0.580, AUC: opp-mlo=0.569\n",
      "Best same-mlo model saved.\n",
      "Iter=155, avg train loss=0.669, \n",
      "\tAvg Val Loss: same-cc=0.607, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.562, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.633, AUC: opp-cc=0.490 \n",
      "\tAvg Val Loss: opp-mlo=0.583, AUC: opp-mlo=0.565\n",
      "Iter=160, avg train loss=0.643, \n",
      "\tAvg Val Loss: same-cc=0.603, AUC: same-cc=0.551 \n",
      "\tAvg Val Loss: same-mlo=0.555, AUC: same-mlo=0.636 \n",
      "\tAvg Val Loss opp-cc=0.625, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.587, AUC: opp-mlo=0.569\n",
      "Best same-cc model saved.\n",
      "Best opp-cc model saved.\n",
      "Iter=165, avg train loss=0.581, \n",
      "\tAvg Val Loss: same-cc=0.601, AUC: same-cc=0.557 \n",
      "\tAvg Val Loss: same-mlo=0.552, AUC: same-mlo=0.639 \n",
      "\tAvg Val Loss opp-cc=0.617, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.585, AUC: opp-mlo=0.587\n",
      "Best same-cc model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=170, avg train loss=0.670, \n",
      "\tAvg Val Loss: same-cc=0.595, AUC: same-cc=0.561 \n",
      "\tAvg Val Loss: same-mlo=0.542, AUC: same-mlo=0.648 \n",
      "\tAvg Val Loss opp-cc=0.614, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.580, AUC: opp-mlo=0.602\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Best opp-mlo model saved.\n",
      "Iter=175, avg train loss=0.719, \n",
      "\tAvg Val Loss: same-cc=0.589, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.540, AUC: same-mlo=0.656 \n",
      "\tAvg Val Loss opp-cc=0.607, AUC: opp-cc=0.521 \n",
      "\tAvg Val Loss: opp-mlo=0.572, AUC: opp-mlo=0.585\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Iter=180, avg train loss=0.709, \n",
      "\tAvg Val Loss: same-cc=0.590, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.541, AUC: same-mlo=0.666 \n",
      "\tAvg Val Loss opp-cc=0.612, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.568, AUC: opp-mlo=0.596\n",
      "Best same-cc model saved.\n",
      "Best same-mlo model saved.\n",
      "Best opp-cc model saved.\n",
      "Iter=185, avg train loss=0.647, \n",
      "\tAvg Val Loss: same-cc=0.591, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.535, AUC: same-mlo=0.680 \n",
      "\tAvg Val Loss opp-cc=0.618, AUC: opp-cc=0.539 \n",
      "\tAvg Val Loss: opp-mlo=0.572, AUC: opp-mlo=0.590\n",
      "Best same-mlo model saved.\n",
      "Iter=190, avg train loss=0.567, \n",
      "\tAvg Val Loss: same-cc=0.587, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.539, AUC: same-mlo=0.674 \n",
      "\tAvg Val Loss opp-cc=0.619, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.572, AUC: opp-mlo=0.597\n",
      "Best opp-cc model saved.\n",
      "Iter=195, avg train loss=0.638, \n",
      "\tAvg Val Loss: same-cc=0.588, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.539, AUC: same-mlo=0.672 \n",
      "\tAvg Val Loss opp-cc=0.617, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.580, AUC: opp-mlo=0.603\n",
      "Best opp-mlo model saved.\n",
      "Iter=200, avg train loss=0.646, \n",
      "\tAvg Val Loss: same-cc=0.595, AUC: same-cc=0.573 \n",
      "\tAvg Val Loss: same-mlo=0.563, AUC: same-mlo=0.661 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.537 \n",
      "\tAvg Val Loss: opp-mlo=0.595, AUC: opp-mlo=0.591\n",
      "Iter=205, avg train loss=0.599, \n",
      "\tAvg Val Loss: same-cc=0.608, AUC: same-cc=0.565 \n",
      "\tAvg Val Loss: same-mlo=0.569, AUC: same-mlo=0.662 \n",
      "\tAvg Val Loss opp-cc=0.613, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.592, AUC: opp-mlo=0.573\n",
      "Iter=210, avg train loss=0.653, \n",
      "\tAvg Val Loss: same-cc=0.615, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.568, AUC: same-mlo=0.662 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.525 \n",
      "\tAvg Val Loss: opp-mlo=0.599, AUC: opp-mlo=0.570\n",
      "Iter=215, avg train loss=0.655, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.595, AUC: same-mlo=0.649 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.506 \n",
      "\tAvg Val Loss: opp-mlo=0.615, AUC: opp-mlo=0.567\n",
      "Iter=220, avg train loss=0.679, \n",
      "\tAvg Val Loss: same-cc=0.623, AUC: same-cc=0.571 \n",
      "\tAvg Val Loss: same-mlo=0.606, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.567\n",
      "Iter=225, avg train loss=0.649, \n",
      "\tAvg Val Loss: same-cc=0.619, AUC: same-cc=0.553 \n",
      "\tAvg Val Loss: same-mlo=0.594, AUC: same-mlo=0.646 \n",
      "\tAvg Val Loss opp-cc=0.637, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.610, AUC: opp-mlo=0.572\n",
      "Iter=230, avg train loss=0.612, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.557 \n",
      "\tAvg Val Loss: same-mlo=0.615, AUC: same-mlo=0.639 \n",
      "\tAvg Val Loss opp-cc=0.638, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.627, AUC: opp-mlo=0.560\n",
      "Iter=235, avg train loss=0.588, \n",
      "\tAvg Val Loss: same-cc=0.637, AUC: same-cc=0.541 \n",
      "\tAvg Val Loss: same-mlo=0.617, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.638, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.640, AUC: opp-mlo=0.563\n",
      "Iter=240, avg train loss=0.696, \n",
      "\tAvg Val Loss: same-cc=0.627, AUC: same-cc=0.526 \n",
      "\tAvg Val Loss: same-mlo=0.567, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.614, AUC: opp-cc=0.520 \n",
      "\tAvg Val Loss: opp-mlo=0.599, AUC: opp-mlo=0.551\n",
      "Iter=245, avg train loss=0.684, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.559, AUC: same-mlo=0.625 \n",
      "\tAvg Val Loss opp-cc=0.625, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.536\n",
      "Iter=250, avg train loss=0.578, \n",
      "\tAvg Val Loss: same-cc=0.644, AUC: same-cc=0.517 \n",
      "\tAvg Val Loss: same-mlo=0.560, AUC: same-mlo=0.618 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.516 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.528\n",
      "Iter=255, avg train loss=0.504, \n",
      "\tAvg Val Loss: same-cc=0.640, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.568, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.599, AUC: opp-mlo=0.522\n",
      "Iter=260, avg train loss=0.675, \n",
      "\tAvg Val Loss: same-cc=0.649, AUC: same-cc=0.504 \n",
      "\tAvg Val Loss: same-mlo=0.579, AUC: same-mlo=0.607 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.509 \n",
      "\tAvg Val Loss: opp-mlo=0.598, AUC: opp-mlo=0.517\n",
      "Iter=265, avg train loss=0.711, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.508 \n",
      "\tAvg Val Loss: opp-mlo=0.601, AUC: opp-mlo=0.492\n",
      "Iter=270, avg train loss=0.582, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.509 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.602 \n",
      "\tAvg Val Loss opp-cc=0.666, AUC: opp-cc=0.495 \n",
      "\tAvg Val Loss: opp-mlo=0.625, AUC: opp-mlo=0.479\n",
      "Iter=275, avg train loss=0.697, \n",
      "\tAvg Val Loss: same-cc=0.679, AUC: same-cc=0.524 \n",
      "\tAvg Val Loss: same-mlo=0.614, AUC: same-mlo=0.591 \n",
      "\tAvg Val Loss opp-cc=0.678, AUC: opp-cc=0.485 \n",
      "\tAvg Val Loss: opp-mlo=0.646, AUC: opp-mlo=0.469\n",
      "Iter=280, avg train loss=0.663, \n",
      "\tAvg Val Loss: same-cc=0.705, AUC: same-cc=0.531 \n",
      "\tAvg Val Loss: same-mlo=0.651, AUC: same-mlo=0.607 \n",
      "\tAvg Val Loss opp-cc=0.702, AUC: opp-cc=0.471 \n",
      "\tAvg Val Loss: opp-mlo=0.689, AUC: opp-mlo=0.473\n",
      "Iter=285, avg train loss=0.639, \n",
      "\tAvg Val Loss: same-cc=0.697, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.646, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.726, AUC: opp-cc=0.458 \n",
      "\tAvg Val Loss: opp-mlo=0.690, AUC: opp-mlo=0.463\n",
      "Iter=290, avg train loss=0.672, \n",
      "\tAvg Val Loss: same-cc=0.713, AUC: same-cc=0.520 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.782, AUC: opp-cc=0.466 \n",
      "\tAvg Val Loss: opp-mlo=0.715, AUC: opp-mlo=0.461\n",
      "Iter=295, avg train loss=0.640, \n",
      "\tAvg Val Loss: same-cc=0.720, AUC: same-cc=0.526 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.781, AUC: opp-cc=0.469 \n",
      "\tAvg Val Loss: opp-mlo=0.725, AUC: opp-mlo=0.461\n",
      "Iter=300, avg train loss=0.586, \n",
      "\tAvg Val Loss: same-cc=0.670, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.630 \n",
      "\tAvg Val Loss opp-cc=0.726, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.708, AUC: opp-mlo=0.459\n",
      "Iter=305, avg train loss=0.656, \n",
      "\tAvg Val Loss: same-cc=0.661, AUC: same-cc=0.500 \n",
      "\tAvg Val Loss: same-mlo=0.590, AUC: same-mlo=0.632 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.513 \n",
      "\tAvg Val Loss: opp-mlo=0.689, AUC: opp-mlo=0.452\n",
      "Iter=310, avg train loss=0.591, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.507 \n",
      "\tAvg Val Loss: same-mlo=0.580, AUC: same-mlo=0.619 \n",
      "\tAvg Val Loss opp-cc=0.654, AUC: opp-cc=0.514 \n",
      "\tAvg Val Loss: opp-mlo=0.659, AUC: opp-mlo=0.474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=315, avg train loss=0.642, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.510 \n",
      "\tAvg Val Loss: same-mlo=0.579, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.634, AUC: opp-cc=0.526 \n",
      "\tAvg Val Loss: opp-mlo=0.652, AUC: opp-mlo=0.485\n",
      "Iter=320, avg train loss=0.565, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.507 \n",
      "\tAvg Val Loss: same-mlo=0.579, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.619, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.631, AUC: opp-mlo=0.506\n",
      "Iter=325, avg train loss=0.552, \n",
      "\tAvg Val Loss: same-cc=0.637, AUC: same-cc=0.499 \n",
      "\tAvg Val Loss: same-mlo=0.578, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.604, AUC: opp-cc=0.527 \n",
      "\tAvg Val Loss: opp-mlo=0.622, AUC: opp-mlo=0.510\n",
      "Iter=330, avg train loss=0.590, \n",
      "\tAvg Val Loss: same-cc=0.625, AUC: same-cc=0.517 \n",
      "\tAvg Val Loss: same-mlo=0.580, AUC: same-mlo=0.614 \n",
      "\tAvg Val Loss opp-cc=0.599, AUC: opp-cc=0.486 \n",
      "\tAvg Val Loss: opp-mlo=0.609, AUC: opp-mlo=0.504\n",
      "Iter=335, avg train loss=0.728, \n",
      "\tAvg Val Loss: same-cc=0.612, AUC: same-cc=0.513 \n",
      "\tAvg Val Loss: same-mlo=0.582, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.595, AUC: opp-cc=0.500 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.506\n",
      "Iter=340, avg train loss=0.559, \n",
      "\tAvg Val Loss: same-cc=0.612, AUC: same-cc=0.535 \n",
      "\tAvg Val Loss: same-mlo=0.579, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.615, AUC: opp-cc=0.523 \n",
      "\tAvg Val Loss: opp-mlo=0.600, AUC: opp-mlo=0.514\n",
      "Iter=345, avg train loss=0.647, \n",
      "\tAvg Val Loss: same-cc=0.629, AUC: same-cc=0.541 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.608 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.522 \n",
      "\tAvg Val Loss: opp-mlo=0.608, AUC: opp-mlo=0.509\n",
      "Iter=350, avg train loss=0.569, \n",
      "\tAvg Val Loss: same-cc=0.656, AUC: same-cc=0.548 \n",
      "\tAvg Val Loss: same-mlo=0.662, AUC: same-mlo=0.624 \n",
      "\tAvg Val Loss opp-cc=0.702, AUC: opp-cc=0.519 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.510\n",
      "Iter=355, avg train loss=0.589, \n",
      "\tAvg Val Loss: same-cc=0.650, AUC: same-cc=0.545 \n",
      "\tAvg Val Loss: same-mlo=0.670, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.530 \n",
      "\tAvg Val Loss: opp-mlo=0.634, AUC: opp-mlo=0.508\n",
      "Iter=360, avg train loss=0.641, \n",
      "\tAvg Val Loss: same-cc=0.652, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.695, AUC: same-mlo=0.589 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.528 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.494\n",
      "Iter=365, avg train loss=0.599, \n",
      "\tAvg Val Loss: same-cc=0.655, AUC: same-cc=0.540 \n",
      "\tAvg Val Loss: same-mlo=0.705, AUC: same-mlo=0.570 \n",
      "\tAvg Val Loss opp-cc=0.659, AUC: opp-cc=0.515 \n",
      "\tAvg Val Loss: opp-mlo=0.685, AUC: opp-mlo=0.487\n",
      "Iter=370, avg train loss=0.666, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.741, AUC: same-mlo=0.540 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.673, AUC: opp-mlo=0.484\n",
      "Iter=375, avg train loss=0.640, \n",
      "\tAvg Val Loss: same-cc=0.687, AUC: same-cc=0.506 \n",
      "\tAvg Val Loss: same-mlo=0.728, AUC: same-mlo=0.538 \n",
      "\tAvg Val Loss opp-cc=0.658, AUC: opp-cc=0.516 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.473\n",
      "Iter=380, avg train loss=0.582, \n",
      "\tAvg Val Loss: same-cc=0.702, AUC: same-cc=0.504 \n",
      "\tAvg Val Loss: same-mlo=0.698, AUC: same-mlo=0.539 \n",
      "\tAvg Val Loss opp-cc=0.684, AUC: opp-cc=0.497 \n",
      "\tAvg Val Loss: opp-mlo=0.672, AUC: opp-mlo=0.466\n",
      "Iter=385, avg train loss=0.616, \n",
      "\tAvg Val Loss: same-cc=0.735, AUC: same-cc=0.493 \n",
      "\tAvg Val Loss: same-mlo=0.697, AUC: same-mlo=0.534 \n",
      "\tAvg Val Loss opp-cc=0.705, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.458\n",
      "Iter=390, avg train loss=0.587, \n",
      "\tAvg Val Loss: same-cc=0.718, AUC: same-cc=0.500 \n",
      "\tAvg Val Loss: same-mlo=0.686, AUC: same-mlo=0.542 \n",
      "\tAvg Val Loss opp-cc=0.683, AUC: opp-cc=0.479 \n",
      "\tAvg Val Loss: opp-mlo=0.687, AUC: opp-mlo=0.443\n",
      "Iter=395, avg train loss=0.522, \n",
      "\tAvg Val Loss: same-cc=0.690, AUC: same-cc=0.476 \n",
      "\tAvg Val Loss: same-mlo=0.654, AUC: same-mlo=0.548 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.496 \n",
      "\tAvg Val Loss: opp-mlo=0.671, AUC: opp-mlo=0.466\n",
      "Iter=400, avg train loss=0.605, \n",
      "\tAvg Val Loss: same-cc=0.674, AUC: same-cc=0.484 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.563 \n",
      "\tAvg Val Loss opp-cc=0.643, AUC: opp-cc=0.484 \n",
      "\tAvg Val Loss: opp-mlo=0.646, AUC: opp-mlo=0.478\n",
      "Iter=405, avg train loss=0.596, \n",
      "\tAvg Val Loss: same-cc=0.679, AUC: same-cc=0.482 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.570 \n",
      "\tAvg Val Loss opp-cc=0.637, AUC: opp-cc=0.506 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.478\n",
      "Iter=410, avg train loss=0.556, \n",
      "\tAvg Val Loss: same-cc=0.713, AUC: same-cc=0.496 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.571 \n",
      "\tAvg Val Loss opp-cc=0.651, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.476\n",
      "Iter=415, avg train loss=0.572, \n",
      "\tAvg Val Loss: same-cc=0.710, AUC: same-cc=0.514 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.578 \n",
      "\tAvg Val Loss opp-cc=0.662, AUC: opp-cc=0.524 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.481\n",
      "Iter=420, avg train loss=0.534, \n",
      "\tAvg Val Loss: same-cc=0.692, AUC: same-cc=0.523 \n",
      "\tAvg Val Loss: same-mlo=0.643, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.534 \n",
      "\tAvg Val Loss: opp-mlo=0.616, AUC: opp-mlo=0.484\n",
      "Iter=425, avg train loss=0.609, \n",
      "\tAvg Val Loss: same-cc=0.732, AUC: same-cc=0.517 \n",
      "\tAvg Val Loss: same-mlo=0.634, AUC: same-mlo=0.593 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.553 \n",
      "\tAvg Val Loss: opp-mlo=0.623, AUC: opp-mlo=0.498\n",
      "Best opp-cc model saved.\n",
      "Iter=430, avg train loss=0.611, \n",
      "\tAvg Val Loss: same-cc=0.712, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.612, AUC: same-mlo=0.609 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.494\n",
      "Iter=435, avg train loss=0.600, \n",
      "\tAvg Val Loss: same-cc=0.719, AUC: same-cc=0.520 \n",
      "\tAvg Val Loss: same-mlo=0.600, AUC: same-mlo=0.627 \n",
      "\tAvg Val Loss opp-cc=0.704, AUC: opp-cc=0.545 \n",
      "\tAvg Val Loss: opp-mlo=0.661, AUC: opp-mlo=0.502\n",
      "Iter=440, avg train loss=0.478, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.510 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.633 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.664, AUC: opp-mlo=0.497\n",
      "Iter=445, avg train loss=0.591, \n",
      "\tAvg Val Loss: same-cc=0.689, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.579, AUC: same-mlo=0.642 \n",
      "\tAvg Val Loss opp-cc=0.660, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.667, AUC: opp-mlo=0.509\n",
      "Best opp-cc model saved.\n",
      "Iter=450, avg train loss=0.592, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.545 \n",
      "\tAvg Val Loss: same-mlo=0.571, AUC: same-mlo=0.645 \n",
      "\tAvg Val Loss opp-cc=0.639, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.644, AUC: opp-mlo=0.525\n",
      "Best opp-cc model saved.\n",
      "Iter=455, avg train loss=0.481, \n",
      "\tAvg Val Loss: same-cc=0.667, AUC: same-cc=0.524 \n",
      "\tAvg Val Loss: same-mlo=0.567, AUC: same-mlo=0.645 \n",
      "\tAvg Val Loss opp-cc=0.620, AUC: opp-cc=0.557 \n",
      "\tAvg Val Loss: opp-mlo=0.624, AUC: opp-mlo=0.528\n",
      "Iter=460, avg train loss=0.480, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.502 \n",
      "\tAvg Val Loss: same-mlo=0.571, AUC: same-mlo=0.640 \n",
      "\tAvg Val Loss opp-cc=0.616, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.547\n",
      "Iter=465, avg train loss=0.645, \n",
      "\tAvg Val Loss: same-cc=0.641, AUC: same-cc=0.519 \n",
      "\tAvg Val Loss: same-mlo=0.560, AUC: same-mlo=0.647 \n",
      "\tAvg Val Loss opp-cc=0.611, AUC: opp-cc=0.556 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.544\n",
      "Iter=470, avg train loss=0.541, \n",
      "\tAvg Val Loss: same-cc=0.631, AUC: same-cc=0.532 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.657 \n",
      "\tAvg Val Loss opp-cc=0.627, AUC: opp-cc=0.570 \n",
      "\tAvg Val Loss: opp-mlo=0.634, AUC: opp-mlo=0.523\n",
      "Iter=475, avg train loss=0.575, \n",
      "\tAvg Val Loss: same-cc=0.656, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.597, AUC: same-mlo=0.670 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.584 \n",
      "\tAvg Val Loss: opp-mlo=0.665, AUC: opp-mlo=0.549\n",
      "Best opp-cc model saved.\n",
      "Iter=480, avg train loss=0.550, \n",
      "\tAvg Val Loss: same-cc=0.677, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=0.645, AUC: same-mlo=0.670 \n",
      "\tAvg Val Loss opp-cc=0.661, AUC: opp-cc=0.591 \n",
      "\tAvg Val Loss: opp-mlo=0.728, AUC: opp-mlo=0.531\n",
      "Best opp-cc model saved.\n",
      "Iter=485, avg train loss=0.543, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.579 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.660 \n",
      "\tAvg Val Loss opp-cc=0.621, AUC: opp-cc=0.571 \n",
      "\tAvg Val Loss: opp-mlo=0.678, AUC: opp-mlo=0.509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=490, avg train loss=0.473, \n",
      "\tAvg Val Loss: same-cc=0.667, AUC: same-cc=0.559 \n",
      "\tAvg Val Loss: same-mlo=0.585, AUC: same-mlo=0.661 \n",
      "\tAvg Val Loss opp-cc=0.635, AUC: opp-cc=0.585 \n",
      "\tAvg Val Loss: opp-mlo=0.646, AUC: opp-mlo=0.535\n",
      "Iter=495, avg train loss=0.543, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.539 \n",
      "\tAvg Val Loss: same-mlo=0.552, AUC: same-mlo=0.664 \n",
      "\tAvg Val Loss opp-cc=0.631, AUC: opp-cc=0.570 \n",
      "\tAvg Val Loss: opp-mlo=0.629, AUC: opp-mlo=0.539\n",
      "Iter=500, avg train loss=0.512, \n",
      "\tAvg Val Loss: same-cc=0.639, AUC: same-cc=0.553 \n",
      "\tAvg Val Loss: same-mlo=0.551, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.609, AUC: opp-cc=0.562 \n",
      "\tAvg Val Loss: opp-mlo=0.606, AUC: opp-mlo=0.548\n",
      "Iter=505, avg train loss=0.603, \n",
      "\tAvg Val Loss: same-cc=0.637, AUC: same-cc=0.539 \n",
      "\tAvg Val Loss: same-mlo=0.557, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.598, AUC: opp-cc=0.564 \n",
      "\tAvg Val Loss: opp-mlo=0.605, AUC: opp-mlo=0.560\n",
      "Iter=510, avg train loss=0.541, \n",
      "\tAvg Val Loss: same-cc=0.636, AUC: same-cc=0.546 \n",
      "\tAvg Val Loss: same-mlo=0.564, AUC: same-mlo=0.665 \n",
      "\tAvg Val Loss opp-cc=0.600, AUC: opp-cc=0.573 \n",
      "\tAvg Val Loss: opp-mlo=0.614, AUC: opp-mlo=0.568\n",
      "Iter=515, avg train loss=0.575, \n",
      "\tAvg Val Loss: same-cc=0.644, AUC: same-cc=0.560 \n",
      "\tAvg Val Loss: same-mlo=0.572, AUC: same-mlo=0.661 \n",
      "\tAvg Val Loss opp-cc=0.623, AUC: opp-cc=0.579 \n",
      "\tAvg Val Loss: opp-mlo=0.612, AUC: opp-mlo=0.569\n",
      "Iter=520, avg train loss=0.580, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.549 \n",
      "\tAvg Val Loss: same-mlo=0.555, AUC: same-mlo=0.664 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.620, AUC: opp-mlo=0.552\n",
      "Iter=525, avg train loss=0.572, \n",
      "\tAvg Val Loss: same-cc=0.693, AUC: same-cc=0.553 \n",
      "\tAvg Val Loss: same-mlo=0.560, AUC: same-mlo=0.654 \n",
      "\tAvg Val Loss opp-cc=0.668, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.618, AUC: opp-mlo=0.551\n",
      "Iter=530, avg train loss=0.490, \n",
      "\tAvg Val Loss: same-cc=0.768, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.577, AUC: same-mlo=0.659 \n",
      "\tAvg Val Loss opp-cc=0.710, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.541\n",
      "Iter=535, avg train loss=0.531, \n",
      "\tAvg Val Loss: same-cc=0.683, AUC: same-cc=0.528 \n",
      "\tAvg Val Loss: same-mlo=0.587, AUC: same-mlo=0.661 \n",
      "\tAvg Val Loss opp-cc=0.693, AUC: opp-cc=0.553 \n",
      "\tAvg Val Loss: opp-mlo=0.628, AUC: opp-mlo=0.529\n",
      "Iter=540, avg train loss=0.536, \n",
      "\tAvg Val Loss: same-cc=0.654, AUC: same-cc=0.529 \n",
      "\tAvg Val Loss: same-mlo=0.658, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.577 \n",
      "\tAvg Val Loss: opp-mlo=0.640, AUC: opp-mlo=0.529\n",
      "Iter=545, avg train loss=0.532, \n",
      "\tAvg Val Loss: same-cc=0.641, AUC: same-cc=0.528 \n",
      "\tAvg Val Loss: same-mlo=0.664, AUC: same-mlo=0.657 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.570 \n",
      "\tAvg Val Loss: opp-mlo=0.638, AUC: opp-mlo=0.509\n",
      "Iter=550, avg train loss=0.577, \n",
      "\tAvg Val Loss: same-cc=0.633, AUC: same-cc=0.515 \n",
      "\tAvg Val Loss: same-mlo=0.574, AUC: same-mlo=0.648 \n",
      "\tAvg Val Loss opp-cc=0.613, AUC: opp-cc=0.593 \n",
      "\tAvg Val Loss: opp-mlo=0.610, AUC: opp-mlo=0.522\n",
      "Best opp-cc model saved.\n",
      "Iter=555, avg train loss=0.509, \n",
      "\tAvg Val Loss: same-cc=0.710, AUC: same-cc=0.498 \n",
      "\tAvg Val Loss: same-mlo=0.570, AUC: same-mlo=0.650 \n",
      "\tAvg Val Loss opp-cc=0.612, AUC: opp-cc=0.603 \n",
      "\tAvg Val Loss: opp-mlo=0.631, AUC: opp-mlo=0.519\n",
      "Best opp-cc model saved.\n",
      "Iter=560, avg train loss=0.474, \n",
      "\tAvg Val Loss: same-cc=0.737, AUC: same-cc=0.527 \n",
      "\tAvg Val Loss: same-mlo=0.581, AUC: same-mlo=0.643 \n",
      "\tAvg Val Loss opp-cc=0.637, AUC: opp-cc=0.597 \n",
      "\tAvg Val Loss: opp-mlo=0.651, AUC: opp-mlo=0.518\n",
      "Iter=565, avg train loss=0.420, \n",
      "\tAvg Val Loss: same-cc=0.672, AUC: same-cc=0.557 \n",
      "\tAvg Val Loss: same-mlo=0.575, AUC: same-mlo=0.649 \n",
      "\tAvg Val Loss opp-cc=0.595, AUC: opp-cc=0.627 \n",
      "\tAvg Val Loss: opp-mlo=0.674, AUC: opp-mlo=0.519\n",
      "Best opp-cc model saved.\n",
      "Iter=570, avg train loss=0.496, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.585, AUC: same-mlo=0.666 \n",
      "\tAvg Val Loss opp-cc=0.594, AUC: opp-cc=0.640 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.522\n",
      "Best opp-cc model saved.\n",
      "Iter=575, avg train loss=0.455, \n",
      "\tAvg Val Loss: same-cc=0.680, AUC: same-cc=0.586 \n",
      "\tAvg Val Loss: same-mlo=0.593, AUC: same-mlo=0.670 \n",
      "\tAvg Val Loss opp-cc=0.610, AUC: opp-cc=0.644 \n",
      "\tAvg Val Loss: opp-mlo=0.680, AUC: opp-mlo=0.504\n",
      "Best opp-cc model saved.\n",
      "Iter=580, avg train loss=0.450, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.589, AUC: same-mlo=0.658 \n",
      "\tAvg Val Loss opp-cc=0.589, AUC: opp-cc=0.638 \n",
      "\tAvg Val Loss: opp-mlo=0.655, AUC: opp-mlo=0.513\n",
      "Iter=585, avg train loss=0.540, \n",
      "\tAvg Val Loss: same-cc=0.648, AUC: same-cc=0.584 \n",
      "\tAvg Val Loss: same-mlo=0.640, AUC: same-mlo=0.653 \n",
      "\tAvg Val Loss opp-cc=0.583, AUC: opp-cc=0.639 \n",
      "\tAvg Val Loss: opp-mlo=0.652, AUC: opp-mlo=0.503\n",
      "Iter=590, avg train loss=0.498, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.575 \n",
      "\tAvg Val Loss: same-mlo=0.675, AUC: same-mlo=0.651 \n",
      "\tAvg Val Loss opp-cc=0.602, AUC: opp-cc=0.625 \n",
      "\tAvg Val Loss: opp-mlo=0.701, AUC: opp-mlo=0.505\n",
      "Iter=595, avg train loss=0.541, \n",
      "\tAvg Val Loss: same-cc=0.740, AUC: same-cc=0.567 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.644 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.602 \n",
      "\tAvg Val Loss: opp-mlo=0.753, AUC: opp-mlo=0.508\n",
      "Iter=600, avg train loss=0.533, \n",
      "\tAvg Val Loss: same-cc=0.778, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.672, AUC: same-mlo=0.637 \n",
      "\tAvg Val Loss opp-cc=0.640, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.728, AUC: opp-mlo=0.523\n",
      "Iter=605, avg train loss=0.511, \n",
      "\tAvg Val Loss: same-cc=0.739, AUC: same-cc=0.565 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.640 \n",
      "\tAvg Val Loss opp-cc=0.628, AUC: opp-cc=0.584 \n",
      "\tAvg Val Loss: opp-mlo=0.704, AUC: opp-mlo=0.536\n",
      "Iter=610, avg train loss=0.555, \n",
      "\tAvg Val Loss: same-cc=0.681, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.641, AUC: same-mlo=0.640 \n",
      "\tAvg Val Loss opp-cc=0.622, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.655, AUC: opp-mlo=0.535\n",
      "Iter=615, avg train loss=0.590, \n",
      "\tAvg Val Loss: same-cc=0.721, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.644, AUC: same-mlo=0.631 \n",
      "\tAvg Val Loss opp-cc=0.673, AUC: opp-cc=0.570 \n",
      "\tAvg Val Loss: opp-mlo=0.666, AUC: opp-mlo=0.523\n",
      "Iter=620, avg train loss=0.474, \n",
      "\tAvg Val Loss: same-cc=0.703, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.672, AUC: same-mlo=0.612 \n",
      "\tAvg Val Loss opp-cc=0.698, AUC: opp-cc=0.550 \n",
      "\tAvg Val Loss: opp-mlo=0.698, AUC: opp-mlo=0.495\n",
      "Best same-cc model saved.\n",
      "Iter=625, avg train loss=0.471, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.640, AUC: same-mlo=0.603 \n",
      "\tAvg Val Loss opp-cc=0.653, AUC: opp-cc=0.573 \n",
      "\tAvg Val Loss: opp-mlo=0.697, AUC: opp-mlo=0.487\n",
      "Iter=630, avg train loss=0.428, \n",
      "\tAvg Val Loss: same-cc=0.694, AUC: same-cc=0.563 \n",
      "\tAvg Val Loss: same-mlo=0.629, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.603 \n",
      "\tAvg Val Loss: opp-mlo=0.697, AUC: opp-mlo=0.479\n",
      "Iter=635, avg train loss=0.410, \n",
      "\tAvg Val Loss: same-cc=0.666, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.691, AUC: opp-cc=0.559 \n",
      "\tAvg Val Loss: opp-mlo=0.697, AUC: opp-mlo=0.469\n",
      "Iter=640, avg train loss=0.438, \n",
      "\tAvg Val Loss: same-cc=0.688, AUC: same-cc=0.601 \n",
      "\tAvg Val Loss: same-mlo=0.641, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.711, AUC: opp-cc=0.582 \n",
      "\tAvg Val Loss: opp-mlo=0.704, AUC: opp-mlo=0.469\n",
      "Best same-cc model saved.\n",
      "Iter=645, avg train loss=0.413, \n",
      "\tAvg Val Loss: same-cc=0.735, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.635, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.725, AUC: opp-cc=0.588 \n",
      "\tAvg Val Loss: opp-mlo=0.696, AUC: opp-mlo=0.474\n",
      "Iter=650, avg train loss=0.444, \n",
      "\tAvg Val Loss: same-cc=0.682, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.648, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.597 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.480\n",
      "Iter=655, avg train loss=0.507, \n",
      "\tAvg Val Loss: same-cc=0.665, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.593 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.592 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.484\n",
      "Iter=660, avg train loss=0.505, \n",
      "\tAvg Val Loss: same-cc=0.726, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.645, AUC: same-mlo=0.610 \n",
      "\tAvg Val Loss opp-cc=0.749, AUC: opp-cc=0.591 \n",
      "\tAvg Val Loss: opp-mlo=0.740, AUC: opp-mlo=0.490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=665, avg train loss=0.601, \n",
      "\tAvg Val Loss: same-cc=0.715, AUC: same-cc=0.597 \n",
      "\tAvg Val Loss: same-mlo=0.655, AUC: same-mlo=0.606 \n",
      "\tAvg Val Loss opp-cc=0.723, AUC: opp-cc=0.609 \n",
      "\tAvg Val Loss: opp-mlo=0.749, AUC: opp-mlo=0.490\n",
      "Iter=670, avg train loss=0.390, \n",
      "\tAvg Val Loss: same-cc=0.664, AUC: same-cc=0.612 \n",
      "\tAvg Val Loss: same-mlo=0.638, AUC: same-mlo=0.616 \n",
      "\tAvg Val Loss opp-cc=0.636, AUC: opp-cc=0.603 \n",
      "\tAvg Val Loss: opp-mlo=0.695, AUC: opp-mlo=0.491\n",
      "Best same-cc model saved.\n",
      "Iter=675, avg train loss=0.473, \n",
      "\tAvg Val Loss: same-cc=0.620, AUC: same-cc=0.597 \n",
      "\tAvg Val Loss: same-mlo=0.621, AUC: same-mlo=0.617 \n",
      "\tAvg Val Loss opp-cc=0.626, AUC: opp-cc=0.578 \n",
      "\tAvg Val Loss: opp-mlo=0.679, AUC: opp-mlo=0.506\n",
      "Iter=680, avg train loss=0.519, \n",
      "\tAvg Val Loss: same-cc=0.612, AUC: same-cc=0.575 \n",
      "\tAvg Val Loss: same-mlo=0.627, AUC: same-mlo=0.622 \n",
      "\tAvg Val Loss opp-cc=0.621, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.650, AUC: opp-mlo=0.523\n",
      "Iter=685, avg train loss=0.560, \n",
      "\tAvg Val Loss: same-cc=0.613, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=0.655, AUC: same-mlo=0.601 \n",
      "\tAvg Val Loss opp-cc=0.643, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.655, AUC: opp-mlo=0.523\n",
      "Iter=690, avg train loss=0.286, \n",
      "\tAvg Val Loss: same-cc=0.653, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=0.660, AUC: same-mlo=0.595 \n",
      "\tAvg Val Loss opp-cc=0.700, AUC: opp-cc=0.556 \n",
      "\tAvg Val Loss: opp-mlo=0.738, AUC: opp-mlo=0.513\n",
      "Iter=695, avg train loss=0.444, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.698, AUC: same-mlo=0.611 \n",
      "\tAvg Val Loss opp-cc=0.721, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.739, AUC: opp-mlo=0.519\n",
      "Iter=700, avg train loss=0.478, \n",
      "\tAvg Val Loss: same-cc=0.766, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.800, AUC: same-mlo=0.589 \n",
      "\tAvg Val Loss opp-cc=0.769, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.782, AUC: opp-mlo=0.498\n",
      "Iter=705, avg train loss=0.399, \n",
      "\tAvg Val Loss: same-cc=0.701, AUC: same-cc=0.602 \n",
      "\tAvg Val Loss: same-mlo=0.838, AUC: same-mlo=0.589 \n",
      "\tAvg Val Loss opp-cc=0.721, AUC: opp-cc=0.571 \n",
      "\tAvg Val Loss: opp-mlo=0.744, AUC: opp-mlo=0.505\n",
      "Iter=710, avg train loss=0.497, \n",
      "\tAvg Val Loss: same-cc=0.642, AUC: same-cc=0.611 \n",
      "\tAvg Val Loss: same-mlo=0.868, AUC: same-mlo=0.584 \n",
      "\tAvg Val Loss opp-cc=0.674, AUC: opp-cc=0.563 \n",
      "\tAvg Val Loss: opp-mlo=0.755, AUC: opp-mlo=0.491\n",
      "Iter=715, avg train loss=0.436, \n",
      "\tAvg Val Loss: same-cc=0.636, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.750, AUC: same-mlo=0.570 \n",
      "\tAvg Val Loss opp-cc=0.644, AUC: opp-cc=0.580 \n",
      "\tAvg Val Loss: opp-mlo=0.686, AUC: opp-mlo=0.491\n",
      "Iter=720, avg train loss=0.439, \n",
      "\tAvg Val Loss: same-cc=0.626, AUC: same-cc=0.562 \n",
      "\tAvg Val Loss: same-mlo=0.682, AUC: same-mlo=0.567 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.565 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.487\n",
      "Iter=725, avg train loss=0.457, \n",
      "\tAvg Val Loss: same-cc=0.682, AUC: same-cc=0.562 \n",
      "\tAvg Val Loss: same-mlo=0.681, AUC: same-mlo=0.561 \n",
      "\tAvg Val Loss opp-cc=0.716, AUC: opp-cc=0.571 \n",
      "\tAvg Val Loss: opp-mlo=0.708, AUC: opp-mlo=0.480\n",
      "Iter=730, avg train loss=0.500, \n",
      "\tAvg Val Loss: same-cc=0.942, AUC: same-cc=0.567 \n",
      "\tAvg Val Loss: same-mlo=0.700, AUC: same-mlo=0.559 \n",
      "\tAvg Val Loss opp-cc=0.879, AUC: opp-cc=0.557 \n",
      "\tAvg Val Loss: opp-mlo=0.842, AUC: opp-mlo=0.486\n",
      "Iter=735, avg train loss=0.397, \n",
      "\tAvg Val Loss: same-cc=1.050, AUC: same-cc=0.571 \n",
      "\tAvg Val Loss: same-mlo=0.738, AUC: same-mlo=0.558 \n",
      "\tAvg Val Loss opp-cc=0.829, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.911, AUC: opp-mlo=0.495\n",
      "Iter=740, avg train loss=0.466, \n",
      "\tAvg Val Loss: same-cc=0.804, AUC: same-cc=0.577 \n",
      "\tAvg Val Loss: same-mlo=0.754, AUC: same-mlo=0.562 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.858, AUC: opp-mlo=0.476\n",
      "Iter=745, avg train loss=0.394, \n",
      "\tAvg Val Loss: same-cc=0.706, AUC: same-cc=0.556 \n",
      "\tAvg Val Loss: same-mlo=0.708, AUC: same-mlo=0.559 \n",
      "\tAvg Val Loss opp-cc=0.654, AUC: opp-cc=0.558 \n",
      "\tAvg Val Loss: opp-mlo=0.771, AUC: opp-mlo=0.469\n",
      "Iter=750, avg train loss=0.457, \n",
      "\tAvg Val Loss: same-cc=0.685, AUC: same-cc=0.571 \n",
      "\tAvg Val Loss: same-mlo=0.694, AUC: same-mlo=0.562 \n",
      "\tAvg Val Loss opp-cc=0.731, AUC: opp-cc=0.549 \n",
      "\tAvg Val Loss: opp-mlo=0.767, AUC: opp-mlo=0.463\n",
      "Iter=755, avg train loss=0.402, \n",
      "\tAvg Val Loss: same-cc=0.711, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.675, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.794, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.737, AUC: opp-mlo=0.458\n",
      "Iter=760, avg train loss=0.414, \n",
      "\tAvg Val Loss: same-cc=0.750, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.689, AUC: same-mlo=0.574 \n",
      "\tAvg Val Loss opp-cc=0.773, AUC: opp-cc=0.540 \n",
      "\tAvg Val Loss: opp-mlo=0.735, AUC: opp-mlo=0.459\n",
      "Iter=765, avg train loss=0.383, \n",
      "\tAvg Val Loss: same-cc=0.716, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.673, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.694, AUC: opp-cc=0.518 \n",
      "\tAvg Val Loss: opp-mlo=0.752, AUC: opp-mlo=0.453\n",
      "Iter=770, avg train loss=0.381, \n",
      "\tAvg Val Loss: same-cc=0.746, AUC: same-cc=0.587 \n",
      "\tAvg Val Loss: same-mlo=0.663, AUC: same-mlo=0.592 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.541 \n",
      "\tAvg Val Loss: opp-mlo=0.774, AUC: opp-mlo=0.450\n",
      "Iter=775, avg train loss=0.432, \n",
      "\tAvg Val Loss: same-cc=0.743, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.651, AUC: same-mlo=0.578 \n",
      "\tAvg Val Loss opp-cc=0.669, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.726, AUC: opp-mlo=0.439\n",
      "Iter=780, avg train loss=0.420, \n",
      "\tAvg Val Loss: same-cc=0.707, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.702, AUC: opp-mlo=0.448\n",
      "Iter=785, avg train loss=0.313, \n",
      "\tAvg Val Loss: same-cc=0.676, AUC: same-cc=0.543 \n",
      "\tAvg Val Loss: same-mlo=0.682, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.732, AUC: opp-cc=0.498 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.478\n",
      "Iter=790, avg train loss=0.469, \n",
      "\tAvg Val Loss: same-cc=0.675, AUC: same-cc=0.532 \n",
      "\tAvg Val Loss: same-mlo=0.674, AUC: same-mlo=0.579 \n",
      "\tAvg Val Loss opp-cc=0.837, AUC: opp-cc=0.482 \n",
      "\tAvg Val Loss: opp-mlo=0.694, AUC: opp-mlo=0.496\n",
      "Iter=795, avg train loss=0.398, \n",
      "\tAvg Val Loss: same-cc=0.788, AUC: same-cc=0.560 \n",
      "\tAvg Val Loss: same-mlo=0.709, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.985, AUC: opp-cc=0.478 \n",
      "\tAvg Val Loss: opp-mlo=0.707, AUC: opp-mlo=0.496\n",
      "Iter=800, avg train loss=0.475, \n",
      "\tAvg Val Loss: same-cc=0.791, AUC: same-cc=0.569 \n",
      "\tAvg Val Loss: same-mlo=0.719, AUC: same-mlo=0.583 \n",
      "\tAvg Val Loss opp-cc=0.827, AUC: opp-cc=0.480 \n",
      "\tAvg Val Loss: opp-mlo=0.708, AUC: opp-mlo=0.498\n",
      "Iter=805, avg train loss=0.345, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.741, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.748, AUC: opp-cc=0.492 \n",
      "\tAvg Val Loss: opp-mlo=0.743, AUC: opp-mlo=0.475\n",
      "Iter=810, avg train loss=0.288, \n",
      "\tAvg Val Loss: same-cc=0.681, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.731, AUC: same-mlo=0.585 \n",
      "\tAvg Val Loss opp-cc=0.697, AUC: opp-cc=0.503 \n",
      "\tAvg Val Loss: opp-mlo=0.729, AUC: opp-mlo=0.482\n",
      "Iter=815, avg train loss=0.430, \n",
      "\tAvg Val Loss: same-cc=0.698, AUC: same-cc=0.592 \n",
      "\tAvg Val Loss: same-mlo=0.683, AUC: same-mlo=0.587 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.517 \n",
      "\tAvg Val Loss: opp-mlo=0.695, AUC: opp-mlo=0.484\n",
      "Iter=820, avg train loss=0.329, \n",
      "\tAvg Val Loss: same-cc=0.747, AUC: same-cc=0.604 \n",
      "\tAvg Val Loss: same-mlo=0.674, AUC: same-mlo=0.597 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.675, AUC: opp-mlo=0.490\n",
      "Iter=825, avg train loss=0.428, \n",
      "\tAvg Val Loss: same-cc=0.758, AUC: same-cc=0.600 \n",
      "\tAvg Val Loss: same-mlo=0.679, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.688, AUC: opp-mlo=0.493\n",
      "Iter=830, avg train loss=0.369, \n",
      "\tAvg Val Loss: same-cc=0.667, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.681, AUC: same-mlo=0.584 \n",
      "\tAvg Val Loss opp-cc=0.645, AUC: opp-cc=0.567 \n",
      "\tAvg Val Loss: opp-mlo=0.685, AUC: opp-mlo=0.493\n",
      "Iter=835, avg train loss=0.368, \n",
      "\tAvg Val Loss: same-cc=0.643, AUC: same-cc=0.590 \n",
      "\tAvg Val Loss: same-mlo=0.690, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.650, AUC: opp-cc=0.556 \n",
      "\tAvg Val Loss: opp-mlo=0.697, AUC: opp-mlo=0.498\n",
      "Iter=840, avg train loss=0.393, \n",
      "\tAvg Val Loss: same-cc=0.697, AUC: same-cc=0.593 \n",
      "\tAvg Val Loss: same-mlo=0.697, AUC: same-mlo=0.557 \n",
      "\tAvg Val Loss opp-cc=0.653, AUC: opp-cc=0.588 \n",
      "\tAvg Val Loss: opp-mlo=0.742, AUC: opp-mlo=0.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=845, avg train loss=0.307, \n",
      "\tAvg Val Loss: same-cc=0.673, AUC: same-cc=0.588 \n",
      "\tAvg Val Loss: same-mlo=0.729, AUC: same-mlo=0.544 \n",
      "\tAvg Val Loss opp-cc=0.638, AUC: opp-cc=0.595 \n",
      "\tAvg Val Loss: opp-mlo=0.780, AUC: opp-mlo=0.501\n",
      "Iter=850, avg train loss=0.419, \n",
      "\tAvg Val Loss: same-cc=0.700, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.762, AUC: same-mlo=0.538 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.610 \n",
      "\tAvg Val Loss: opp-mlo=0.752, AUC: opp-mlo=0.519\n",
      "Iter=855, avg train loss=0.299, \n",
      "\tAvg Val Loss: same-cc=0.718, AUC: same-cc=0.583 \n",
      "\tAvg Val Loss: same-mlo=0.714, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.655, AUC: opp-cc=0.580 \n",
      "\tAvg Val Loss: opp-mlo=0.712, AUC: opp-mlo=0.512\n",
      "Iter=860, avg train loss=0.389, \n",
      "\tAvg Val Loss: same-cc=0.724, AUC: same-cc=0.577 \n",
      "\tAvg Val Loss: same-mlo=0.762, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.679, AUC: opp-mlo=0.515\n",
      "Iter=865, avg train loss=0.343, \n",
      "\tAvg Val Loss: same-cc=0.711, AUC: same-cc=0.579 \n",
      "\tAvg Val Loss: same-mlo=0.822, AUC: same-mlo=0.535 \n",
      "\tAvg Val Loss opp-cc=0.721, AUC: opp-cc=0.552 \n",
      "\tAvg Val Loss: opp-mlo=0.704, AUC: opp-mlo=0.500\n",
      "Iter=870, avg train loss=0.328, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.570 \n",
      "\tAvg Val Loss: same-mlo=0.752, AUC: same-mlo=0.547 \n",
      "\tAvg Val Loss opp-cc=0.717, AUC: opp-cc=0.538 \n",
      "\tAvg Val Loss: opp-mlo=0.707, AUC: opp-mlo=0.475\n",
      "Iter=875, avg train loss=0.382, \n",
      "\tAvg Val Loss: same-cc=0.727, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.762, AUC: same-mlo=0.552 \n",
      "\tAvg Val Loss opp-cc=0.728, AUC: opp-cc=0.551 \n",
      "\tAvg Val Loss: opp-mlo=0.726, AUC: opp-mlo=0.482\n",
      "Iter=880, avg train loss=0.474, \n",
      "\tAvg Val Loss: same-cc=0.709, AUC: same-cc=0.575 \n",
      "\tAvg Val Loss: same-mlo=0.737, AUC: same-mlo=0.555 \n",
      "\tAvg Val Loss opp-cc=0.672, AUC: opp-cc=0.553 \n",
      "\tAvg Val Loss: opp-mlo=0.727, AUC: opp-mlo=0.491\n",
      "Iter=885, avg train loss=0.331, \n",
      "\tAvg Val Loss: same-cc=0.757, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.756, AUC: same-mlo=0.551 \n",
      "\tAvg Val Loss opp-cc=0.692, AUC: opp-cc=0.561 \n",
      "\tAvg Val Loss: opp-mlo=0.784, AUC: opp-mlo=0.503\n",
      "Iter=890, avg train loss=0.370, \n",
      "\tAvg Val Loss: same-cc=0.732, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.683, AUC: same-mlo=0.557 \n",
      "\tAvg Val Loss opp-cc=0.679, AUC: opp-cc=0.547 \n",
      "\tAvg Val Loss: opp-mlo=0.719, AUC: opp-mlo=0.493\n",
      "Iter=895, avg train loss=0.389, \n",
      "\tAvg Val Loss: same-cc=0.748, AUC: same-cc=0.577 \n",
      "\tAvg Val Loss: same-mlo=0.681, AUC: same-mlo=0.547 \n",
      "\tAvg Val Loss opp-cc=0.676, AUC: opp-cc=0.553 \n",
      "\tAvg Val Loss: opp-mlo=0.703, AUC: opp-mlo=0.492\n",
      "Iter=900, avg train loss=0.400, \n",
      "\tAvg Val Loss: same-cc=0.727, AUC: same-cc=0.572 \n",
      "\tAvg Val Loss: same-mlo=0.676, AUC: same-mlo=0.543 \n",
      "\tAvg Val Loss opp-cc=0.700, AUC: opp-cc=0.529 \n",
      "\tAvg Val Loss: opp-mlo=0.685, AUC: opp-mlo=0.471\n",
      "Iter=905, avg train loss=0.436, \n",
      "\tAvg Val Loss: same-cc=0.728, AUC: same-cc=0.566 \n",
      "\tAvg Val Loss: same-mlo=0.685, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.675, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.692, AUC: opp-mlo=0.474\n",
      "Iter=910, avg train loss=0.297, \n",
      "\tAvg Val Loss: same-cc=0.734, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.687, AUC: same-mlo=0.570 \n",
      "\tAvg Val Loss opp-cc=0.700, AUC: opp-cc=0.560 \n",
      "\tAvg Val Loss: opp-mlo=0.756, AUC: opp-mlo=0.476\n",
      "Iter=915, avg train loss=0.363, \n",
      "\tAvg Val Loss: same-cc=0.651, AUC: same-cc=0.589 \n",
      "\tAvg Val Loss: same-mlo=0.671, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.671, AUC: opp-cc=0.580 \n",
      "\tAvg Val Loss: opp-mlo=0.740, AUC: opp-mlo=0.481\n",
      "Iter=920, avg train loss=0.356, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.582 \n",
      "\tAvg Val Loss: same-mlo=0.659, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.641, AUC: opp-cc=0.582 \n",
      "\tAvg Val Loss: opp-mlo=0.740, AUC: opp-mlo=0.476\n",
      "Iter=925, avg train loss=0.358, \n",
      "\tAvg Val Loss: same-cc=0.712, AUC: same-cc=0.581 \n",
      "\tAvg Val Loss: same-mlo=0.668, AUC: same-mlo=0.573 \n",
      "\tAvg Val Loss opp-cc=0.652, AUC: opp-cc=0.568 \n",
      "\tAvg Val Loss: opp-mlo=0.734, AUC: opp-mlo=0.466\n",
      "Iter=930, avg train loss=0.303, \n",
      "\tAvg Val Loss: same-cc=0.671, AUC: same-cc=0.561 \n",
      "\tAvg Val Loss: same-mlo=0.661, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.648, AUC: opp-cc=0.555 \n",
      "\tAvg Val Loss: opp-mlo=0.727, AUC: opp-mlo=0.461\n",
      "Iter=935, avg train loss=0.318, \n",
      "\tAvg Val Loss: same-cc=0.645, AUC: same-cc=0.562 \n",
      "\tAvg Val Loss: same-mlo=0.687, AUC: same-mlo=0.582 \n",
      "\tAvg Val Loss opp-cc=0.685, AUC: opp-cc=0.535 \n",
      "\tAvg Val Loss: opp-mlo=0.725, AUC: opp-mlo=0.455\n",
      "Iter=940, avg train loss=0.410, \n",
      "\tAvg Val Loss: same-cc=0.707, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.708, AUC: same-mlo=0.574 \n",
      "\tAvg Val Loss opp-cc=0.742, AUC: opp-cc=0.566 \n",
      "\tAvg Val Loss: opp-mlo=0.746, AUC: opp-mlo=0.470\n",
      "Iter=945, avg train loss=0.454, \n",
      "\tAvg Val Loss: same-cc=0.798, AUC: same-cc=0.575 \n",
      "\tAvg Val Loss: same-mlo=0.779, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.744, AUC: opp-cc=0.574 \n",
      "\tAvg Val Loss: opp-mlo=0.766, AUC: opp-mlo=0.488\n",
      "Iter=950, avg train loss=0.360, \n",
      "\tAvg Val Loss: same-cc=0.826, AUC: same-cc=0.596 \n",
      "\tAvg Val Loss: same-mlo=0.794, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.671, AUC: opp-cc=0.601 \n",
      "\tAvg Val Loss: opp-mlo=0.728, AUC: opp-mlo=0.500\n",
      "Iter=955, avg train loss=0.287, \n",
      "\tAvg Val Loss: same-cc=0.773, AUC: same-cc=0.574 \n",
      "\tAvg Val Loss: same-mlo=0.682, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.609 \n",
      "\tAvg Val Loss: opp-mlo=0.709, AUC: opp-mlo=0.501\n",
      "Iter=960, avg train loss=0.445, \n",
      "\tAvg Val Loss: same-cc=0.813, AUC: same-cc=0.551 \n",
      "\tAvg Val Loss: same-mlo=0.714, AUC: same-mlo=0.568 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.596 \n",
      "\tAvg Val Loss: opp-mlo=0.722, AUC: opp-mlo=0.500\n",
      "Iter=965, avg train loss=0.330, \n",
      "\tAvg Val Loss: same-cc=0.807, AUC: same-cc=0.550 \n",
      "\tAvg Val Loss: same-mlo=0.827, AUC: same-mlo=0.570 \n",
      "\tAvg Val Loss opp-cc=0.665, AUC: opp-cc=0.595 \n",
      "\tAvg Val Loss: opp-mlo=0.761, AUC: opp-mlo=0.496\n",
      "Iter=970, avg train loss=0.396, \n",
      "\tAvg Val Loss: same-cc=0.774, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.796, AUC: same-mlo=0.559 \n",
      "\tAvg Val Loss opp-cc=0.639, AUC: opp-cc=0.605 \n",
      "\tAvg Val Loss: opp-mlo=0.730, AUC: opp-mlo=0.521\n",
      "Iter=975, avg train loss=0.330, \n",
      "\tAvg Val Loss: same-cc=0.746, AUC: same-cc=0.523 \n",
      "\tAvg Val Loss: same-mlo=0.678, AUC: same-mlo=0.557 \n",
      "\tAvg Val Loss opp-cc=0.670, AUC: opp-cc=0.591 \n",
      "\tAvg Val Loss: opp-mlo=0.676, AUC: opp-mlo=0.540\n",
      "Iter=980, avg train loss=0.423, \n",
      "\tAvg Val Loss: same-cc=0.719, AUC: same-cc=0.525 \n",
      "\tAvg Val Loss: same-mlo=0.703, AUC: same-mlo=0.577 \n",
      "\tAvg Val Loss opp-cc=0.690, AUC: opp-cc=0.590 \n",
      "\tAvg Val Loss: opp-mlo=0.660, AUC: opp-mlo=0.540\n",
      "Iter=985, avg train loss=0.389, \n",
      "\tAvg Val Loss: same-cc=0.668, AUC: same-cc=0.555 \n",
      "\tAvg Val Loss: same-mlo=0.669, AUC: same-mlo=0.572 \n",
      "\tAvg Val Loss opp-cc=0.647, AUC: opp-cc=0.595 \n",
      "\tAvg Val Loss: opp-mlo=0.640, AUC: opp-mlo=0.517\n",
      "Iter=990, avg train loss=0.328, \n",
      "\tAvg Val Loss: same-cc=0.755, AUC: same-cc=0.580 \n",
      "\tAvg Val Loss: same-mlo=0.697, AUC: same-mlo=0.560 \n",
      "\tAvg Val Loss opp-cc=0.710, AUC: opp-cc=0.603 \n",
      "\tAvg Val Loss: opp-mlo=0.684, AUC: opp-mlo=0.525\n",
      "Iter=995, avg train loss=0.335, \n",
      "\tAvg Val Loss: same-cc=0.709, AUC: same-cc=0.578 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.557 \n",
      "\tAvg Val Loss opp-cc=0.643, AUC: opp-cc=0.588 \n",
      "\tAvg Val Loss: opp-mlo=0.736, AUC: opp-mlo=0.529\n",
      "Iter=1000, avg train loss=0.290, \n",
      "\tAvg Val Loss: same-cc=0.696, AUC: same-cc=0.558 \n",
      "\tAvg Val Loss: same-mlo=0.672, AUC: same-mlo=0.558 \n",
      "\tAvg Val Loss opp-cc=0.663, AUC: opp-cc=0.586 \n",
      "\tAvg Val Loss: opp-mlo=0.692, AUC: opp-mlo=0.545\n",
      "Iter=1005, avg train loss=0.219, \n",
      "\tAvg Val Loss: same-cc=0.721, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.680, AUC: same-mlo=0.561 \n",
      "\tAvg Val Loss opp-cc=0.637, AUC: opp-cc=0.600 \n",
      "\tAvg Val Loss: opp-mlo=0.665, AUC: opp-mlo=0.551\n",
      "Iter=1010, avg train loss=0.280, \n",
      "\tAvg Val Loss: same-cc=0.723, AUC: same-cc=0.552 \n",
      "\tAvg Val Loss: same-mlo=0.664, AUC: same-mlo=0.557 \n",
      "\tAvg Val Loss opp-cc=0.642, AUC: opp-cc=0.602 \n",
      "\tAvg Val Loss: opp-mlo=0.682, AUC: opp-mlo=0.539\n",
      "Iter=1015, avg train loss=0.326, \n",
      "\tAvg Val Loss: same-cc=0.756, AUC: same-cc=0.539 \n",
      "\tAvg Val Loss: same-mlo=0.684, AUC: same-mlo=0.559 \n",
      "\tAvg Val Loss opp-cc=0.696, AUC: opp-cc=0.595 \n",
      "\tAvg Val Loss: opp-mlo=0.666, AUC: opp-mlo=0.541\n",
      "Iter=1020, avg train loss=0.285, \n",
      "\tAvg Val Loss: same-cc=0.899, AUC: same-cc=0.545 \n",
      "\tAvg Val Loss: same-mlo=0.912, AUC: same-mlo=0.569 \n",
      "\tAvg Val Loss opp-cc=0.771, AUC: opp-cc=0.597 \n",
      "\tAvg Val Loss: opp-mlo=0.729, AUC: opp-mlo=0.527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1025, avg train loss=0.251, \n",
      "\tAvg Val Loss: same-cc=0.752, AUC: same-cc=0.542 \n",
      "\tAvg Val Loss: same-mlo=0.763, AUC: same-mlo=0.580 \n",
      "\tAvg Val Loss opp-cc=0.609, AUC: opp-cc=0.618 \n",
      "\tAvg Val Loss: opp-mlo=0.743, AUC: opp-mlo=0.536\n",
      "Iter=1030, avg train loss=0.326, \n",
      "\tAvg Val Loss: same-cc=0.717, AUC: same-cc=0.551 \n",
      "\tAvg Val Loss: same-mlo=0.693, AUC: same-mlo=0.594 \n",
      "\tAvg Val Loss opp-cc=0.621, AUC: opp-cc=0.609 \n",
      "\tAvg Val Loss: opp-mlo=0.766, AUC: opp-mlo=0.535\n",
      "Iter=1035, avg train loss=0.403, \n",
      "\tAvg Val Loss: same-cc=0.783, AUC: same-cc=0.536 \n",
      "\tAvg Val Loss: same-mlo=0.652, AUC: same-mlo=0.595 \n",
      "\tAvg Val Loss opp-cc=0.677, AUC: opp-cc=0.587 \n",
      "\tAvg Val Loss: opp-mlo=0.756, AUC: opp-mlo=0.515\n",
      "Iter=1040, avg train loss=0.280, \n",
      "\tAvg Val Loss: same-cc=0.708, AUC: same-cc=0.565 \n",
      "\tAvg Val Loss: same-mlo=0.632, AUC: same-mlo=0.604 \n",
      "\tAvg Val Loss opp-cc=0.688, AUC: opp-cc=0.572 \n",
      "\tAvg Val Loss: opp-mlo=0.699, AUC: opp-mlo=0.510\n",
      "Best models loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training: same-cc=0.671, same-mlo=0.625, opp-cc=0.538, opp-mlo=0.665\n",
      "Max-Score Based AUC After Training: 0.656, Mean-Score Based AUC After Training: 0.634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "cpu_threads = 4\n",
    "batch_size = 4\n",
    "n_folds = 5\n",
    "epochs = 20\n",
    "subject_pool, exam_pool = [], []\n",
    "pred_pool, label_pool, machine_pool = [], [], []\n",
    "age_pool, race_pool, bmi_pool = [], [], []\n",
    "birads_pool, libra_pool = [], []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=12345)\n",
    "fold = 0\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t1p), 1)), ys_t1p):\n",
    "    fold += 1\n",
    "    # train-val-test idx.\n",
    "    train_y = ys_t1p[train_ix_]\n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(\n",
    "        train_ix_, test_size=test_prop, \n",
    "        stratify=train_y, random_state=12345)\n",
    "    # subset.\n",
    "    train_dataset = Subset(risk_dataset_t1p, train_ix)\n",
    "    val_dataset = Subset(risk_dataset_t1p, val_ix)\n",
    "    test_dataset = Subset(risk_dataset_t1p, test_ix)\n",
    "    train_y = ys_t1p[train_ix]\n",
    "    val_y = ys_t1p[val_ix]\n",
    "    test_y = ys_t1p[test_ix]\n",
    "    # weighted sampler.\n",
    "    f0, f1 = np.bincount(train_y)\n",
    "    train_w = np.zeros_like(train_y, dtype='float')\n",
    "    train_w[train_y==0] = 1/f0\n",
    "    train_w[train_y==1] = 1/f1\n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        train_w, len(train_y)//batch_size*batch_size, \n",
    "        replacement=True)\n",
    "    \n",
    "    # data loaders.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "        collate_fn=mammo_collate)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    \n",
    "    models = dict.fromkeys(('same-cc', 'same-mlo', 'opp-cc', 'opp-mlo'))\n",
    "    \n",
    "    for key in models.keys(): \n",
    "        # Reset model before training.\n",
    "        model, device_ = load_model(image_only_parameters)\n",
    "        model = nn.DataParallel(model)\n",
    "        model = model.to(device)\n",
    "        models[key] = model\n",
    "        \n",
    "    # train & test.\n",
    "    best_name_ = 'best_model_{}_placeholder.pt'.format(fold)\n",
    "    print('='*10, 'Fold', fold, '='*10)\n",
    "    \n",
    "    fold_auc_max, fold_auc_mean = test_auc_all_views(models, test_loader, device)\n",
    "    \n",
    "    _, start_auc_same_cc = val_loss_single_view(models['same-cc'], test_loader, 'same-cc', device, return_auc=True)\n",
    "    _, start_auc_same_mlo = val_loss_single_view(models['same-mlo'], test_loader, 'same-mlo', device, return_auc=True)\n",
    "    _, start_auc_opp_cc = val_loss_single_view(models['opp-cc'], test_loader, 'opposite-cc', device, return_auc=True)\n",
    "    _, start_auc_opp_mlo = val_loss_single_view(models['opp-mlo'], test_loader, 'opposite-mlo', device, return_auc=True)\n",
    "    \n",
    "    #start_auc_m = test_max_auc(model, test_loader, device)\n",
    "    \n",
    "    print('Test AUC at start: same-cc={:.3f}, same-mlo={:.3f}, opp-cc={:.3f}, opp-mlo={:.3f}'.format(\n",
    "        start_auc_same_cc, start_auc_same_mlo, start_auc_opp_cc, start_auc_opp_mlo))\n",
    "    print('Max-Score Based AUC Before Training: {:.3f}, Mean-Score Based AUC Before Training: {:.3f}'.format(fold_auc_max, fold_auc_mean))\n",
    "    \n",
    "    train_all_views(models, train_loader, val_loader, fold, device, \n",
    "          epochs=epochs, lr=1e-5, check_iters=5, log_name='finetune_multiview_t1p.txt')\n",
    "    \n",
    "    print('Predicting on the test set...', end='')\n",
    "    subject_list, exam_list, \\\n",
    "    pred_list, label_list, machine_list, \\\n",
    "    age_list, race_list, bmi_list, \\\n",
    "    birads_list, libra_list = test_all_views(models, test_loader, device)\n",
    "    \n",
    "    subject_pool.extend(subject_list)\n",
    "    exam_pool.extend(exam_list)\n",
    "    pred_pool.extend(pred_list)\n",
    "    label_pool.extend(label_list)\n",
    "    machine_pool.extend(machine_list)\n",
    "    age_pool.extend(age_list)\n",
    "    race_pool.extend(race_list)\n",
    "    bmi_pool.extend(bmi_list)\n",
    "    birads_pool.extend(birads_list)\n",
    "    libra_pool.extend(libra_list) \n",
    "    \n",
    "    # test AUC.\n",
    "    _, end_auc_same_cc = val_loss_single_view(models['same-cc'], test_loader, 'same-cc', device, return_auc=True)\n",
    "    _, end_auc_same_mlo = val_loss_single_view(models['same-mlo'], test_loader, 'same-mlo', device, return_auc=True)\n",
    "    _, end_auc_opp_cc = val_loss_single_view(models['opp-cc'], test_loader, 'opposite-cc', device, return_auc=True)\n",
    "    _, end_auc_opp_mlo = val_loss_single_view(models['opp-mlo'], test_loader, 'opposite-mlo', device, return_auc=True)\n",
    "    \n",
    "    \n",
    "    fold_auc_max, fold_auc_mean = test_auc_all_views(models, test_loader, device)\n",
    "    \n",
    "    print('Done')\n",
    "    print('Test AUC after training: same-cc={:.3f}, same-mlo={:.3f}, opp-cc={:.3f}, opp-mlo={:.3f}'.format(\n",
    "        end_auc_same_cc, end_auc_same_mlo, end_auc_opp_cc, end_auc_opp_mlo))\n",
    "    print('Max-Score Based AUC After Training: {:.3f}, Mean-Score Based AUC After Training: {:.3f}'.format(fold_auc_max, fold_auc_mean))\n",
    "    print()\n",
    "    \n",
    "    if fold < n_folds:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1p_mv = np.concatenate(subject_pool)\n",
    "all_exam_t1p_mv = np.concatenate(exam_pool)\n",
    "all_preds_t1p_mv = torch.cat(pred_pool)\n",
    "all_labels_t1p_mv = torch.cat(label_pool)\n",
    "all_preds_t1p_mv = all_preds_t1p_mv.cpu().numpy()\n",
    "all_labels_t1p_mv = all_labels_t1p_mv.numpy()\n",
    "all_probs_max_t1p_mv = all_preds_t1p_mv.max(1)\n",
    "all_probs_mean_t1p_mv = all_preds_t1p_mv.mean(1)\n",
    "\n",
    "all_machines_t1p_mv = np.concatenate(machine_pool)\n",
    "all_ages_t1p_mv = np.concatenate(age_pool)\n",
    "all_races_t1p_mv = np.concatenate(race_pool)\n",
    "all_bmis_t1p_mv = np.concatenate(bmi_pool)\n",
    "all_birads_t1p_mv = np.concatenate(birads_pool)\n",
    "all_libras_t1p_mv = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.607\n",
      "1 0.587\n",
      "2 0.523\n",
      "3 0.573\n",
      "0.5724969869146006\n"
     ]
    }
   ],
   "source": [
    "total_auroc = 0\n",
    "for i in range(4):\n",
    "    roc = roc_auc_score(all_labels_t1p_mv, all_preds_t1p_mv[:, i])\n",
    "    print(\"{} {:.3f}\".format(i, roc))\n",
    "    total_auroc += roc\n",
    "print(total_auroc / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=352\n",
      "4view max AUC=0.578\n",
      "4view mean AUC=0.597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1p_mv)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_mv, all_probs_max_t1p_mv)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t1p_mv, all_preds_t1p_mv.mean(1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "risk_dataset_t1 = torch.load('../time_set/risk_pred_GEHolo_4view_matchedSepCase_T1.pt')\n",
    "len(risk_dataset_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    <mit_risk_data.FaceRight object at 0x7f92e4ed1c50>\n",
       "    <mit_risk_data.NormalizePix object at 0x7f92e4ed1a90>\n",
       "    <mit_risk_data.ToTensor3D object at 0x7f92e4ed1cd0>\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_dataset_t1.transform = trans\n",
    "risk_dataset_t1.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from: ys_t1.pkl\n"
     ]
    }
   ],
   "source": [
    "#ys_t1 = np.array([ sample['label'].item() for sample in risk_dataset_t1])\n",
    "ys_t1 = load_pkl('ys_t1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Fold 1 ==========\n",
      "Test AUC at start=0.626, max-score-based AUC=0.664\n",
      "Iter=5, avg train loss=1.098, avg val loss=0.529, auc=0.654\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.622, avg val loss=0.522, auc=0.682\n",
      "Best model saved.\n",
      "Iter=15, avg train loss=0.908, avg val loss=0.520, auc=0.698\n",
      "Best model saved.\n",
      "Iter=20, avg train loss=0.808, avg val loss=0.525, auc=0.704\n",
      "Best model saved.\n",
      "Iter=25, avg train loss=0.870, avg val loss=0.522, auc=0.695\n",
      "Iter=30, avg train loss=0.809, avg val loss=0.541, auc=0.694\n",
      "Iter=35, avg train loss=0.621, avg val loss=0.533, auc=0.690\n",
      "Iter=40, avg train loss=0.687, avg val loss=0.567, auc=0.693\n",
      "Iter=45, avg train loss=0.636, avg val loss=0.559, auc=0.701\n",
      "Iter=50, avg train loss=0.578, avg val loss=0.548, auc=0.704\n",
      "Iter=55, avg train loss=0.735, avg val loss=0.542, auc=0.696\n",
      "Iter=60, avg train loss=0.693, avg val loss=0.549, auc=0.697\n",
      "Iter=65, avg train loss=0.648, avg val loss=0.549, auc=0.697\n",
      "Iter=70, avg train loss=0.777, avg val loss=0.554, auc=0.691\n",
      "Iter=75, avg train loss=0.558, avg val loss=0.547, auc=0.692\n",
      "Iter=80, avg train loss=0.636, avg val loss=0.551, auc=0.692\n",
      "Iter=85, avg train loss=0.654, avg val loss=0.556, auc=0.692\n",
      "Iter=90, avg train loss=0.659, avg val loss=0.551, auc=0.691\n",
      "Iter=95, avg train loss=0.642, avg val loss=0.547, auc=0.702\n",
      "Iter=100, avg train loss=0.749, avg val loss=0.550, auc=0.685\n",
      "Iter=105, avg train loss=0.688, avg val loss=0.546, auc=0.683\n",
      "Iter=110, avg train loss=0.703, avg val loss=0.559, auc=0.682\n",
      "Iter=115, avg train loss=0.652, avg val loss=0.581, auc=0.676\n",
      "Iter=120, avg train loss=0.633, avg val loss=0.566, auc=0.670\n",
      "Iter=125, avg train loss=0.740, avg val loss=0.586, auc=0.675\n",
      "Iter=130, avg train loss=0.615, avg val loss=0.598, auc=0.671\n",
      "Iter=135, avg train loss=0.663, avg val loss=0.603, auc=0.664\n",
      "Iter=140, avg train loss=0.643, avg val loss=0.604, auc=0.667\n",
      "Iter=145, avg train loss=0.648, avg val loss=0.590, auc=0.677\n",
      "Iter=150, avg train loss=0.672, avg val loss=0.591, auc=0.684\n",
      "Iter=155, avg train loss=0.633, avg val loss=0.575, auc=0.697\n",
      "Iter=160, avg train loss=0.713, avg val loss=0.546, auc=0.690\n",
      "Iter=165, avg train loss=0.644, avg val loss=0.592, auc=0.693\n",
      "Iter=170, avg train loss=0.632, avg val loss=0.590, auc=0.694\n",
      "Iter=175, avg train loss=0.682, avg val loss=0.564, auc=0.685\n",
      "Iter=180, avg train loss=0.629, avg val loss=0.561, auc=0.686\n",
      "Iter=185, avg train loss=0.676, avg val loss=0.566, auc=0.691\n",
      "Iter=190, avg train loss=0.669, avg val loss=0.576, auc=0.690\n",
      "Iter=195, avg train loss=0.641, avg val loss=0.595, auc=0.684\n",
      "Iter=200, avg train loss=0.589, avg val loss=0.602, auc=0.683\n",
      "Iter=205, avg train loss=0.609, avg val loss=0.593, auc=0.688\n",
      "Iter=210, avg train loss=0.676, avg val loss=0.605, auc=0.685\n",
      "Iter=215, avg train loss=0.620, avg val loss=0.593, auc=0.687\n",
      "Iter=220, avg train loss=0.644, avg val loss=0.605, auc=0.677\n",
      "Iter=225, avg train loss=0.703, avg val loss=0.579, auc=0.680\n",
      "Iter=230, avg train loss=0.603, avg val loss=0.574, auc=0.667\n",
      "Iter=235, avg train loss=0.715, avg val loss=0.589, auc=0.678\n",
      "Iter=240, avg train loss=0.606, avg val loss=0.594, auc=0.672\n",
      "Iter=245, avg train loss=0.596, avg val loss=0.588, auc=0.670\n",
      "Iter=250, avg train loss=0.668, avg val loss=0.615, auc=0.668\n",
      "Iter=255, avg train loss=0.627, avg val loss=0.633, auc=0.659\n",
      "Iter=260, avg train loss=0.590, avg val loss=0.583, auc=0.671\n",
      "Iter=265, avg train loss=0.683, avg val loss=0.577, auc=0.676\n",
      "Iter=270, avg train loss=0.663, avg val loss=0.575, auc=0.671\n",
      "Iter=275, avg train loss=0.552, avg val loss=0.584, auc=0.664\n",
      "Iter=280, avg train loss=0.622, avg val loss=0.584, auc=0.663\n",
      "Iter=285, avg train loss=0.648, avg val loss=0.600, auc=0.671\n",
      "Iter=290, avg train loss=0.681, avg val loss=0.557, auc=0.657\n",
      "Iter=295, avg train loss=0.577, avg val loss=0.553, auc=0.654\n",
      "Iter=300, avg train loss=0.605, avg val loss=0.552, auc=0.645\n",
      "Iter=305, avg train loss=0.751, avg val loss=0.574, auc=0.651\n",
      "Iter=310, avg train loss=0.621, avg val loss=0.581, auc=0.661\n",
      "Iter=315, avg train loss=0.740, avg val loss=0.578, auc=0.649\n",
      "Iter=320, avg train loss=0.574, avg val loss=0.571, auc=0.649\n",
      "Iter=325, avg train loss=0.641, avg val loss=0.575, auc=0.657\n",
      "Iter=330, avg train loss=0.581, avg val loss=0.607, auc=0.660\n",
      "Iter=335, avg train loss=0.565, avg val loss=0.573, auc=0.662\n",
      "Iter=340, avg train loss=0.685, avg val loss=0.577, auc=0.661\n",
      "Iter=345, avg train loss=0.699, avg val loss=0.618, auc=0.664\n",
      "Iter=350, avg train loss=0.618, avg val loss=0.664, auc=0.669\n",
      "Iter=355, avg train loss=0.685, avg val loss=0.649, auc=0.668\n",
      "Iter=360, avg train loss=0.669, avg val loss=0.638, auc=0.665\n",
      "Iter=365, avg train loss=0.636, avg val loss=0.609, auc=0.672\n",
      "Iter=370, avg train loss=0.668, avg val loss=0.626, auc=0.673\n",
      "Iter=375, avg train loss=0.612, avg val loss=0.618, auc=0.673\n",
      "Iter=380, avg train loss=0.634, avg val loss=0.643, auc=0.673\n",
      "Iter=385, avg train loss=0.607, avg val loss=0.601, auc=0.672\n",
      "Iter=390, avg train loss=0.597, avg val loss=0.588, auc=0.675\n",
      "Iter=395, avg train loss=0.678, avg val loss=0.575, auc=0.680\n",
      "Iter=400, avg train loss=0.608, avg val loss=0.582, auc=0.685\n",
      "Iter=405, avg train loss=0.632, avg val loss=0.567, auc=0.680\n",
      "Iter=410, avg train loss=0.578, avg val loss=0.565, auc=0.680\n",
      "Iter=415, avg train loss=0.681, avg val loss=0.567, auc=0.678\n",
      "Iter=420, avg train loss=0.528, avg val loss=0.546, auc=0.683\n",
      "Iter=425, avg train loss=0.665, avg val loss=0.556, auc=0.689\n",
      "Iter=430, avg train loss=0.604, avg val loss=0.554, auc=0.684\n",
      "Iter=435, avg train loss=0.691, avg val loss=0.568, auc=0.682\n",
      "Iter=440, avg train loss=0.586, avg val loss=0.594, auc=0.677\n",
      "Iter=445, avg train loss=0.629, avg val loss=0.636, auc=0.679\n",
      "Iter=450, avg train loss=0.557, avg val loss=0.698, auc=0.674\n",
      "Iter=455, avg train loss=0.624, avg val loss=0.649, auc=0.662\n",
      "Iter=460, avg train loss=0.648, avg val loss=0.589, auc=0.647\n",
      "Iter=465, avg train loss=0.655, avg val loss=0.573, auc=0.658\n",
      "Iter=470, avg train loss=0.634, avg val loss=0.590, auc=0.667\n",
      "Iter=475, avg train loss=0.589, avg val loss=0.580, auc=0.661\n",
      "Iter=480, avg train loss=0.676, avg val loss=0.588, auc=0.670\n",
      "Iter=485, avg train loss=0.616, avg val loss=0.598, auc=0.673\n",
      "Iter=490, avg train loss=0.614, avg val loss=0.586, auc=0.681\n",
      "Iter=495, avg train loss=0.614, avg val loss=0.569, auc=0.679\n",
      "Iter=500, avg train loss=0.565, avg val loss=0.572, auc=0.675\n",
      "Iter=505, avg train loss=0.544, avg val loss=0.569, auc=0.681\n",
      "Iter=510, avg train loss=0.589, avg val loss=0.575, auc=0.682\n",
      "Iter=515, avg train loss=0.566, avg val loss=0.565, auc=0.682\n",
      "Iter=520, avg train loss=0.554, avg val loss=0.548, auc=0.685\n",
      "Iter=525, avg train loss=0.635, avg val loss=0.554, auc=0.686\n",
      "Iter=530, avg train loss=0.571, avg val loss=0.553, auc=0.691\n",
      "Iter=535, avg train loss=0.576, avg val loss=0.553, auc=0.689\n",
      "Iter=540, avg train loss=0.727, avg val loss=0.580, auc=0.684\n",
      "Iter=545, avg train loss=0.556, avg val loss=0.596, auc=0.681\n",
      "Iter=550, avg train loss=0.554, avg val loss=0.610, auc=0.670\n",
      "Iter=555, avg train loss=0.702, avg val loss=0.596, auc=0.670\n",
      "Iter=560, avg train loss=0.597, avg val loss=0.584, auc=0.674\n",
      "Iter=565, avg train loss=0.657, avg val loss=0.575, auc=0.668\n",
      "Iter=570, avg train loss=0.620, avg val loss=0.578, auc=0.676\n",
      "Iter=575, avg train loss=0.591, avg val loss=0.586, auc=0.676\n",
      "Iter=580, avg train loss=0.658, avg val loss=0.564, auc=0.666\n",
      "Iter=585, avg train loss=0.541, avg val loss=0.559, auc=0.665\n",
      "Iter=590, avg train loss=0.535, avg val loss=0.541, auc=0.659\n",
      "Iter=595, avg train loss=0.613, avg val loss=0.538, auc=0.655\n",
      "Iter=600, avg train loss=0.518, avg val loss=0.540, auc=0.668\n",
      "Iter=605, avg train loss=0.598, avg val loss=0.561, auc=0.669\n",
      "Iter=610, avg train loss=0.435, avg val loss=0.573, auc=0.664\n",
      "Iter=615, avg train loss=0.646, avg val loss=0.589, auc=0.662\n",
      "Iter=620, avg train loss=0.580, avg val loss=0.598, auc=0.660\n",
      "Iter=625, avg train loss=0.680, avg val loss=0.626, auc=0.661\n",
      "Iter=630, avg train loss=0.600, avg val loss=0.609, auc=0.660\n",
      "Iter=635, avg train loss=0.615, avg val loss=0.600, auc=0.659\n",
      "Iter=640, avg train loss=0.595, avg val loss=0.604, auc=0.667\n",
      "Iter=645, avg train loss=0.669, avg val loss=0.636, auc=0.678\n",
      "Iter=650, avg train loss=0.535, avg val loss=0.649, auc=0.663\n",
      "Iter=655, avg train loss=0.590, avg val loss=0.649, auc=0.669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=660, avg train loss=0.614, avg val loss=0.659, auc=0.665\n",
      "Iter=665, avg train loss=0.665, avg val loss=0.645, auc=0.672\n",
      "Iter=670, avg train loss=0.674, avg val loss=0.585, auc=0.671\n",
      "Iter=675, avg train loss=0.531, avg val loss=0.568, auc=0.662\n",
      "Iter=680, avg train loss=0.589, avg val loss=0.558, auc=0.677\n",
      "Iter=685, avg train loss=0.583, avg val loss=0.559, auc=0.665\n",
      "Iter=690, avg train loss=0.670, avg val loss=0.566, auc=0.660\n",
      "Iter=695, avg train loss=0.569, avg val loss=0.594, auc=0.682\n",
      "Iter=700, avg train loss=0.586, avg val loss=0.623, auc=0.690\n",
      "Iter=705, avg train loss=0.525, avg val loss=0.606, auc=0.684\n",
      "Iter=710, avg train loss=0.581, avg val loss=0.599, auc=0.689\n",
      "Iter=715, avg train loss=0.525, avg val loss=0.584, auc=0.685\n",
      "Iter=720, avg train loss=0.651, avg val loss=0.559, auc=0.687\n",
      "Iter=725, avg train loss=0.618, avg val loss=0.562, auc=0.681\n",
      "Iter=730, avg train loss=0.629, avg val loss=0.582, auc=0.673\n",
      "Iter=735, avg train loss=0.585, avg val loss=0.609, auc=0.683\n",
      "Iter=740, avg train loss=0.617, avg val loss=0.633, auc=0.687\n",
      "Iter=745, avg train loss=0.611, avg val loss=0.630, auc=0.691\n",
      "Iter=750, avg train loss=0.612, avg val loss=0.638, auc=0.701\n",
      "Iter=755, avg train loss=0.576, avg val loss=0.628, auc=0.693\n",
      "Iter=760, avg train loss=0.529, avg val loss=0.615, auc=0.692\n",
      "Iter=765, avg train loss=0.624, avg val loss=0.599, auc=0.688\n",
      "Iter=770, avg train loss=0.566, avg val loss=0.571, auc=0.680\n",
      "Iter=775, avg train loss=0.619, avg val loss=0.564, auc=0.695\n",
      "Iter=780, avg train loss=0.667, avg val loss=0.561, auc=0.693\n",
      "Iter=785, avg train loss=0.564, avg val loss=0.598, auc=0.696\n",
      "Iter=790, avg train loss=0.561, avg val loss=0.632, auc=0.701\n",
      "Iter=795, avg train loss=0.640, avg val loss=0.643, auc=0.709\n",
      "Best model saved.\n",
      "Iter=800, avg train loss=0.572, avg val loss=0.673, auc=0.702\n",
      "Iter=805, avg train loss=0.600, avg val loss=0.646, auc=0.689\n",
      "Iter=810, avg train loss=0.601, avg val loss=0.592, auc=0.685\n",
      "Iter=815, avg train loss=0.618, avg val loss=0.568, auc=0.678\n",
      "Iter=820, avg train loss=0.652, avg val loss=0.568, auc=0.686\n",
      "Iter=825, avg train loss=0.502, avg val loss=0.572, auc=0.679\n",
      "Iter=830, avg train loss=0.512, avg val loss=0.577, auc=0.664\n",
      "Iter=835, avg train loss=0.573, avg val loss=0.547, auc=0.677\n",
      "Iter=840, avg train loss=0.589, avg val loss=0.555, auc=0.678\n",
      "Iter=845, avg train loss=0.598, avg val loss=0.574, auc=0.688\n",
      "Iter=850, avg train loss=0.526, avg val loss=0.594, auc=0.685\n",
      "Iter=855, avg train loss=0.545, avg val loss=0.589, auc=0.685\n",
      "Iter=860, avg train loss=0.577, avg val loss=0.572, auc=0.682\n",
      "Iter=865, avg train loss=0.550, avg val loss=0.546, auc=0.680\n",
      "Iter=870, avg train loss=0.595, avg val loss=0.562, auc=0.676\n",
      "Iter=875, avg train loss=0.614, avg val loss=0.576, auc=0.673\n",
      "Iter=880, avg train loss=0.549, avg val loss=0.580, auc=0.674\n",
      "Iter=885, avg train loss=0.507, avg val loss=0.581, auc=0.675\n",
      "Iter=890, avg train loss=0.555, avg val loss=0.588, auc=0.669\n",
      "Iter=895, avg train loss=0.579, avg val loss=0.635, auc=0.665\n",
      "Iter=900, avg train loss=0.524, avg val loss=0.634, auc=0.658\n",
      "Iter=905, avg train loss=0.565, avg val loss=0.635, auc=0.666\n",
      "Iter=910, avg train loss=0.597, avg val loss=0.636, auc=0.667\n",
      "Iter=915, avg train loss=0.591, avg val loss=0.657, auc=0.674\n",
      "Iter=920, avg train loss=0.577, avg val loss=0.621, auc=0.673\n",
      "Iter=925, avg train loss=0.514, avg val loss=0.601, auc=0.676\n",
      "Iter=930, avg train loss=0.510, avg val loss=0.631, auc=0.676\n",
      "Iter=935, avg train loss=0.494, avg val loss=0.621, auc=0.633\n",
      "Iter=940, avg train loss=0.626, avg val loss=0.581, auc=0.670\n",
      "Iter=945, avg train loss=0.630, avg val loss=0.565, auc=0.674\n",
      "Iter=950, avg train loss=0.538, avg val loss=0.624, auc=0.682\n",
      "Iter=955, avg train loss=0.573, avg val loss=0.605, auc=0.671\n",
      "Iter=960, avg train loss=0.716, avg val loss=0.595, auc=0.670\n",
      "Iter=965, avg train loss=0.557, avg val loss=0.592, auc=0.661\n",
      "Iter=970, avg train loss=0.547, avg val loss=0.581, auc=0.665\n",
      "Iter=975, avg train loss=0.461, avg val loss=0.598, auc=0.664\n",
      "Iter=980, avg train loss=0.544, avg val loss=0.586, auc=0.667\n",
      "Iter=985, avg train loss=0.542, avg val loss=0.603, auc=0.671\n",
      "Iter=990, avg train loss=0.481, avg val loss=0.618, auc=0.671\n",
      "Iter=995, avg train loss=0.473, avg val loss=0.632, auc=0.666\n",
      "Iter=1000, avg train loss=0.442, avg val loss=0.643, auc=0.664\n",
      "Iter=1005, avg train loss=0.529, avg val loss=0.616, auc=0.659\n",
      "Iter=1010, avg train loss=0.519, avg val loss=0.590, auc=0.660\n",
      "Iter=1015, avg train loss=0.567, avg val loss=0.602, auc=0.659\n",
      "Iter=1020, avg train loss=0.569, avg val loss=0.586, auc=0.664\n",
      "Iter=1025, avg train loss=0.468, avg val loss=0.577, auc=0.672\n",
      "Iter=1030, avg train loss=0.498, avg val loss=0.558, auc=0.682\n",
      "Iter=1035, avg train loss=0.578, avg val loss=0.547, auc=0.682\n",
      "Iter=1040, avg train loss=0.518, avg val loss=0.547, auc=0.675\n",
      "Iter=1045, avg train loss=0.568, avg val loss=0.619, auc=0.689\n",
      "Iter=1050, avg train loss=0.429, avg val loss=0.688, auc=0.700\n",
      "Iter=1055, avg train loss=0.562, avg val loss=0.598, auc=0.656\n",
      "Iter=1060, avg train loss=0.585, avg val loss=0.584, auc=0.668\n",
      "Iter=1065, avg train loss=0.634, avg val loss=0.564, auc=0.650\n",
      "Iter=1070, avg train loss=0.545, avg val loss=0.566, auc=0.665\n",
      "Iter=1075, avg train loss=0.473, avg val loss=0.557, auc=0.662\n",
      "Iter=1080, avg train loss=0.541, avg val loss=0.586, auc=0.658\n",
      "Iter=1085, avg train loss=0.581, avg val loss=0.636, auc=0.661\n",
      "Iter=1090, avg train loss=0.475, avg val loss=0.612, auc=0.660\n",
      "Iter=1095, avg train loss=0.460, avg val loss=0.591, auc=0.663\n",
      "Iter=1100, avg train loss=0.520, avg val loss=0.592, auc=0.647\n",
      "Iter=1105, avg train loss=0.486, avg val loss=0.628, auc=0.652\n",
      "Iter=1110, avg train loss=0.455, avg val loss=0.604, auc=0.671\n",
      "Iter=1115, avg train loss=0.535, avg val loss=0.632, auc=0.686\n",
      "Iter=1120, avg train loss=0.409, avg val loss=0.603, auc=0.686\n",
      "Iter=1125, avg train loss=0.514, avg val loss=0.567, auc=0.689\n",
      "Iter=1130, avg train loss=0.556, avg val loss=0.559, auc=0.664\n",
      "Iter=1135, avg train loss=0.468, avg val loss=0.581, auc=0.687\n",
      "Iter=1140, avg train loss=0.493, avg val loss=0.586, auc=0.695\n",
      "Iter=1145, avg train loss=0.608, avg val loss=0.562, auc=0.687\n",
      "Iter=1150, avg train loss=0.441, avg val loss=0.546, auc=0.680\n",
      "Iter=1155, avg train loss=0.515, avg val loss=0.556, auc=0.685\n",
      "Iter=1160, avg train loss=0.401, avg val loss=0.582, auc=0.693\n",
      "Iter=1165, avg train loss=0.464, avg val loss=0.597, auc=0.706\n",
      "Iter=1170, avg train loss=0.495, avg val loss=0.585, auc=0.719\n",
      "Best model saved.\n",
      "Iter=1175, avg train loss=0.547, avg val loss=0.590, auc=0.705\n",
      "Iter=1180, avg train loss=0.488, avg val loss=0.586, auc=0.692\n",
      "Iter=1185, avg train loss=0.486, avg val loss=0.630, auc=0.698\n",
      "Iter=1190, avg train loss=0.571, avg val loss=0.641, auc=0.694\n",
      "Iter=1195, avg train loss=0.505, avg val loss=0.614, auc=0.688\n",
      "Iter=1200, avg train loss=0.412, avg val loss=0.600, auc=0.695\n",
      "Iter=1205, avg train loss=0.506, avg val loss=0.594, auc=0.695\n",
      "Iter=1210, avg train loss=0.543, avg val loss=0.585, auc=0.693\n",
      "Iter=1215, avg train loss=0.492, avg val loss=0.566, auc=0.691\n",
      "Iter=1220, avg train loss=0.498, avg val loss=0.536, auc=0.702\n",
      "Iter=1225, avg train loss=0.549, avg val loss=0.572, auc=0.708\n",
      "Iter=1230, avg train loss=0.581, avg val loss=0.622, auc=0.712\n",
      "Iter=1235, avg train loss=0.520, avg val loss=0.699, auc=0.714\n",
      "Iter=1240, avg train loss=0.445, avg val loss=0.681, auc=0.721\n",
      "Best model saved.\n",
      "Iter=1245, avg train loss=0.596, avg val loss=0.621, auc=0.723\n",
      "Best model saved.\n",
      "Iter=1250, avg train loss=0.475, avg val loss=0.577, auc=0.708\n",
      "Iter=1255, avg train loss=0.461, avg val loss=0.557, auc=0.706\n",
      "Iter=1260, avg train loss=0.486, avg val loss=0.568, auc=0.711\n",
      "Iter=1265, avg train loss=0.551, avg val loss=0.572, auc=0.708\n",
      "Iter=1270, avg train loss=0.425, avg val loss=0.598, auc=0.697\n",
      "Iter=1275, avg train loss=0.565, avg val loss=0.567, auc=0.691\n",
      "Iter=1280, avg train loss=0.457, avg val loss=0.617, auc=0.692\n",
      "Iter=1285, avg train loss=0.527, avg val loss=0.591, auc=0.689\n",
      "Iter=1290, avg train loss=0.448, avg val loss=0.582, auc=0.681\n",
      "Iter=1295, avg train loss=0.471, avg val loss=0.620, auc=0.687\n",
      "Iter=1300, avg train loss=0.571, avg val loss=0.586, auc=0.674\n",
      "Iter=1305, avg train loss=0.605, avg val loss=0.635, auc=0.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1310, avg train loss=0.376, avg val loss=0.609, auc=0.689\n",
      "Iter=1315, avg train loss=0.508, avg val loss=0.587, auc=0.692\n",
      "Iter=1320, avg train loss=0.533, avg val loss=0.611, auc=0.687\n",
      "Iter=1325, avg train loss=0.480, avg val loss=0.619, auc=0.688\n",
      "Iter=1330, avg train loss=0.594, avg val loss=0.590, auc=0.698\n",
      "Iter=1335, avg train loss=0.528, avg val loss=0.599, auc=0.705\n",
      "Iter=1340, avg train loss=0.482, avg val loss=0.547, auc=0.696\n",
      "Iter=1345, avg train loss=0.443, avg val loss=0.552, auc=0.682\n",
      "Iter=1350, avg train loss=0.576, avg val loss=0.663, auc=0.702\n",
      "Iter=1355, avg train loss=0.433, avg val loss=0.583, auc=0.694\n",
      "Iter=1360, avg train loss=0.470, avg val loss=0.562, auc=0.699\n",
      "Iter=1365, avg train loss=0.477, avg val loss=0.573, auc=0.703\n",
      "Iter=1370, avg train loss=0.421, avg val loss=0.564, auc=0.704\n",
      "Iter=1375, avg train loss=0.410, avg val loss=0.594, auc=0.704\n",
      "Iter=1380, avg train loss=0.564, avg val loss=0.575, auc=0.687\n",
      "Iter=1385, avg train loss=0.578, avg val loss=0.542, auc=0.670\n",
      "Iter=1390, avg train loss=0.405, avg val loss=0.541, auc=0.648\n",
      "Iter=1395, avg train loss=0.441, avg val loss=0.553, auc=0.665\n",
      "Iter=1400, avg train loss=0.513, avg val loss=0.578, auc=0.673\n",
      "Iter=1405, avg train loss=0.508, avg val loss=0.640, auc=0.681\n",
      "Iter=1410, avg train loss=0.396, avg val loss=0.627, auc=0.677\n",
      "Iter=1415, avg train loss=0.439, avg val loss=0.578, auc=0.663\n",
      "Iter=1420, avg train loss=0.482, avg val loss=0.578, auc=0.680\n",
      "Iter=1425, avg train loss=0.579, avg val loss=0.605, auc=0.675\n",
      "Iter=1430, avg train loss=0.563, avg val loss=0.576, auc=0.677\n",
      "Iter=1435, avg train loss=0.504, avg val loss=0.577, auc=0.682\n",
      "Iter=1440, avg train loss=0.493, avg val loss=0.595, auc=0.668\n",
      "Iter=1445, avg train loss=0.361, avg val loss=0.556, auc=0.664\n",
      "Iter=1450, avg train loss=0.450, avg val loss=0.575, auc=0.674\n",
      "Iter=1455, avg train loss=0.400, avg val loss=0.656, auc=0.699\n",
      "Iter=1460, avg train loss=0.388, avg val loss=0.618, auc=0.707\n",
      "Iter=1465, avg train loss=0.449, avg val loss=0.573, auc=0.700\n",
      "Iter=1470, avg train loss=0.494, avg val loss=0.567, auc=0.707\n",
      "Iter=1475, avg train loss=0.461, avg val loss=0.538, auc=0.702\n",
      "Iter=1480, avg train loss=0.459, avg val loss=0.542, auc=0.693\n",
      "Iter=1485, avg train loss=0.478, avg val loss=0.599, auc=0.702\n",
      "Iter=1490, avg train loss=0.432, avg val loss=0.593, auc=0.699\n",
      "Iter=1495, avg train loss=0.331, avg val loss=0.622, auc=0.684\n",
      "Iter=1500, avg train loss=0.486, avg val loss=0.660, auc=0.707\n",
      "Iter=1505, avg train loss=0.355, avg val loss=0.592, auc=0.712\n",
      "Iter=1510, avg train loss=0.537, avg val loss=0.526, auc=0.690\n",
      "Iter=1515, avg train loss=0.416, avg val loss=0.531, auc=0.665\n",
      "Iter=1520, avg train loss=0.484, avg val loss=0.532, auc=0.694\n",
      "Iter=1525, avg train loss=0.457, avg val loss=0.563, auc=0.712\n",
      "Iter=1530, avg train loss=0.512, avg val loss=0.799, auc=0.713\n",
      "Iter=1535, avg train loss=0.546, avg val loss=0.749, auc=0.710\n",
      "Iter=1540, avg train loss=0.522, avg val loss=0.630, auc=0.714\n",
      "Iter=1545, avg train loss=0.502, avg val loss=0.589, auc=0.721\n",
      "Iter=1550, avg train loss=0.383, avg val loss=0.615, auc=0.718\n",
      "Iter=1555, avg train loss=0.429, avg val loss=0.632, auc=0.717\n",
      "Iter=1560, avg train loss=0.516, avg val loss=0.594, auc=0.712\n",
      "Iter=1565, avg train loss=0.527, avg val loss=0.538, auc=0.685\n",
      "Iter=1570, avg train loss=0.400, avg val loss=0.548, auc=0.633\n",
      "Iter=1575, avg train loss=0.534, avg val loss=0.542, auc=0.666\n",
      "Iter=1580, avg train loss=0.394, avg val loss=0.610, auc=0.710\n",
      "Iter=1585, avg train loss=0.360, avg val loss=0.702, auc=0.709\n",
      "Iter=1590, avg train loss=0.305, avg val loss=0.633, auc=0.702\n",
      "Iter=1595, avg train loss=0.418, avg val loss=0.621, auc=0.691\n",
      "Iter=1600, avg train loss=0.450, avg val loss=0.553, auc=0.684\n",
      "Iter=1605, avg train loss=0.502, avg val loss=0.556, auc=0.670\n",
      "Iter=1610, avg train loss=0.397, avg val loss=0.547, auc=0.674\n",
      "Iter=1615, avg train loss=0.557, avg val loss=0.603, auc=0.692\n",
      "Iter=1620, avg train loss=0.358, avg val loss=0.634, auc=0.696\n",
      "Iter=1625, avg train loss=0.487, avg val loss=0.612, auc=0.683\n",
      "Iter=1630, avg train loss=0.474, avg val loss=0.620, auc=0.682\n",
      "Iter=1635, avg train loss=0.411, avg val loss=0.566, auc=0.679\n",
      "Iter=1640, avg train loss=0.551, avg val loss=0.556, auc=0.680\n",
      "Iter=1645, avg train loss=0.330, avg val loss=0.572, auc=0.686\n",
      "Iter=1650, avg train loss=0.368, avg val loss=0.578, auc=0.690\n",
      "Iter=1655, avg train loss=0.340, avg val loss=0.630, auc=0.694\n",
      "Iter=1660, avg train loss=0.379, avg val loss=0.556, auc=0.672\n",
      "Iter=1665, avg train loss=0.410, avg val loss=0.566, auc=0.683\n",
      "Iter=1670, avg train loss=0.499, avg val loss=0.605, auc=0.698\n",
      "Iter=1675, avg train loss=0.458, avg val loss=0.661, auc=0.699\n",
      "Iter=1680, avg train loss=0.386, avg val loss=0.572, auc=0.667\n",
      "Iter=1685, avg train loss=0.298, avg val loss=0.618, auc=0.672\n",
      "Iter=1690, avg train loss=0.319, avg val loss=0.648, auc=0.663\n",
      "Iter=1695, avg train loss=0.437, avg val loss=0.604, auc=0.673\n",
      "Iter=1700, avg train loss=0.410, avg val loss=0.595, auc=0.678\n",
      "Iter=1705, avg train loss=0.373, avg val loss=0.581, auc=0.674\n",
      "Iter=1710, avg train loss=0.512, avg val loss=0.575, auc=0.695\n",
      "Iter=1715, avg train loss=0.366, avg val loss=0.553, auc=0.688\n",
      "Iter=1720, avg train loss=0.365, avg val loss=0.538, auc=0.676\n",
      "Iter=1725, avg train loss=0.581, avg val loss=0.594, auc=0.703\n",
      "Iter=1730, avg train loss=0.568, avg val loss=0.733, auc=0.712\n",
      "Iter=1735, avg train loss=0.707, avg val loss=0.685, auc=0.709\n",
      "Iter=1740, avg train loss=0.396, avg val loss=0.598, auc=0.705\n",
      "Iter=1745, avg train loss=0.351, avg val loss=0.541, auc=0.678\n",
      "Iter=1750, avg train loss=0.421, avg val loss=0.540, auc=0.660\n",
      "Iter=1755, avg train loss=0.478, avg val loss=0.544, auc=0.659\n",
      "Iter=1760, avg train loss=0.434, avg val loss=0.560, auc=0.673\n",
      "Iter=1765, avg train loss=0.320, avg val loss=0.595, auc=0.681\n",
      "Iter=1770, avg train loss=0.341, avg val loss=0.724, auc=0.699\n",
      "Iter=1775, avg train loss=0.377, avg val loss=0.664, auc=0.694\n",
      "Iter=1780, avg train loss=0.462, avg val loss=0.560, auc=0.682\n",
      "Iter=1785, avg train loss=0.451, avg val loss=0.552, auc=0.672\n",
      "Iter=1790, avg train loss=0.445, avg val loss=0.560, auc=0.679\n",
      "Iter=1795, avg train loss=0.450, avg val loss=0.583, auc=0.671\n",
      "Iter=1800, avg train loss=0.551, avg val loss=0.596, auc=0.700\n",
      "Iter=1805, avg train loss=0.400, avg val loss=0.542, auc=0.662\n",
      "Iter=1810, avg train loss=0.360, avg val loss=0.551, auc=0.643\n",
      "Iter=1815, avg train loss=0.373, avg val loss=0.546, auc=0.660\n",
      "Iter=1820, avg train loss=0.422, avg val loss=0.538, auc=0.677\n",
      "Iter=1825, avg train loss=0.343, avg val loss=0.549, auc=0.688\n",
      "Iter=1830, avg train loss=0.376, avg val loss=0.575, auc=0.683\n",
      "Iter=1835, avg train loss=0.371, avg val loss=0.602, auc=0.675\n",
      "Iter=1840, avg train loss=0.362, avg val loss=0.543, auc=0.684\n",
      "Iter=1845, avg train loss=0.373, avg val loss=0.568, auc=0.685\n",
      "Iter=1850, avg train loss=0.393, avg val loss=0.575, auc=0.685\n",
      "Iter=1855, avg train loss=0.389, avg val loss=0.600, auc=0.687\n",
      "Iter=1860, avg train loss=0.419, avg val loss=0.598, auc=0.698\n",
      "Iter=1865, avg train loss=0.387, avg val loss=0.567, auc=0.697\n",
      "Iter=1870, avg train loss=0.487, avg val loss=0.569, auc=0.703\n",
      "Iter=1875, avg train loss=0.487, avg val loss=0.665, auc=0.693\n",
      "Iter=1880, avg train loss=0.532, avg val loss=0.571, auc=0.696\n",
      "Iter=1885, avg train loss=0.316, avg val loss=0.561, auc=0.683\n",
      "Iter=1890, avg train loss=0.398, avg val loss=0.542, auc=0.647\n",
      "Iter=1895, avg train loss=0.442, avg val loss=0.547, auc=0.653\n",
      "Iter=1900, avg train loss=0.430, avg val loss=0.580, auc=0.676\n",
      "Iter=1905, avg train loss=0.261, avg val loss=0.653, auc=0.679\n",
      "Iter=1910, avg train loss=0.421, avg val loss=0.609, auc=0.662\n",
      "Iter=1915, avg train loss=0.482, avg val loss=0.649, auc=0.696\n",
      "Iter=1920, avg train loss=0.313, avg val loss=0.672, auc=0.691\n",
      "Iter=1925, avg train loss=0.416, avg val loss=0.600, auc=0.700\n",
      "Iter=1930, avg train loss=0.359, avg val loss=0.560, auc=0.676\n",
      "Iter=1935, avg train loss=0.321, avg val loss=0.590, auc=0.686\n",
      "Iter=1940, avg train loss=0.354, avg val loss=0.647, auc=0.698\n",
      "Iter=1945, avg train loss=0.330, avg val loss=0.635, auc=0.684\n",
      "Iter=1950, avg train loss=0.459, avg val loss=0.595, auc=0.668\n",
      "Iter=1955, avg train loss=0.374, avg val loss=0.544, auc=0.643\n",
      "Iter=1960, avg train loss=0.408, avg val loss=0.552, auc=0.637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1965, avg train loss=0.249, avg val loss=0.558, auc=0.650\n",
      "Iter=1970, avg train loss=0.369, avg val loss=0.564, auc=0.667\n",
      "Iter=1975, avg train loss=0.435, avg val loss=0.658, auc=0.702\n",
      "Iter=1980, avg train loss=0.433, avg val loss=0.619, auc=0.694\n",
      "Iter=1985, avg train loss=0.461, avg val loss=0.544, auc=0.676\n",
      "Iter=1990, avg train loss=0.419, avg val loss=0.545, auc=0.672\n",
      "Iter=1995, avg train loss=0.293, avg val loss=0.572, auc=0.693\n",
      "Iter=2000, avg train loss=0.284, avg val loss=0.595, auc=0.699\n",
      "Iter=2005, avg train loss=0.457, avg val loss=0.639, auc=0.699\n",
      "Iter=2010, avg train loss=0.319, avg val loss=0.566, auc=0.687\n",
      "Iter=2015, avg train loss=0.274, avg val loss=0.597, auc=0.683\n",
      "Iter=2020, avg train loss=0.337, avg val loss=0.624, auc=0.679\n",
      "Iter=2025, avg train loss=0.393, avg val loss=0.615, auc=0.691\n",
      "Iter=2030, avg train loss=0.301, avg val loss=0.600, auc=0.680\n",
      "Iter=2035, avg train loss=0.348, avg val loss=0.599, auc=0.678\n",
      "Iter=2040, avg train loss=0.328, avg val loss=0.587, auc=0.669\n",
      "Iter=2045, avg train loss=0.342, avg val loss=0.654, auc=0.672\n",
      "Iter=2050, avg train loss=0.352, avg val loss=0.629, auc=0.678\n",
      "Iter=2055, avg train loss=0.296, avg val loss=0.560, auc=0.659\n",
      "Iter=2060, avg train loss=0.290, avg val loss=0.553, auc=0.655\n",
      "Iter=2065, avg train loss=0.326, avg val loss=0.564, auc=0.652\n",
      "Iter=2070, avg train loss=0.250, avg val loss=0.603, auc=0.675\n",
      "Iter=2075, avg train loss=0.357, avg val loss=0.606, auc=0.670\n",
      "Iter=2080, avg train loss=0.371, avg val loss=0.615, auc=0.678\n",
      "Iter=2085, avg train loss=0.354, avg val loss=0.571, auc=0.669\n",
      "Iter=2090, avg train loss=0.255, avg val loss=0.582, auc=0.660\n",
      "Iter=2095, avg train loss=0.225, avg val loss=0.613, auc=0.674\n",
      "Iter=2100, avg train loss=0.462, avg val loss=0.569, auc=0.687\n",
      "Iter=2105, avg train loss=0.390, avg val loss=0.552, auc=0.693\n",
      "Iter=2110, avg train loss=0.473, avg val loss=0.573, auc=0.694\n",
      "Iter=2115, avg train loss=0.313, avg val loss=0.637, auc=0.688\n",
      "Iter=2120, avg train loss=0.417, avg val loss=0.617, auc=0.679\n",
      "Iter=2125, avg train loss=0.343, avg val loss=0.556, auc=0.666\n",
      "Iter=2130, avg train loss=0.290, avg val loss=0.553, auc=0.666\n",
      "Iter=2135, avg train loss=0.330, avg val loss=0.560, auc=0.670\n",
      "Iter=2140, avg train loss=0.288, avg val loss=0.556, auc=0.663\n",
      "Iter=2145, avg train loss=0.322, avg val loss=0.551, auc=0.641\n",
      "Iter=2150, avg train loss=0.283, avg val loss=0.585, auc=0.660\n",
      "Iter=2155, avg train loss=0.295, avg val loss=0.633, auc=0.666\n",
      "Iter=2160, avg train loss=0.314, avg val loss=0.647, auc=0.673\n",
      "Iter=2165, avg train loss=0.365, avg val loss=0.595, auc=0.657\n",
      "Iter=2170, avg train loss=0.279, avg val loss=0.563, auc=0.684\n",
      "Iter=2175, avg train loss=0.316, avg val loss=0.561, auc=0.653\n",
      "Iter=2180, avg train loss=0.278, avg val loss=0.562, auc=0.637\n",
      "Iter=2185, avg train loss=0.397, avg val loss=0.622, auc=0.656\n",
      "Iter=2190, avg train loss=0.341, avg val loss=0.806, auc=0.680\n",
      "Iter=2195, avg train loss=0.375, avg val loss=0.753, auc=0.682\n",
      "Iter=2200, avg train loss=0.275, avg val loss=0.592, auc=0.674\n",
      "Iter=2205, avg train loss=0.210, avg val loss=0.570, auc=0.665\n",
      "Iter=2210, avg train loss=0.370, avg val loss=0.564, auc=0.649\n",
      "Iter=2215, avg train loss=0.401, avg val loss=0.558, auc=0.656\n",
      "Iter=2220, avg train loss=0.314, avg val loss=0.568, auc=0.669\n",
      "Iter=2225, avg train loss=0.377, avg val loss=0.582, auc=0.677\n",
      "Iter=2230, avg train loss=0.399, avg val loss=0.562, auc=0.655\n",
      "Iter=2235, avg train loss=0.282, avg val loss=0.572, auc=0.662\n",
      "Iter=2240, avg train loss=0.369, avg val loss=0.622, auc=0.663\n",
      "Iter=2245, avg train loss=0.276, avg val loss=0.622, auc=0.667\n",
      "Iter=2250, avg train loss=0.358, avg val loss=0.616, auc=0.684\n",
      "Iter=2255, avg train loss=0.386, avg val loss=0.614, auc=0.667\n",
      "Iter=2260, avg train loss=0.328, avg val loss=0.649, auc=0.658\n",
      "Iter=2265, avg train loss=0.450, avg val loss=0.670, auc=0.665\n",
      "Iter=2270, avg train loss=0.428, avg val loss=0.620, auc=0.676\n",
      "Iter=2275, avg train loss=0.196, avg val loss=0.601, auc=0.677\n",
      "Iter=2280, avg train loss=0.226, avg val loss=0.590, auc=0.680\n",
      "Iter=2285, avg train loss=0.381, avg val loss=0.584, auc=0.665\n",
      "Iter=2290, avg train loss=0.379, avg val loss=0.600, auc=0.666\n",
      "Iter=2295, avg train loss=0.248, avg val loss=0.564, auc=0.657\n",
      "Iter=2300, avg train loss=0.343, avg val loss=0.577, auc=0.638\n",
      "Iter=2305, avg train loss=0.281, avg val loss=0.578, auc=0.652\n",
      "Iter=2310, avg train loss=0.172, avg val loss=0.631, auc=0.667\n",
      "Iter=2315, avg train loss=0.327, avg val loss=0.579, auc=0.666\n",
      "Iter=2320, avg train loss=0.359, avg val loss=0.584, auc=0.653\n",
      "Iter=2325, avg train loss=0.237, avg val loss=0.702, auc=0.656\n",
      "Iter=2330, avg train loss=0.489, avg val loss=0.645, auc=0.648\n",
      "Iter=2335, avg train loss=0.269, avg val loss=0.638, auc=0.661\n",
      "Iter=2340, avg train loss=0.330, avg val loss=0.623, auc=0.670\n",
      "Iter=2345, avg train loss=0.261, avg val loss=0.597, auc=0.661\n",
      "Iter=2350, avg train loss=0.277, avg val loss=0.577, auc=0.670\n",
      "Iter=2355, avg train loss=0.200, avg val loss=0.589, auc=0.675\n",
      "Iter=2360, avg train loss=0.338, avg val loss=0.574, auc=0.668\n",
      "Iter=2365, avg train loss=0.234, avg val loss=0.586, auc=0.676\n",
      "Iter=2370, avg train loss=0.298, avg val loss=0.577, auc=0.672\n",
      "Iter=2375, avg train loss=0.239, avg val loss=0.603, auc=0.674\n",
      "Iter=2380, avg train loss=0.260, avg val loss=0.597, auc=0.682\n",
      "Iter=2385, avg train loss=0.278, avg val loss=0.614, auc=0.669\n",
      "Iter=2390, avg train loss=0.313, avg val loss=0.565, auc=0.676\n",
      "Iter=2395, avg train loss=0.213, avg val loss=0.549, auc=0.671\n",
      "Iter=2400, avg train loss=0.293, avg val loss=0.575, auc=0.664\n",
      "Iter=2405, avg train loss=0.344, avg val loss=0.582, auc=0.678\n",
      "Iter=2410, avg train loss=0.291, avg val loss=0.645, auc=0.679\n",
      "Iter=2415, avg train loss=0.246, avg val loss=0.587, auc=0.688\n",
      "Iter=2420, avg train loss=0.380, avg val loss=0.567, auc=0.684\n",
      "Iter=2425, avg train loss=0.217, avg val loss=0.661, auc=0.715\n",
      "Iter=2430, avg train loss=0.223, avg val loss=0.654, auc=0.710\n",
      "Iter=2435, avg train loss=0.324, avg val loss=0.622, auc=0.706\n",
      "Iter=2440, avg train loss=0.309, avg val loss=0.716, auc=0.699\n",
      "Iter=2445, avg train loss=0.389, avg val loss=0.574, auc=0.684\n",
      "Iter=2450, avg train loss=0.282, avg val loss=0.555, auc=0.659\n",
      "Iter=2455, avg train loss=0.315, avg val loss=0.547, auc=0.655\n",
      "Iter=2460, avg train loss=0.202, avg val loss=0.572, auc=0.650\n",
      "Iter=2465, avg train loss=0.272, avg val loss=0.622, auc=0.690\n",
      "Iter=2470, avg train loss=0.207, avg val loss=0.653, auc=0.687\n",
      "Iter=2475, avg train loss=0.311, avg val loss=0.729, auc=0.682\n",
      "Iter=2480, avg train loss=0.212, avg val loss=0.576, auc=0.677\n",
      "Iter=2485, avg train loss=0.310, avg val loss=0.548, auc=0.671\n",
      "Iter=2490, avg train loss=0.299, avg val loss=0.551, auc=0.678\n",
      "Iter=2495, avg train loss=0.208, avg val loss=0.564, auc=0.674\n",
      "Iter=2500, avg train loss=0.173, avg val loss=0.596, auc=0.681\n",
      "Iter=2505, avg train loss=0.266, avg val loss=0.568, auc=0.669\n",
      "Iter=2510, avg train loss=0.218, avg val loss=0.566, auc=0.666\n",
      "Iter=2515, avg train loss=0.277, avg val loss=0.619, auc=0.680\n",
      "Iter=2520, avg train loss=0.292, avg val loss=0.603, auc=0.687\n",
      "Iter=2525, avg train loss=0.206, avg val loss=0.560, auc=0.679\n",
      "Iter=2530, avg train loss=0.282, avg val loss=0.599, auc=0.680\n",
      "Iter=2535, avg train loss=0.213, avg val loss=0.646, auc=0.678\n",
      "Iter=2540, avg train loss=0.309, avg val loss=0.620, auc=0.647\n",
      "Iter=2545, avg train loss=0.299, avg val loss=0.600, auc=0.644\n",
      "Iter=2550, avg train loss=0.231, avg val loss=0.597, auc=0.677\n",
      "Iter=2555, avg train loss=0.171, avg val loss=0.641, auc=0.681\n",
      "Iter=2560, avg train loss=0.244, avg val loss=0.700, auc=0.682\n",
      "Iter=2565, avg train loss=0.261, avg val loss=0.668, auc=0.674\n",
      "Iter=2570, avg train loss=0.297, avg val loss=0.592, auc=0.679\n",
      "Iter=2575, avg train loss=0.210, avg val loss=0.596, auc=0.682\n",
      "Iter=2580, avg train loss=0.404, avg val loss=0.576, auc=0.677\n",
      "Iter=2585, avg train loss=0.398, avg val loss=0.566, auc=0.637\n",
      "Iter=2590, avg train loss=0.216, avg val loss=0.598, auc=0.659\n",
      "Iter=2595, avg train loss=0.353, avg val loss=0.628, auc=0.682\n",
      "Iter=2600, avg train loss=0.286, avg val loss=0.617, auc=0.674\n",
      "Iter=2605, avg train loss=0.300, avg val loss=0.654, auc=0.667\n",
      "Iter=2610, avg train loss=0.219, avg val loss=0.648, auc=0.678\n",
      "Iter=2615, avg train loss=0.248, avg val loss=0.639, auc=0.666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=2620, avg train loss=0.218, avg val loss=0.661, auc=0.683\n",
      "Iter=2625, avg train loss=0.245, avg val loss=0.642, auc=0.690\n",
      "Iter=2630, avg train loss=0.194, avg val loss=0.630, auc=0.698\n",
      "Iter=2635, avg train loss=0.203, avg val loss=0.605, auc=0.693\n",
      "Iter=2640, avg train loss=0.295, avg val loss=0.609, auc=0.695\n",
      "Iter=2645, avg train loss=0.279, avg val loss=0.564, auc=0.684\n",
      "Iter=2650, avg train loss=0.380, avg val loss=0.560, auc=0.670\n",
      "Iter=2655, avg train loss=0.270, avg val loss=0.552, auc=0.678\n",
      "Iter=2660, avg train loss=0.189, avg val loss=0.588, auc=0.653\n",
      "Iter=2665, avg train loss=0.232, avg val loss=0.600, auc=0.669\n",
      "Iter=2670, avg train loss=0.203, avg val loss=0.628, auc=0.685\n",
      "Iter=2675, avg train loss=0.182, avg val loss=0.620, auc=0.685\n",
      "Iter=2680, avg train loss=0.338, avg val loss=0.595, auc=0.673\n",
      "Iter=2685, avg train loss=0.189, avg val loss=0.608, auc=0.673\n",
      "Iter=2690, avg train loss=0.289, avg val loss=0.603, auc=0.668\n",
      "Iter=2695, avg train loss=0.256, avg val loss=0.593, auc=0.669\n",
      "Iter=2700, avg train loss=0.261, avg val loss=0.585, auc=0.654\n",
      "Iter=2705, avg train loss=0.248, avg val loss=0.619, auc=0.666\n",
      "Iter=2710, avg train loss=0.307, avg val loss=0.591, auc=0.661\n",
      "Iter=2715, avg train loss=0.405, avg val loss=0.588, auc=0.655\n",
      "Iter=2720, avg train loss=0.216, avg val loss=0.556, auc=0.656\n",
      "Iter=2725, avg train loss=0.153, avg val loss=0.583, auc=0.674\n",
      "Iter=2730, avg train loss=0.235, avg val loss=0.612, auc=0.681\n",
      "Iter=2735, avg train loss=0.232, avg val loss=0.613, auc=0.694\n",
      "Iter=2740, avg train loss=0.189, avg val loss=0.625, auc=0.706\n",
      "Iter=2745, avg train loss=0.167, avg val loss=0.565, auc=0.694\n",
      "Iter=2750, avg train loss=0.245, avg val loss=0.547, auc=0.690\n",
      "Iter=2755, avg train loss=0.298, avg val loss=0.555, auc=0.704\n",
      "Iter=2760, avg train loss=0.188, avg val loss=0.605, auc=0.709\n",
      "Iter=2765, avg train loss=0.247, avg val loss=0.610, auc=0.705\n",
      "Iter=2770, avg train loss=0.215, avg val loss=0.556, auc=0.681\n",
      "Iter=2775, avg train loss=0.262, avg val loss=0.551, auc=0.670\n",
      "Iter=2780, avg train loss=0.222, avg val loss=0.554, auc=0.674\n",
      "Iter=2785, avg train loss=0.322, avg val loss=0.571, auc=0.687\n",
      "Iter=2790, avg train loss=0.335, avg val loss=0.635, auc=0.704\n",
      "Iter=2795, avg train loss=0.180, avg val loss=0.786, auc=0.709\n",
      "Iter=2800, avg train loss=0.232, avg val loss=0.642, auc=0.708\n",
      "Iter=2805, avg train loss=0.168, avg val loss=0.573, auc=0.709\n",
      "Iter=2810, avg train loss=0.210, avg val loss=0.557, auc=0.707\n",
      "Iter=2815, avg train loss=0.206, avg val loss=0.557, auc=0.696\n",
      "Iter=2820, avg train loss=0.170, avg val loss=0.565, auc=0.677\n",
      "Iter=2825, avg train loss=0.174, avg val loss=0.597, auc=0.697\n",
      "Iter=2830, avg train loss=0.148, avg val loss=0.592, auc=0.702\n",
      "Iter=2835, avg train loss=0.206, avg val loss=0.585, auc=0.690\n",
      "Iter=2840, avg train loss=0.235, avg val loss=0.600, auc=0.688\n",
      "Iter=2845, avg train loss=0.165, avg val loss=0.629, auc=0.694\n",
      "Iter=2850, avg train loss=0.190, avg val loss=0.607, auc=0.685\n",
      "Iter=2855, avg train loss=0.340, avg val loss=0.585, auc=0.687\n",
      "Iter=2860, avg train loss=0.169, avg val loss=0.554, auc=0.668\n",
      "Iter=2865, avg train loss=0.176, avg val loss=0.579, auc=0.643\n",
      "Iter=2870, avg train loss=0.289, avg val loss=0.596, auc=0.674\n",
      "Iter=2875, avg train loss=0.192, avg val loss=0.655, auc=0.680\n",
      "Iter=2880, avg train loss=0.206, avg val loss=0.559, auc=0.651\n",
      "Iter=2885, avg train loss=0.157, avg val loss=0.601, auc=0.666\n",
      "Iter=2890, avg train loss=0.230, avg val loss=0.594, auc=0.682\n",
      "Iter=2895, avg train loss=0.165, avg val loss=0.627, auc=0.692\n",
      "Iter=2900, avg train loss=0.220, avg val loss=0.660, auc=0.682\n",
      "Iter=2905, avg train loss=0.198, avg val loss=0.596, auc=0.678\n",
      "Iter=2910, avg train loss=0.135, avg val loss=0.576, auc=0.686\n",
      "Iter=2915, avg train loss=0.240, avg val loss=0.551, auc=0.694\n",
      "Iter=2920, avg train loss=0.160, avg val loss=0.573, auc=0.688\n",
      "Iter=2925, avg train loss=0.138, avg val loss=0.587, auc=0.688\n",
      "Iter=2930, avg train loss=0.121, avg val loss=0.590, auc=0.688\n",
      "Iter=2935, avg train loss=0.188, avg val loss=0.557, auc=0.676\n",
      "Iter=2940, avg train loss=0.282, avg val loss=0.578, auc=0.667\n",
      "Iter=2945, avg train loss=0.145, avg val loss=0.586, auc=0.641\n",
      "Iter=2950, avg train loss=0.168, avg val loss=0.597, auc=0.662\n",
      "Iter=2955, avg train loss=0.196, avg val loss=0.592, auc=0.661\n",
      "Iter=2960, avg train loss=0.333, avg val loss=0.621, auc=0.631\n",
      "Iter=2965, avg train loss=0.269, avg val loss=0.625, auc=0.643\n",
      "Iter=2970, avg train loss=0.297, avg val loss=0.719, auc=0.674\n",
      "Iter=2975, avg train loss=0.165, avg val loss=0.618, auc=0.670\n",
      "Iter=2980, avg train loss=0.148, avg val loss=0.589, auc=0.674\n",
      "Iter=2985, avg train loss=0.238, avg val loss=0.587, auc=0.666\n",
      "Iter=2990, avg train loss=0.179, avg val loss=0.617, auc=0.654\n",
      "Iter=2995, avg train loss=0.250, avg val loss=0.602, auc=0.652\n",
      "Iter=3000, avg train loss=0.339, avg val loss=0.595, auc=0.666\n",
      "Iter=3005, avg train loss=0.229, avg val loss=0.598, auc=0.662\n",
      "Iter=3010, avg train loss=0.222, avg val loss=0.598, auc=0.661\n",
      "Iter=3015, avg train loss=0.192, avg val loss=0.571, auc=0.668\n",
      "Iter=3020, avg train loss=0.280, avg val loss=0.596, auc=0.666\n",
      "Iter=3025, avg train loss=0.236, avg val loss=0.559, auc=0.652\n",
      "Iter=3030, avg train loss=0.136, avg val loss=0.568, auc=0.676\n",
      "Iter=3035, avg train loss=0.214, avg val loss=0.613, auc=0.690\n",
      "Iter=3040, avg train loss=0.167, avg val loss=0.596, auc=0.690\n",
      "Iter=3045, avg train loss=0.149, avg val loss=0.661, auc=0.688\n",
      "Iter=3050, avg train loss=0.191, avg val loss=0.606, auc=0.676\n",
      "Iter=3055, avg train loss=0.232, avg val loss=0.596, auc=0.673\n",
      "Iter=3060, avg train loss=0.227, avg val loss=0.571, auc=0.668\n",
      "Iter=3065, avg train loss=0.202, avg val loss=0.618, auc=0.668\n",
      "Iter=3070, avg train loss=0.175, avg val loss=0.616, auc=0.658\n",
      "Iter=3075, avg train loss=0.224, avg val loss=0.618, auc=0.666\n",
      "Iter=3080, avg train loss=0.128, avg val loss=0.645, auc=0.678\n",
      "Iter=3085, avg train loss=0.123, avg val loss=0.673, auc=0.665\n",
      "Iter=3090, avg train loss=0.203, avg val loss=0.632, auc=0.674\n",
      "Iter=3095, avg train loss=0.214, avg val loss=0.594, auc=0.673\n",
      "Iter=3100, avg train loss=0.346, avg val loss=0.582, auc=0.673\n",
      "Iter=3105, avg train loss=0.191, avg val loss=0.643, auc=0.679\n",
      "Iter=3110, avg train loss=0.247, avg val loss=0.624, auc=0.684\n",
      "Iter=3115, avg train loss=0.187, avg val loss=0.583, auc=0.673\n",
      "Iter=3120, avg train loss=0.253, avg val loss=0.579, auc=0.661\n",
      "Iter=3125, avg train loss=0.349, avg val loss=0.593, auc=0.659\n",
      "Iter=3130, avg train loss=0.219, avg val loss=0.588, auc=0.654\n",
      "Iter=3135, avg train loss=0.227, avg val loss=0.590, auc=0.657\n",
      "Iter=3140, avg train loss=0.159, avg val loss=0.594, auc=0.655\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.729, max-score-based AUC=0.745\n",
      "\n",
      "\n",
      "========== Fold 2 ==========\n",
      "Test AUC at start=0.596, max-score-based AUC=0.620\n",
      "Iter=5, avg train loss=0.912, avg val loss=0.532, auc=0.654\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.776, avg val loss=0.535, auc=0.652\n",
      "Iter=15, avg train loss=0.991, avg val loss=0.530, auc=0.675\n",
      "Best model saved.\n",
      "Iter=20, avg train loss=0.897, avg val loss=0.529, auc=0.680\n",
      "Best model saved.\n",
      "Iter=25, avg train loss=0.809, avg val loss=0.541, auc=0.687\n",
      "Best model saved.\n",
      "Iter=30, avg train loss=0.722, avg val loss=0.550, auc=0.688\n",
      "Best model saved.\n",
      "Iter=35, avg train loss=0.700, avg val loss=0.550, auc=0.689\n",
      "Best model saved.\n",
      "Iter=40, avg train loss=0.702, avg val loss=0.542, auc=0.684\n",
      "Iter=45, avg train loss=0.692, avg val loss=0.535, auc=0.695\n",
      "Best model saved.\n",
      "Iter=50, avg train loss=0.804, avg val loss=0.545, auc=0.680\n",
      "Iter=55, avg train loss=0.702, avg val loss=0.551, auc=0.685\n",
      "Iter=60, avg train loss=0.748, avg val loss=0.550, auc=0.682\n",
      "Iter=65, avg train loss=0.714, avg val loss=0.568, auc=0.685\n",
      "Iter=70, avg train loss=0.659, avg val loss=0.568, auc=0.688\n",
      "Iter=75, avg train loss=0.702, avg val loss=0.579, auc=0.693\n",
      "Iter=80, avg train loss=0.706, avg val loss=0.579, auc=0.688\n",
      "Iter=85, avg train loss=0.685, avg val loss=0.581, auc=0.678\n",
      "Iter=90, avg train loss=0.657, avg val loss=0.601, auc=0.666\n",
      "Iter=95, avg train loss=0.657, avg val loss=0.601, auc=0.667\n",
      "Iter=100, avg train loss=0.658, avg val loss=0.602, auc=0.669\n",
      "Iter=105, avg train loss=0.624, avg val loss=0.583, auc=0.652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=110, avg train loss=0.633, avg val loss=0.582, auc=0.652\n",
      "Iter=115, avg train loss=0.620, avg val loss=0.594, auc=0.649\n",
      "Iter=120, avg train loss=0.709, avg val loss=0.568, auc=0.645\n",
      "Iter=125, avg train loss=0.678, avg val loss=0.587, auc=0.635\n",
      "Iter=130, avg train loss=0.685, avg val loss=0.600, auc=0.651\n",
      "Iter=135, avg train loss=0.618, avg val loss=0.586, auc=0.660\n",
      "Iter=140, avg train loss=0.643, avg val loss=0.622, auc=0.652\n",
      "Iter=145, avg train loss=0.619, avg val loss=0.597, auc=0.658\n",
      "Iter=150, avg train loss=0.596, avg val loss=0.587, auc=0.670\n",
      "Iter=155, avg train loss=0.674, avg val loss=0.593, auc=0.661\n",
      "Iter=160, avg train loss=0.709, avg val loss=0.617, auc=0.652\n",
      "Iter=165, avg train loss=0.688, avg val loss=0.654, auc=0.660\n",
      "Iter=170, avg train loss=0.637, avg val loss=0.666, auc=0.657\n",
      "Iter=175, avg train loss=0.645, avg val loss=0.642, auc=0.659\n",
      "Iter=180, avg train loss=0.634, avg val loss=0.619, auc=0.682\n",
      "Iter=185, avg train loss=0.621, avg val loss=0.620, auc=0.688\n",
      "Iter=190, avg train loss=0.719, avg val loss=0.602, auc=0.647\n",
      "Iter=195, avg train loss=0.666, avg val loss=0.596, auc=0.654\n",
      "Iter=200, avg train loss=0.690, avg val loss=0.590, auc=0.654\n",
      "Iter=205, avg train loss=0.657, avg val loss=0.613, auc=0.638\n",
      "Iter=210, avg train loss=0.737, avg val loss=0.639, auc=0.630\n",
      "Iter=215, avg train loss=0.724, avg val loss=0.615, auc=0.662\n",
      "Iter=220, avg train loss=0.719, avg val loss=0.599, auc=0.664\n",
      "Iter=225, avg train loss=0.637, avg val loss=0.631, auc=0.671\n",
      "Iter=230, avg train loss=0.656, avg val loss=0.674, auc=0.667\n",
      "Iter=235, avg train loss=0.675, avg val loss=0.648, auc=0.672\n",
      "Iter=240, avg train loss=0.680, avg val loss=0.669, auc=0.683\n",
      "Iter=245, avg train loss=0.619, avg val loss=0.616, auc=0.684\n",
      "Iter=250, avg train loss=0.682, avg val loss=0.665, auc=0.668\n",
      "Iter=255, avg train loss=0.679, avg val loss=0.651, auc=0.668\n",
      "Iter=260, avg train loss=0.647, avg val loss=0.652, auc=0.666\n",
      "Iter=265, avg train loss=0.656, avg val loss=0.610, auc=0.670\n",
      "Iter=270, avg train loss=0.615, avg val loss=0.614, auc=0.670\n",
      "Iter=275, avg train loss=0.637, avg val loss=0.608, auc=0.656\n",
      "Iter=280, avg train loss=0.670, avg val loss=0.634, auc=0.680\n",
      "Iter=285, avg train loss=0.700, avg val loss=0.646, auc=0.690\n",
      "Iter=290, avg train loss=0.670, avg val loss=0.595, auc=0.678\n",
      "Iter=295, avg train loss=0.679, avg val loss=0.590, auc=0.685\n",
      "Iter=300, avg train loss=0.710, avg val loss=0.613, auc=0.670\n",
      "Iter=305, avg train loss=0.663, avg val loss=0.618, auc=0.685\n",
      "Iter=310, avg train loss=0.607, avg val loss=0.610, auc=0.691\n",
      "Iter=315, avg train loss=0.671, avg val loss=0.603, auc=0.698\n",
      "Best model saved.\n",
      "Iter=320, avg train loss=0.535, avg val loss=0.560, auc=0.704\n",
      "Best model saved.\n",
      "Iter=325, avg train loss=0.718, avg val loss=0.561, auc=0.704\n",
      "Iter=330, avg train loss=0.626, avg val loss=0.578, auc=0.696\n",
      "Iter=335, avg train loss=0.628, avg val loss=0.606, auc=0.693\n",
      "Iter=340, avg train loss=0.651, avg val loss=0.629, auc=0.692\n",
      "Iter=345, avg train loss=0.620, avg val loss=0.632, auc=0.691\n",
      "Iter=350, avg train loss=0.697, avg val loss=0.614, auc=0.692\n",
      "Iter=355, avg train loss=0.694, avg val loss=0.580, auc=0.691\n",
      "Iter=360, avg train loss=0.605, avg val loss=0.594, auc=0.687\n",
      "Iter=365, avg train loss=0.649, avg val loss=0.573, auc=0.694\n",
      "Iter=370, avg train loss=0.664, avg val loss=0.617, auc=0.694\n",
      "Iter=375, avg train loss=0.721, avg val loss=0.587, auc=0.693\n",
      "Iter=380, avg train loss=0.645, avg val loss=0.576, auc=0.697\n",
      "Iter=385, avg train loss=0.632, avg val loss=0.560, auc=0.698\n",
      "Iter=390, avg train loss=0.676, avg val loss=0.581, auc=0.696\n",
      "Iter=395, avg train loss=0.604, avg val loss=0.573, auc=0.693\n",
      "Iter=400, avg train loss=0.638, avg val loss=0.594, auc=0.686\n",
      "Iter=405, avg train loss=0.769, avg val loss=0.580, auc=0.702\n",
      "Iter=410, avg train loss=0.717, avg val loss=0.624, auc=0.691\n",
      "Iter=415, avg train loss=0.667, avg val loss=0.689, auc=0.679\n",
      "Iter=420, avg train loss=0.670, avg val loss=0.643, auc=0.684\n",
      "Iter=425, avg train loss=0.575, avg val loss=0.637, auc=0.684\n",
      "Iter=430, avg train loss=0.665, avg val loss=0.610, auc=0.682\n",
      "Iter=435, avg train loss=0.641, avg val loss=0.589, auc=0.678\n",
      "Iter=440, avg train loss=0.599, avg val loss=0.571, auc=0.689\n",
      "Iter=445, avg train loss=0.654, avg val loss=0.575, auc=0.689\n",
      "Iter=450, avg train loss=0.694, avg val loss=0.600, auc=0.679\n",
      "Iter=455, avg train loss=0.642, avg val loss=0.603, auc=0.679\n",
      "Iter=460, avg train loss=0.591, avg val loss=0.583, auc=0.673\n",
      "Iter=465, avg train loss=0.659, avg val loss=0.577, auc=0.664\n",
      "Iter=470, avg train loss=0.640, avg val loss=0.613, auc=0.657\n",
      "Iter=475, avg train loss=0.599, avg val loss=0.627, auc=0.666\n",
      "Iter=480, avg train loss=0.643, avg val loss=0.623, auc=0.670\n",
      "Iter=485, avg train loss=0.672, avg val loss=0.635, auc=0.673\n",
      "Iter=490, avg train loss=0.584, avg val loss=0.641, auc=0.677\n",
      "Iter=495, avg train loss=0.654, avg val loss=0.609, auc=0.678\n",
      "Iter=500, avg train loss=0.589, avg val loss=0.610, auc=0.681\n",
      "Iter=505, avg train loss=0.640, avg val loss=0.630, auc=0.679\n",
      "Iter=510, avg train loss=0.583, avg val loss=0.619, auc=0.683\n",
      "Iter=515, avg train loss=0.565, avg val loss=0.584, auc=0.686\n",
      "Iter=520, avg train loss=0.613, avg val loss=0.572, auc=0.687\n",
      "Iter=525, avg train loss=0.638, avg val loss=0.576, auc=0.685\n",
      "Iter=530, avg train loss=0.526, avg val loss=0.563, auc=0.686\n",
      "Iter=535, avg train loss=0.567, avg val loss=0.561, auc=0.689\n",
      "Iter=540, avg train loss=0.532, avg val loss=0.589, auc=0.685\n",
      "Iter=545, avg train loss=0.524, avg val loss=0.578, auc=0.685\n",
      "Iter=550, avg train loss=0.585, avg val loss=0.542, auc=0.689\n",
      "Iter=555, avg train loss=0.728, avg val loss=0.531, auc=0.688\n",
      "Iter=560, avg train loss=0.514, avg val loss=0.545, auc=0.685\n",
      "Iter=565, avg train loss=0.732, avg val loss=0.539, auc=0.674\n",
      "Iter=570, avg train loss=0.583, avg val loss=0.590, auc=0.677\n",
      "Iter=575, avg train loss=0.649, avg val loss=0.647, auc=0.676\n",
      "Iter=580, avg train loss=0.658, avg val loss=0.684, auc=0.677\n",
      "Iter=585, avg train loss=0.588, avg val loss=0.593, auc=0.690\n",
      "Iter=590, avg train loss=0.570, avg val loss=0.542, auc=0.692\n",
      "Iter=595, avg train loss=0.712, avg val loss=0.557, auc=0.682\n",
      "Iter=600, avg train loss=0.638, avg val loss=0.542, auc=0.684\n",
      "Iter=605, avg train loss=0.646, avg val loss=0.556, auc=0.700\n",
      "Iter=610, avg train loss=0.581, avg val loss=0.568, auc=0.694\n",
      "Iter=615, avg train loss=0.617, avg val loss=0.555, auc=0.677\n",
      "Iter=620, avg train loss=0.608, avg val loss=0.563, auc=0.687\n",
      "Iter=625, avg train loss=0.740, avg val loss=0.596, auc=0.680\n",
      "Iter=630, avg train loss=0.563, avg val loss=0.646, auc=0.667\n",
      "Iter=635, avg train loss=0.643, avg val loss=0.611, auc=0.668\n",
      "Iter=640, avg train loss=0.538, avg val loss=0.610, auc=0.677\n",
      "Iter=645, avg train loss=0.633, avg val loss=0.677, auc=0.660\n",
      "Iter=650, avg train loss=0.648, avg val loss=0.771, auc=0.646\n",
      "Iter=655, avg train loss=0.654, avg val loss=0.618, auc=0.657\n",
      "Iter=660, avg train loss=0.685, avg val loss=0.604, auc=0.646\n",
      "Iter=665, avg train loss=0.604, avg val loss=0.618, auc=0.664\n",
      "Iter=670, avg train loss=0.563, avg val loss=0.604, auc=0.664\n",
      "Iter=675, avg train loss=0.684, avg val loss=0.562, auc=0.670\n",
      "Iter=680, avg train loss=0.619, avg val loss=0.569, auc=0.679\n",
      "Iter=685, avg train loss=0.636, avg val loss=0.634, auc=0.683\n",
      "Iter=690, avg train loss=0.587, avg val loss=0.601, auc=0.692\n",
      "Iter=695, avg train loss=0.524, avg val loss=0.592, auc=0.694\n",
      "Iter=700, avg train loss=0.620, avg val loss=0.601, auc=0.694\n",
      "Iter=705, avg train loss=0.606, avg val loss=0.609, auc=0.688\n",
      "Iter=710, avg train loss=0.567, avg val loss=0.592, auc=0.696\n",
      "Iter=715, avg train loss=0.525, avg val loss=0.544, auc=0.696\n",
      "Iter=720, avg train loss=0.692, avg val loss=0.544, auc=0.694\n",
      "Iter=725, avg train loss=0.679, avg val loss=0.552, auc=0.695\n",
      "Iter=730, avg train loss=0.668, avg val loss=0.550, auc=0.686\n",
      "Iter=735, avg train loss=0.674, avg val loss=0.571, auc=0.703\n",
      "Iter=740, avg train loss=0.703, avg val loss=0.597, auc=0.708\n",
      "Best model saved.\n",
      "Iter=745, avg train loss=0.620, avg val loss=0.699, auc=0.700\n",
      "Iter=750, avg train loss=0.575, avg val loss=0.659, auc=0.710\n",
      "Best model saved.\n",
      "Iter=755, avg train loss=0.628, avg val loss=0.703, auc=0.690\n",
      "Iter=760, avg train loss=0.623, avg val loss=0.647, auc=0.685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=765, avg train loss=0.572, avg val loss=0.610, auc=0.677\n",
      "Iter=770, avg train loss=0.662, avg val loss=0.549, auc=0.695\n",
      "Iter=775, avg train loss=0.598, avg val loss=0.522, auc=0.697\n",
      "Iter=780, avg train loss=0.547, avg val loss=0.524, auc=0.697\n",
      "Iter=785, avg train loss=0.557, avg val loss=0.532, auc=0.702\n",
      "Iter=790, avg train loss=0.609, avg val loss=0.543, auc=0.706\n",
      "Iter=795, avg train loss=0.654, avg val loss=0.594, auc=0.699\n",
      "Iter=800, avg train loss=0.652, avg val loss=0.567, auc=0.698\n",
      "Iter=805, avg train loss=0.549, avg val loss=0.559, auc=0.699\n",
      "Iter=810, avg train loss=0.566, avg val loss=0.575, auc=0.705\n",
      "Iter=815, avg train loss=0.556, avg val loss=0.561, auc=0.700\n",
      "Iter=820, avg train loss=0.575, avg val loss=0.586, auc=0.702\n",
      "Iter=825, avg train loss=0.541, avg val loss=0.557, auc=0.713\n",
      "Best model saved.\n",
      "Iter=830, avg train loss=0.608, avg val loss=0.581, auc=0.721\n",
      "Best model saved.\n",
      "Iter=835, avg train loss=0.583, avg val loss=0.591, auc=0.718\n",
      "Iter=840, avg train loss=0.580, avg val loss=0.618, auc=0.726\n",
      "Best model saved.\n",
      "Iter=845, avg train loss=0.576, avg val loss=0.640, auc=0.717\n",
      "Iter=850, avg train loss=0.563, avg val loss=0.609, auc=0.713\n",
      "Iter=855, avg train loss=0.512, avg val loss=0.555, auc=0.719\n",
      "Iter=860, avg train loss=0.495, avg val loss=0.522, auc=0.717\n",
      "Iter=865, avg train loss=0.572, avg val loss=0.511, auc=0.716\n",
      "Iter=870, avg train loss=0.611, avg val loss=0.514, auc=0.725\n",
      "Iter=875, avg train loss=0.575, avg val loss=0.547, auc=0.722\n",
      "Iter=880, avg train loss=0.469, avg val loss=0.547, auc=0.716\n",
      "Iter=885, avg train loss=0.538, avg val loss=0.603, auc=0.710\n",
      "Iter=890, avg train loss=0.556, avg val loss=0.573, auc=0.712\n",
      "Iter=895, avg train loss=0.521, avg val loss=0.544, auc=0.715\n",
      "Iter=900, avg train loss=0.584, avg val loss=0.558, auc=0.712\n",
      "Iter=905, avg train loss=0.640, avg val loss=0.563, auc=0.716\n",
      "Iter=910, avg train loss=0.496, avg val loss=0.586, auc=0.716\n",
      "Iter=915, avg train loss=0.582, avg val loss=0.645, auc=0.704\n",
      "Iter=920, avg train loss=0.493, avg val loss=0.706, auc=0.697\n",
      "Iter=925, avg train loss=0.553, avg val loss=0.686, auc=0.695\n",
      "Iter=930, avg train loss=0.582, avg val loss=0.646, auc=0.695\n",
      "Iter=935, avg train loss=0.561, avg val loss=0.603, auc=0.699\n",
      "Iter=940, avg train loss=0.566, avg val loss=0.568, auc=0.697\n",
      "Iter=945, avg train loss=0.509, avg val loss=0.540, auc=0.701\n",
      "Iter=950, avg train loss=0.652, avg val loss=0.545, auc=0.700\n",
      "Iter=955, avg train loss=0.590, avg val loss=0.543, auc=0.711\n",
      "Iter=960, avg train loss=0.540, avg val loss=0.543, auc=0.712\n",
      "Iter=965, avg train loss=0.542, avg val loss=0.585, auc=0.704\n",
      "Iter=970, avg train loss=0.501, avg val loss=0.638, auc=0.692\n",
      "Iter=975, avg train loss=0.483, avg val loss=0.590, auc=0.707\n",
      "Iter=980, avg train loss=0.587, avg val loss=0.599, auc=0.709\n",
      "Iter=985, avg train loss=0.535, avg val loss=0.585, auc=0.715\n",
      "Iter=990, avg train loss=0.532, avg val loss=0.567, auc=0.722\n",
      "Iter=995, avg train loss=0.551, avg val loss=0.524, auc=0.722\n",
      "Iter=1000, avg train loss=0.580, avg val loss=0.525, auc=0.725\n",
      "Iter=1005, avg train loss=0.594, avg val loss=0.559, auc=0.718\n",
      "Iter=1010, avg train loss=0.548, avg val loss=0.570, auc=0.714\n",
      "Iter=1015, avg train loss=0.597, avg val loss=0.585, auc=0.717\n",
      "Iter=1020, avg train loss=0.539, avg val loss=0.553, auc=0.710\n",
      "Iter=1025, avg train loss=0.578, avg val loss=0.626, auc=0.711\n",
      "Iter=1030, avg train loss=0.549, avg val loss=0.592, auc=0.703\n",
      "Iter=1035, avg train loss=0.432, avg val loss=0.600, auc=0.704\n",
      "Iter=1040, avg train loss=0.475, avg val loss=0.648, auc=0.704\n",
      "Iter=1045, avg train loss=0.542, avg val loss=0.606, auc=0.708\n",
      "Iter=1050, avg train loss=0.518, avg val loss=0.618, auc=0.707\n",
      "Iter=1055, avg train loss=0.529, avg val loss=0.563, auc=0.705\n",
      "Iter=1060, avg train loss=0.512, avg val loss=0.552, auc=0.704\n",
      "Iter=1065, avg train loss=0.406, avg val loss=0.541, auc=0.706\n",
      "Iter=1070, avg train loss=0.611, avg val loss=0.541, auc=0.707\n",
      "Iter=1075, avg train loss=0.658, avg val loss=0.644, auc=0.703\n",
      "Iter=1080, avg train loss=0.574, avg val loss=0.622, auc=0.697\n",
      "Iter=1085, avg train loss=0.625, avg val loss=0.665, auc=0.692\n",
      "Iter=1090, avg train loss=0.413, avg val loss=0.637, auc=0.688\n",
      "Iter=1095, avg train loss=0.544, avg val loss=0.573, auc=0.689\n",
      "Iter=1100, avg train loss=0.554, avg val loss=0.536, auc=0.700\n",
      "Iter=1105, avg train loss=0.572, avg val loss=0.658, auc=0.693\n",
      "Iter=1110, avg train loss=0.630, avg val loss=0.641, auc=0.688\n",
      "Iter=1115, avg train loss=0.531, avg val loss=0.632, auc=0.688\n",
      "Iter=1120, avg train loss=0.537, avg val loss=0.578, auc=0.684\n",
      "Iter=1125, avg train loss=0.459, avg val loss=0.521, auc=0.694\n",
      "Iter=1130, avg train loss=0.455, avg val loss=0.523, auc=0.697\n",
      "Iter=1135, avg train loss=0.517, avg val loss=0.523, auc=0.693\n",
      "Iter=1140, avg train loss=0.466, avg val loss=0.544, auc=0.699\n",
      "Iter=1145, avg train loss=0.557, avg val loss=0.693, auc=0.683\n",
      "Iter=1150, avg train loss=0.413, avg val loss=0.683, auc=0.684\n",
      "Iter=1155, avg train loss=0.500, avg val loss=0.765, auc=0.679\n",
      "Iter=1160, avg train loss=0.505, avg val loss=0.612, auc=0.681\n",
      "Iter=1165, avg train loss=0.542, avg val loss=0.555, auc=0.680\n",
      "Iter=1170, avg train loss=0.558, avg val loss=0.563, auc=0.687\n",
      "Iter=1175, avg train loss=0.532, avg val loss=0.574, auc=0.688\n",
      "Iter=1180, avg train loss=0.556, avg val loss=0.547, auc=0.680\n",
      "Iter=1185, avg train loss=0.515, avg val loss=0.525, auc=0.688\n",
      "Iter=1190, avg train loss=0.599, avg val loss=0.577, auc=0.685\n",
      "Iter=1195, avg train loss=0.577, avg val loss=0.673, auc=0.686\n",
      "Iter=1200, avg train loss=0.511, avg val loss=0.644, auc=0.692\n",
      "Iter=1205, avg train loss=0.542, avg val loss=0.558, auc=0.700\n",
      "Iter=1210, avg train loss=0.444, avg val loss=0.560, auc=0.703\n",
      "Iter=1215, avg train loss=0.499, avg val loss=0.566, auc=0.709\n",
      "Iter=1220, avg train loss=0.477, avg val loss=0.603, auc=0.696\n",
      "Iter=1225, avg train loss=0.602, avg val loss=0.531, auc=0.695\n",
      "Iter=1230, avg train loss=0.395, avg val loss=0.593, auc=0.700\n",
      "Iter=1235, avg train loss=0.548, avg val loss=0.541, auc=0.697\n",
      "Iter=1240, avg train loss=0.532, avg val loss=0.587, auc=0.700\n",
      "Iter=1245, avg train loss=0.526, avg val loss=0.750, auc=0.702\n",
      "Iter=1250, avg train loss=0.416, avg val loss=0.874, auc=0.696\n",
      "Iter=1255, avg train loss=0.496, avg val loss=0.772, auc=0.694\n",
      "Iter=1260, avg train loss=0.613, avg val loss=0.646, auc=0.707\n",
      "Iter=1265, avg train loss=0.617, avg val loss=0.524, auc=0.704\n",
      "Iter=1270, avg train loss=0.503, avg val loss=0.503, auc=0.707\n",
      "Iter=1275, avg train loss=0.473, avg val loss=0.521, auc=0.702\n",
      "Iter=1280, avg train loss=0.490, avg val loss=0.530, auc=0.710\n",
      "Iter=1285, avg train loss=0.499, avg val loss=0.598, auc=0.704\n",
      "Iter=1290, avg train loss=0.534, avg val loss=0.578, auc=0.701\n",
      "Iter=1295, avg train loss=0.555, avg val loss=0.628, auc=0.701\n",
      "Iter=1300, avg train loss=0.506, avg val loss=0.661, auc=0.695\n",
      "Iter=1305, avg train loss=0.658, avg val loss=0.576, auc=0.703\n",
      "Iter=1310, avg train loss=0.538, avg val loss=0.568, auc=0.710\n",
      "Iter=1315, avg train loss=0.514, avg val loss=0.540, auc=0.697\n",
      "Iter=1320, avg train loss=0.499, avg val loss=0.524, auc=0.694\n",
      "Iter=1325, avg train loss=0.424, avg val loss=0.519, auc=0.710\n",
      "Iter=1330, avg train loss=0.563, avg val loss=0.518, auc=0.713\n",
      "Iter=1335, avg train loss=0.466, avg val loss=0.556, auc=0.706\n",
      "Iter=1340, avg train loss=0.428, avg val loss=0.569, auc=0.707\n",
      "Iter=1345, avg train loss=0.680, avg val loss=0.591, auc=0.708\n",
      "Iter=1350, avg train loss=0.499, avg val loss=0.551, auc=0.699\n",
      "Iter=1355, avg train loss=0.475, avg val loss=0.557, auc=0.697\n",
      "Iter=1360, avg train loss=0.512, avg val loss=0.570, auc=0.697\n",
      "Iter=1365, avg train loss=0.397, avg val loss=0.535, auc=0.702\n",
      "Iter=1370, avg train loss=0.445, avg val loss=0.604, auc=0.693\n",
      "Iter=1375, avg train loss=0.555, avg val loss=0.542, auc=0.694\n",
      "Iter=1380, avg train loss=0.454, avg val loss=0.511, auc=0.688\n",
      "Iter=1385, avg train loss=0.369, avg val loss=0.516, auc=0.684\n",
      "Iter=1390, avg train loss=0.527, avg val loss=0.539, auc=0.671\n",
      "Iter=1395, avg train loss=0.329, avg val loss=0.587, auc=0.697\n",
      "Iter=1400, avg train loss=0.447, avg val loss=0.631, auc=0.683\n",
      "Iter=1405, avg train loss=0.465, avg val loss=0.618, auc=0.691\n",
      "Iter=1410, avg train loss=0.563, avg val loss=0.562, auc=0.673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1415, avg train loss=0.474, avg val loss=0.546, auc=0.678\n",
      "Iter=1420, avg train loss=0.502, avg val loss=0.589, auc=0.688\n",
      "Iter=1425, avg train loss=0.642, avg val loss=0.587, auc=0.693\n",
      "Iter=1430, avg train loss=0.501, avg val loss=0.663, auc=0.680\n",
      "Iter=1435, avg train loss=0.591, avg val loss=0.575, auc=0.677\n",
      "Iter=1440, avg train loss=0.419, avg val loss=0.552, auc=0.681\n",
      "Iter=1445, avg train loss=0.499, avg val loss=0.569, auc=0.673\n",
      "Iter=1450, avg train loss=0.601, avg val loss=0.561, auc=0.668\n",
      "Iter=1455, avg train loss=0.533, avg val loss=0.582, auc=0.667\n",
      "Iter=1460, avg train loss=0.527, avg val loss=0.565, auc=0.679\n",
      "Iter=1465, avg train loss=0.468, avg val loss=0.609, auc=0.691\n",
      "Iter=1470, avg train loss=0.497, avg val loss=0.579, auc=0.701\n",
      "Iter=1475, avg train loss=0.549, avg val loss=0.549, auc=0.699\n",
      "Iter=1480, avg train loss=0.474, avg val loss=0.569, auc=0.701\n",
      "Iter=1485, avg train loss=0.405, avg val loss=0.542, auc=0.699\n",
      "Iter=1490, avg train loss=0.507, avg val loss=0.553, auc=0.699\n",
      "Iter=1495, avg train loss=0.348, avg val loss=0.611, auc=0.699\n",
      "Iter=1500, avg train loss=0.374, avg val loss=0.530, auc=0.700\n",
      "Iter=1505, avg train loss=0.465, avg val loss=0.530, auc=0.692\n",
      "Iter=1510, avg train loss=0.510, avg val loss=0.533, auc=0.692\n",
      "Iter=1515, avg train loss=0.403, avg val loss=0.569, auc=0.703\n",
      "Iter=1520, avg train loss=0.377, avg val loss=0.590, auc=0.701\n",
      "Iter=1525, avg train loss=0.368, avg val loss=0.547, auc=0.710\n",
      "Iter=1530, avg train loss=0.507, avg val loss=0.503, auc=0.711\n",
      "Iter=1535, avg train loss=0.457, avg val loss=0.515, auc=0.707\n",
      "Iter=1540, avg train loss=0.476, avg val loss=0.519, auc=0.712\n",
      "Iter=1545, avg train loss=0.448, avg val loss=0.522, auc=0.713\n",
      "Iter=1550, avg train loss=0.547, avg val loss=0.744, auc=0.701\n",
      "Iter=1555, avg train loss=0.555, avg val loss=0.761, auc=0.705\n",
      "Iter=1560, avg train loss=0.473, avg val loss=0.697, auc=0.712\n",
      "Iter=1565, avg train loss=0.519, avg val loss=0.547, auc=0.698\n",
      "Iter=1570, avg train loss=0.536, avg val loss=0.510, auc=0.689\n",
      "Iter=1575, avg train loss=0.385, avg val loss=0.509, auc=0.694\n",
      "Iter=1580, avg train loss=0.528, avg val loss=0.532, auc=0.718\n",
      "Iter=1585, avg train loss=0.354, avg val loss=0.636, auc=0.725\n",
      "Iter=1590, avg train loss=0.502, avg val loss=0.579, auc=0.732\n",
      "Best model saved.\n",
      "Iter=1595, avg train loss=0.509, avg val loss=0.549, auc=0.731\n",
      "Iter=1600, avg train loss=0.452, avg val loss=0.521, auc=0.717\n",
      "Iter=1605, avg train loss=0.477, avg val loss=0.543, auc=0.711\n",
      "Iter=1610, avg train loss=0.453, avg val loss=0.548, auc=0.705\n",
      "Iter=1615, avg train loss=0.399, avg val loss=0.652, auc=0.705\n",
      "Iter=1620, avg train loss=0.370, avg val loss=0.777, auc=0.707\n",
      "Iter=1625, avg train loss=0.395, avg val loss=0.758, auc=0.715\n",
      "Iter=1630, avg train loss=0.583, avg val loss=0.592, auc=0.722\n",
      "Iter=1635, avg train loss=0.292, avg val loss=0.507, auc=0.721\n",
      "Iter=1640, avg train loss=0.397, avg val loss=0.524, auc=0.717\n",
      "Iter=1645, avg train loss=0.465, avg val loss=0.539, auc=0.715\n",
      "Iter=1650, avg train loss=0.355, avg val loss=0.597, auc=0.720\n",
      "Iter=1655, avg train loss=0.354, avg val loss=0.554, auc=0.716\n",
      "Iter=1660, avg train loss=0.351, avg val loss=0.571, auc=0.703\n",
      "Iter=1665, avg train loss=0.352, avg val loss=0.573, auc=0.702\n",
      "Iter=1670, avg train loss=0.458, avg val loss=0.553, auc=0.708\n",
      "Iter=1675, avg train loss=0.376, avg val loss=0.537, auc=0.698\n",
      "Iter=1680, avg train loss=0.421, avg val loss=0.515, auc=0.692\n",
      "Iter=1685, avg train loss=0.392, avg val loss=0.528, auc=0.690\n",
      "Iter=1690, avg train loss=0.484, avg val loss=0.530, auc=0.683\n",
      "Iter=1695, avg train loss=0.356, avg val loss=0.580, auc=0.704\n",
      "Iter=1700, avg train loss=0.407, avg val loss=0.584, auc=0.705\n",
      "Iter=1705, avg train loss=0.465, avg val loss=0.699, auc=0.694\n",
      "Iter=1710, avg train loss=0.450, avg val loss=0.560, auc=0.700\n",
      "Iter=1715, avg train loss=0.417, avg val loss=0.591, auc=0.704\n",
      "Iter=1720, avg train loss=0.325, avg val loss=0.597, auc=0.702\n",
      "Iter=1725, avg train loss=0.512, avg val loss=0.561, auc=0.702\n",
      "Iter=1730, avg train loss=0.329, avg val loss=0.550, auc=0.704\n",
      "Iter=1735, avg train loss=0.392, avg val loss=0.540, auc=0.698\n",
      "Iter=1740, avg train loss=0.359, avg val loss=0.531, auc=0.691\n",
      "Iter=1745, avg train loss=0.436, avg val loss=0.557, auc=0.685\n",
      "Iter=1750, avg train loss=0.384, avg val loss=0.574, auc=0.699\n",
      "Iter=1755, avg train loss=0.370, avg val loss=0.771, auc=0.698\n",
      "Iter=1760, avg train loss=0.519, avg val loss=0.694, auc=0.702\n",
      "Iter=1765, avg train loss=0.574, avg val loss=0.543, auc=0.700\n",
      "Iter=1770, avg train loss=0.354, avg val loss=0.520, auc=0.692\n",
      "Iter=1775, avg train loss=0.599, avg val loss=0.549, auc=0.692\n",
      "Iter=1780, avg train loss=0.449, avg val loss=0.566, auc=0.688\n",
      "Iter=1785, avg train loss=0.448, avg val loss=0.650, auc=0.685\n",
      "Iter=1790, avg train loss=0.376, avg val loss=0.645, auc=0.688\n",
      "Iter=1795, avg train loss=0.432, avg val loss=0.621, auc=0.677\n",
      "Iter=1800, avg train loss=0.312, avg val loss=0.553, auc=0.681\n",
      "Iter=1805, avg train loss=0.478, avg val loss=0.568, auc=0.696\n",
      "Iter=1810, avg train loss=0.468, avg val loss=0.559, auc=0.702\n",
      "Iter=1815, avg train loss=0.434, avg val loss=0.592, auc=0.697\n",
      "Iter=1820, avg train loss=0.373, avg val loss=0.544, auc=0.699\n",
      "Iter=1825, avg train loss=0.337, avg val loss=0.560, auc=0.701\n",
      "Iter=1830, avg train loss=0.500, avg val loss=0.551, auc=0.703\n",
      "Iter=1835, avg train loss=0.424, avg val loss=0.516, auc=0.696\n",
      "Iter=1840, avg train loss=0.366, avg val loss=0.541, auc=0.693\n",
      "Iter=1845, avg train loss=0.357, avg val loss=0.536, auc=0.700\n",
      "Iter=1850, avg train loss=0.440, avg val loss=0.583, auc=0.692\n",
      "Iter=1855, avg train loss=0.339, avg val loss=0.619, auc=0.691\n",
      "Iter=1860, avg train loss=0.368, avg val loss=0.633, auc=0.694\n",
      "Iter=1865, avg train loss=0.342, avg val loss=0.733, auc=0.698\n",
      "Iter=1870, avg train loss=0.387, avg val loss=0.572, auc=0.694\n",
      "Iter=1875, avg train loss=0.352, avg val loss=0.540, auc=0.689\n",
      "Iter=1880, avg train loss=0.316, avg val loss=0.531, auc=0.685\n",
      "Iter=1885, avg train loss=0.384, avg val loss=0.545, auc=0.700\n",
      "Iter=1890, avg train loss=0.385, avg val loss=0.608, auc=0.702\n",
      "Iter=1895, avg train loss=0.384, avg val loss=0.623, auc=0.704\n",
      "Iter=1900, avg train loss=0.337, avg val loss=0.569, auc=0.710\n",
      "Iter=1905, avg train loss=0.349, avg val loss=0.571, auc=0.703\n",
      "Iter=1910, avg train loss=0.383, avg val loss=0.573, auc=0.700\n",
      "Iter=1915, avg train loss=0.266, avg val loss=0.577, auc=0.706\n",
      "Iter=1920, avg train loss=0.429, avg val loss=0.531, auc=0.704\n",
      "Iter=1925, avg train loss=0.506, avg val loss=0.537, auc=0.704\n",
      "Iter=1930, avg train loss=0.474, avg val loss=0.715, auc=0.698\n",
      "Iter=1935, avg train loss=0.392, avg val loss=0.562, auc=0.692\n",
      "Iter=1940, avg train loss=0.373, avg val loss=0.560, auc=0.683\n",
      "Iter=1945, avg train loss=0.265, avg val loss=0.551, auc=0.681\n",
      "Iter=1950, avg train loss=0.342, avg val loss=0.538, auc=0.682\n",
      "Iter=1955, avg train loss=0.382, avg val loss=0.522, auc=0.684\n",
      "Iter=1960, avg train loss=0.299, avg val loss=0.531, auc=0.698\n",
      "Iter=1965, avg train loss=0.258, avg val loss=0.544, auc=0.696\n",
      "Iter=1970, avg train loss=0.394, avg val loss=0.572, auc=0.685\n",
      "Iter=1975, avg train loss=0.395, avg val loss=0.552, auc=0.672\n",
      "Iter=1980, avg train loss=0.358, avg val loss=0.540, auc=0.675\n",
      "Iter=1985, avg train loss=0.386, avg val loss=0.536, auc=0.693\n",
      "Iter=1990, avg train loss=0.315, avg val loss=0.582, auc=0.704\n",
      "Iter=1995, avg train loss=0.529, avg val loss=0.601, auc=0.696\n",
      "Iter=2000, avg train loss=0.362, avg val loss=0.612, auc=0.701\n",
      "Iter=2005, avg train loss=0.376, avg val loss=0.614, auc=0.701\n",
      "Iter=2010, avg train loss=0.344, avg val loss=0.552, auc=0.702\n",
      "Iter=2015, avg train loss=0.289, avg val loss=0.541, auc=0.685\n",
      "Iter=2020, avg train loss=0.420, avg val loss=0.592, auc=0.664\n",
      "Iter=2025, avg train loss=0.298, avg val loss=0.574, auc=0.686\n",
      "Iter=2030, avg train loss=0.427, avg val loss=0.775, auc=0.681\n",
      "Iter=2035, avg train loss=0.532, avg val loss=0.765, auc=0.684\n",
      "Iter=2040, avg train loss=0.360, avg val loss=0.651, auc=0.689\n",
      "Iter=2045, avg train loss=0.346, avg val loss=0.547, auc=0.689\n",
      "Iter=2050, avg train loss=0.382, avg val loss=0.556, auc=0.691\n",
      "Iter=2055, avg train loss=0.361, avg val loss=0.607, auc=0.698\n",
      "Iter=2060, avg train loss=0.446, avg val loss=0.589, auc=0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=2065, avg train loss=0.277, avg val loss=0.550, auc=0.662\n",
      "Iter=2070, avg train loss=0.393, avg val loss=0.544, auc=0.658\n",
      "Iter=2075, avg train loss=0.366, avg val loss=0.561, auc=0.665\n",
      "Iter=2080, avg train loss=0.292, avg val loss=0.556, auc=0.666\n",
      "Iter=2085, avg train loss=0.388, avg val loss=0.584, auc=0.681\n",
      "Iter=2090, avg train loss=0.265, avg val loss=0.565, auc=0.688\n",
      "Iter=2095, avg train loss=0.362, avg val loss=0.547, auc=0.692\n",
      "Iter=2100, avg train loss=0.345, avg val loss=0.567, auc=0.688\n",
      "Iter=2105, avg train loss=0.383, avg val loss=0.552, auc=0.697\n",
      "Iter=2110, avg train loss=0.510, avg val loss=0.543, auc=0.698\n",
      "Iter=2115, avg train loss=0.421, avg val loss=0.532, auc=0.668\n",
      "Iter=2120, avg train loss=0.314, avg val loss=0.531, auc=0.679\n",
      "Iter=2125, avg train loss=0.249, avg val loss=0.585, auc=0.687\n",
      "Iter=2130, avg train loss=0.336, avg val loss=0.670, auc=0.690\n",
      "Iter=2135, avg train loss=0.298, avg val loss=0.587, auc=0.684\n",
      "Iter=2140, avg train loss=0.478, avg val loss=0.557, auc=0.688\n",
      "Iter=2145, avg train loss=0.400, avg val loss=0.532, auc=0.671\n",
      "Iter=2150, avg train loss=0.285, avg val loss=0.542, auc=0.668\n",
      "Iter=2155, avg train loss=0.293, avg val loss=0.579, auc=0.683\n",
      "Iter=2160, avg train loss=0.468, avg val loss=0.570, auc=0.696\n",
      "Iter=2165, avg train loss=0.391, avg val loss=0.711, auc=0.701\n",
      "Iter=2170, avg train loss=0.278, avg val loss=0.783, auc=0.694\n",
      "Iter=2175, avg train loss=0.455, avg val loss=0.748, auc=0.690\n",
      "Iter=2180, avg train loss=0.220, avg val loss=0.718, auc=0.696\n",
      "Iter=2185, avg train loss=0.500, avg val loss=0.573, auc=0.700\n",
      "Iter=2190, avg train loss=0.339, avg val loss=0.527, auc=0.671\n",
      "Iter=2195, avg train loss=0.390, avg val loss=0.542, auc=0.676\n",
      "Iter=2200, avg train loss=0.239, avg val loss=0.551, auc=0.674\n",
      "Iter=2205, avg train loss=0.293, avg val loss=0.559, auc=0.670\n",
      "Iter=2210, avg train loss=0.282, avg val loss=0.605, auc=0.680\n",
      "Iter=2215, avg train loss=0.368, avg val loss=0.656, auc=0.678\n",
      "Iter=2220, avg train loss=0.196, avg val loss=0.632, auc=0.686\n",
      "Iter=2225, avg train loss=0.285, avg val loss=0.561, auc=0.718\n",
      "Iter=2230, avg train loss=0.259, avg val loss=0.555, auc=0.701\n",
      "Iter=2235, avg train loss=0.359, avg val loss=0.506, auc=0.707\n",
      "Iter=2240, avg train loss=0.371, avg val loss=0.548, auc=0.716\n",
      "Iter=2245, avg train loss=0.414, avg val loss=0.593, auc=0.710\n",
      "Iter=2250, avg train loss=0.314, avg val loss=0.594, auc=0.707\n",
      "Iter=2255, avg train loss=0.340, avg val loss=0.542, auc=0.696\n",
      "Iter=2260, avg train loss=0.346, avg val loss=0.568, auc=0.691\n",
      "Iter=2265, avg train loss=0.261, avg val loss=0.628, auc=0.682\n",
      "Iter=2270, avg train loss=0.404, avg val loss=0.568, auc=0.682\n",
      "Iter=2275, avg train loss=0.367, avg val loss=0.555, auc=0.691\n",
      "Iter=2280, avg train loss=0.315, avg val loss=0.546, auc=0.692\n",
      "Iter=2285, avg train loss=0.431, avg val loss=0.540, auc=0.696\n",
      "Iter=2290, avg train loss=0.293, avg val loss=0.582, auc=0.707\n",
      "Iter=2295, avg train loss=0.320, avg val loss=0.645, auc=0.711\n",
      "Iter=2300, avg train loss=0.313, avg val loss=0.555, auc=0.729\n",
      "Iter=2305, avg train loss=0.239, avg val loss=0.604, auc=0.732\n",
      "Iter=2310, avg train loss=0.336, avg val loss=0.538, auc=0.722\n",
      "Iter=2315, avg train loss=0.399, avg val loss=0.563, auc=0.710\n",
      "Iter=2320, avg train loss=0.301, avg val loss=0.676, auc=0.726\n",
      "Iter=2325, avg train loss=0.422, avg val loss=0.618, auc=0.718\n",
      "Iter=2330, avg train loss=0.304, avg val loss=0.537, auc=0.725\n",
      "Iter=2335, avg train loss=0.264, avg val loss=0.519, auc=0.703\n",
      "Iter=2340, avg train loss=0.270, avg val loss=0.527, auc=0.716\n",
      "Iter=2345, avg train loss=0.384, avg val loss=0.580, auc=0.718\n",
      "Iter=2350, avg train loss=0.372, avg val loss=0.605, auc=0.718\n",
      "Iter=2355, avg train loss=0.330, avg val loss=0.681, auc=0.716\n",
      "Iter=2360, avg train loss=0.462, avg val loss=0.638, auc=0.701\n",
      "Iter=2365, avg train loss=0.352, avg val loss=0.542, auc=0.694\n",
      "Iter=2370, avg train loss=0.213, avg val loss=0.540, auc=0.694\n",
      "Iter=2375, avg train loss=0.302, avg val loss=0.536, auc=0.689\n",
      "Iter=2380, avg train loss=0.284, avg val loss=0.557, auc=0.684\n",
      "Iter=2385, avg train loss=0.347, avg val loss=0.550, auc=0.687\n",
      "Iter=2390, avg train loss=0.314, avg val loss=0.575, auc=0.697\n",
      "Iter=2395, avg train loss=0.255, avg val loss=0.555, auc=0.702\n",
      "Iter=2400, avg train loss=0.385, avg val loss=0.539, auc=0.702\n",
      "Iter=2405, avg train loss=0.398, avg val loss=0.570, auc=0.703\n",
      "Iter=2410, avg train loss=0.265, avg val loss=0.558, auc=0.696\n",
      "Iter=2415, avg train loss=0.243, avg val loss=0.542, auc=0.682\n",
      "Iter=2420, avg train loss=0.266, avg val loss=0.548, auc=0.674\n",
      "Iter=2425, avg train loss=0.352, avg val loss=0.627, auc=0.690\n",
      "Iter=2430, avg train loss=0.259, avg val loss=0.584, auc=0.685\n",
      "Iter=2435, avg train loss=0.306, avg val loss=0.547, auc=0.693\n",
      "Iter=2440, avg train loss=0.258, avg val loss=0.525, auc=0.682\n",
      "Iter=2445, avg train loss=0.269, avg val loss=0.560, auc=0.689\n",
      "Iter=2450, avg train loss=0.229, avg val loss=0.571, auc=0.679\n",
      "Iter=2455, avg train loss=0.214, avg val loss=0.581, auc=0.680\n",
      "Iter=2460, avg train loss=0.227, avg val loss=0.625, auc=0.692\n",
      "Iter=2465, avg train loss=0.310, avg val loss=0.657, auc=0.686\n",
      "Iter=2470, avg train loss=0.342, avg val loss=0.551, auc=0.689\n",
      "Iter=2475, avg train loss=0.331, avg val loss=0.563, auc=0.693\n",
      "Iter=2480, avg train loss=0.280, avg val loss=0.644, auc=0.689\n",
      "Iter=2485, avg train loss=0.292, avg val loss=0.702, auc=0.676\n",
      "Iter=2490, avg train loss=0.412, avg val loss=0.624, auc=0.698\n",
      "Iter=2495, avg train loss=0.209, avg val loss=0.581, auc=0.698\n",
      "Iter=2500, avg train loss=0.319, avg val loss=0.561, auc=0.691\n",
      "Iter=2505, avg train loss=0.255, avg val loss=0.530, auc=0.687\n",
      "Iter=2510, avg train loss=0.224, avg val loss=0.548, auc=0.685\n",
      "Iter=2515, avg train loss=0.314, avg val loss=0.567, auc=0.671\n",
      "Iter=2520, avg train loss=0.259, avg val loss=0.584, auc=0.673\n",
      "Iter=2525, avg train loss=0.210, avg val loss=0.573, auc=0.696\n",
      "Iter=2530, avg train loss=0.244, avg val loss=0.558, auc=0.721\n",
      "Iter=2535, avg train loss=0.282, avg val loss=0.566, auc=0.709\n",
      "Iter=2540, avg train loss=0.221, avg val loss=0.540, auc=0.713\n",
      "Iter=2545, avg train loss=0.225, avg val loss=0.546, auc=0.722\n",
      "Iter=2550, avg train loss=0.218, avg val loss=0.569, auc=0.717\n",
      "Iter=2555, avg train loss=0.202, avg val loss=0.564, auc=0.714\n",
      "Iter=2560, avg train loss=0.283, avg val loss=0.574, auc=0.701\n",
      "Iter=2565, avg train loss=0.268, avg val loss=0.533, auc=0.708\n",
      "Iter=2570, avg train loss=0.166, avg val loss=0.542, auc=0.716\n",
      "Iter=2575, avg train loss=0.191, avg val loss=0.517, auc=0.698\n",
      "Iter=2580, avg train loss=0.359, avg val loss=0.523, auc=0.700\n",
      "Iter=2585, avg train loss=0.229, avg val loss=0.530, auc=0.704\n",
      "Iter=2590, avg train loss=0.278, avg val loss=0.534, auc=0.683\n",
      "Iter=2595, avg train loss=0.230, avg val loss=0.541, auc=0.695\n",
      "Iter=2600, avg train loss=0.237, avg val loss=0.535, auc=0.694\n",
      "Iter=2605, avg train loss=0.263, avg val loss=0.587, auc=0.699\n",
      "Iter=2610, avg train loss=0.266, avg val loss=0.567, auc=0.695\n",
      "Iter=2615, avg train loss=0.333, avg val loss=0.576, auc=0.696\n",
      "Iter=2620, avg train loss=0.214, avg val loss=0.550, auc=0.680\n",
      "Iter=2625, avg train loss=0.316, avg val loss=0.535, auc=0.687\n",
      "Iter=2630, avg train loss=0.246, avg val loss=0.576, auc=0.696\n",
      "Iter=2635, avg train loss=0.331, avg val loss=0.586, auc=0.695\n",
      "Iter=2640, avg train loss=0.270, avg val loss=0.598, auc=0.699\n",
      "Iter=2645, avg train loss=0.185, avg val loss=0.641, auc=0.700\n",
      "Iter=2650, avg train loss=0.221, avg val loss=0.579, auc=0.693\n",
      "Iter=2655, avg train loss=0.217, avg val loss=0.539, auc=0.686\n",
      "Iter=2660, avg train loss=0.235, avg val loss=0.546, auc=0.675\n",
      "Iter=2665, avg train loss=0.415, avg val loss=0.539, auc=0.688\n",
      "Iter=2670, avg train loss=0.423, avg val loss=0.568, auc=0.718\n",
      "Iter=2675, avg train loss=0.295, avg val loss=0.626, auc=0.714\n",
      "Iter=2680, avg train loss=0.290, avg val loss=0.589, auc=0.690\n",
      "Iter=2685, avg train loss=0.261, avg val loss=0.551, auc=0.689\n",
      "Iter=2690, avg train loss=0.275, avg val loss=0.541, auc=0.684\n",
      "Iter=2695, avg train loss=0.255, avg val loss=0.565, auc=0.693\n",
      "Iter=2700, avg train loss=0.202, avg val loss=0.568, auc=0.697\n",
      "Iter=2705, avg train loss=0.327, avg val loss=0.589, auc=0.697\n",
      "Iter=2710, avg train loss=0.282, avg val loss=0.542, auc=0.676\n",
      "Iter=2715, avg train loss=0.246, avg val loss=0.575, auc=0.674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=2720, avg train loss=0.186, avg val loss=0.580, auc=0.666\n",
      "Iter=2725, avg train loss=0.308, avg val loss=0.595, auc=0.686\n",
      "Iter=2730, avg train loss=0.382, avg val loss=0.585, auc=0.686\n",
      "Iter=2735, avg train loss=0.195, avg val loss=0.574, auc=0.693\n",
      "Iter=2740, avg train loss=0.271, avg val loss=0.534, auc=0.690\n",
      "Iter=2745, avg train loss=0.225, avg val loss=0.557, auc=0.700\n",
      "Iter=2750, avg train loss=0.280, avg val loss=0.641, auc=0.703\n",
      "Iter=2755, avg train loss=0.275, avg val loss=0.580, auc=0.695\n",
      "Iter=2760, avg train loss=0.254, avg val loss=0.534, auc=0.680\n",
      "Iter=2765, avg train loss=0.258, avg val loss=0.528, auc=0.694\n",
      "Iter=2770, avg train loss=0.400, avg val loss=0.520, auc=0.692\n",
      "Iter=2775, avg train loss=0.203, avg val loss=0.524, auc=0.708\n",
      "Iter=2780, avg train loss=0.279, avg val loss=0.635, auc=0.725\n",
      "Iter=2785, avg train loss=0.235, avg val loss=0.656, auc=0.724\n",
      "Iter=2790, avg train loss=0.242, avg val loss=0.611, auc=0.724\n",
      "Iter=2795, avg train loss=0.160, avg val loss=0.560, auc=0.723\n",
      "Iter=2800, avg train loss=0.213, avg val loss=0.565, auc=0.715\n",
      "Iter=2805, avg train loss=0.289, avg val loss=0.547, auc=0.699\n",
      "Iter=2810, avg train loss=0.152, avg val loss=0.561, auc=0.694\n",
      "Iter=2815, avg train loss=0.378, avg val loss=0.572, auc=0.670\n",
      "Iter=2820, avg train loss=0.230, avg val loss=0.584, auc=0.664\n",
      "Iter=2825, avg train loss=0.170, avg val loss=0.583, auc=0.690\n",
      "Iter=2830, avg train loss=0.248, avg val loss=0.572, auc=0.692\n",
      "Iter=2835, avg train loss=0.226, avg val loss=0.558, auc=0.697\n",
      "Iter=2840, avg train loss=0.286, avg val loss=0.586, auc=0.707\n",
      "Iter=2845, avg train loss=0.172, avg val loss=0.653, auc=0.698\n",
      "Iter=2850, avg train loss=0.194, avg val loss=0.584, auc=0.684\n",
      "Iter=2855, avg train loss=0.222, avg val loss=0.609, auc=0.698\n",
      "Iter=2860, avg train loss=0.376, avg val loss=0.608, auc=0.707\n",
      "Iter=2865, avg train loss=0.301, avg val loss=0.536, auc=0.705\n",
      "Iter=2870, avg train loss=0.259, avg val loss=0.580, auc=0.699\n",
      "Iter=2875, avg train loss=0.249, avg val loss=0.619, auc=0.697\n",
      "Iter=2880, avg train loss=0.234, avg val loss=0.564, auc=0.686\n",
      "Iter=2885, avg train loss=0.212, avg val loss=0.583, auc=0.674\n",
      "Iter=2890, avg train loss=0.301, avg val loss=0.531, auc=0.695\n",
      "Iter=2895, avg train loss=0.247, avg val loss=0.532, auc=0.693\n",
      "Iter=2900, avg train loss=0.250, avg val loss=0.515, auc=0.695\n",
      "Iter=2905, avg train loss=0.271, avg val loss=0.521, auc=0.691\n",
      "Iter=2910, avg train loss=0.424, avg val loss=0.608, auc=0.696\n",
      "Iter=2915, avg train loss=0.334, avg val loss=0.620, auc=0.699\n",
      "Iter=2920, avg train loss=0.229, avg val loss=0.541, auc=0.688\n",
      "Iter=2925, avg train loss=0.197, avg val loss=0.549, auc=0.697\n",
      "Iter=2930, avg train loss=0.198, avg val loss=0.535, auc=0.693\n",
      "Iter=2935, avg train loss=0.307, avg val loss=0.572, auc=0.693\n",
      "Iter=2940, avg train loss=0.242, avg val loss=0.563, auc=0.692\n",
      "Iter=2945, avg train loss=0.183, avg val loss=0.562, auc=0.673\n",
      "Iter=2950, avg train loss=0.194, avg val loss=0.569, auc=0.673\n",
      "Iter=2955, avg train loss=0.174, avg val loss=0.592, auc=0.668\n",
      "Iter=2960, avg train loss=0.190, avg val loss=0.633, auc=0.680\n",
      "Iter=2965, avg train loss=0.215, avg val loss=0.548, auc=0.692\n",
      "Iter=2970, avg train loss=0.208, avg val loss=0.558, auc=0.688\n",
      "Iter=2975, avg train loss=0.131, avg val loss=0.597, auc=0.706\n",
      "Iter=2980, avg train loss=0.216, avg val loss=0.560, auc=0.700\n",
      "Iter=2985, avg train loss=0.279, avg val loss=0.585, auc=0.702\n",
      "Iter=2990, avg train loss=0.238, avg val loss=0.593, auc=0.688\n",
      "Iter=2995, avg train loss=0.329, avg val loss=0.532, auc=0.689\n",
      "Iter=3000, avg train loss=0.183, avg val loss=0.559, auc=0.682\n",
      "Iter=3005, avg train loss=0.331, avg val loss=0.560, auc=0.684\n",
      "Iter=3010, avg train loss=0.213, avg val loss=0.581, auc=0.695\n",
      "Iter=3015, avg train loss=0.183, avg val loss=0.606, auc=0.692\n",
      "Iter=3020, avg train loss=0.254, avg val loss=0.569, auc=0.689\n",
      "Iter=3025, avg train loss=0.300, avg val loss=0.547, auc=0.685\n",
      "Iter=3030, avg train loss=0.195, avg val loss=0.562, auc=0.679\n",
      "Iter=3035, avg train loss=0.209, avg val loss=0.602, auc=0.686\n",
      "Iter=3040, avg train loss=0.270, avg val loss=0.598, auc=0.674\n",
      "Iter=3045, avg train loss=0.192, avg val loss=0.638, auc=0.676\n",
      "Iter=3050, avg train loss=0.139, avg val loss=0.685, auc=0.662\n",
      "Iter=3055, avg train loss=0.229, avg val loss=0.569, auc=0.671\n",
      "Iter=3060, avg train loss=0.182, avg val loss=0.578, auc=0.679\n",
      "Iter=3065, avg train loss=0.184, avg val loss=0.586, auc=0.678\n",
      "Iter=3070, avg train loss=0.234, avg val loss=0.577, auc=0.669\n",
      "Iter=3075, avg train loss=0.212, avg val loss=0.601, auc=0.673\n",
      "Iter=3080, avg train loss=0.171, avg val loss=0.587, auc=0.671\n",
      "Iter=3085, avg train loss=0.244, avg val loss=0.582, auc=0.682\n",
      "Iter=3090, avg train loss=0.319, avg val loss=0.628, auc=0.697\n",
      "Iter=3095, avg train loss=0.151, avg val loss=0.747, auc=0.711\n",
      "Iter=3100, avg train loss=0.227, avg val loss=0.767, auc=0.684\n",
      "Iter=3105, avg train loss=0.239, avg val loss=0.659, auc=0.680\n",
      "Iter=3110, avg train loss=0.251, avg val loss=0.627, auc=0.677\n",
      "Iter=3115, avg train loss=0.316, avg val loss=0.614, auc=0.677\n",
      "Iter=3120, avg train loss=0.166, avg val loss=0.575, auc=0.714\n",
      "Iter=3125, avg train loss=0.178, avg val loss=0.756, auc=0.705\n",
      "Iter=3130, avg train loss=0.204, avg val loss=0.609, auc=0.703\n",
      "Iter=3135, avg train loss=0.252, avg val loss=0.550, auc=0.705\n",
      "Iter=3140, avg train loss=0.150, avg val loss=0.524, auc=0.703\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.642, max-score-based AUC=0.646\n",
      "\n",
      "\n",
      "========== Fold 3 ==========\n",
      "Test AUC at start=0.653, max-score-based AUC=0.674\n",
      "Iter=5, avg train loss=0.782, avg val loss=0.549, auc=0.612\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.851, avg val loss=0.555, auc=0.604\n",
      "Iter=15, avg train loss=0.853, avg val loss=0.570, auc=0.599\n",
      "Iter=20, avg train loss=0.769, avg val loss=0.552, auc=0.611\n",
      "Iter=25, avg train loss=0.852, avg val loss=0.552, auc=0.601\n",
      "Iter=30, avg train loss=0.725, avg val loss=0.558, auc=0.595\n",
      "Iter=35, avg train loss=0.583, avg val loss=0.559, auc=0.592\n",
      "Iter=40, avg train loss=0.791, avg val loss=0.563, auc=0.598\n",
      "Iter=45, avg train loss=0.689, avg val loss=0.581, auc=0.609\n",
      "Iter=50, avg train loss=0.757, avg val loss=0.598, auc=0.615\n",
      "Best model saved.\n",
      "Iter=55, avg train loss=0.668, avg val loss=0.588, auc=0.612\n",
      "Iter=60, avg train loss=0.689, avg val loss=0.589, auc=0.602\n",
      "Iter=65, avg train loss=0.672, avg val loss=0.582, auc=0.608\n",
      "Iter=70, avg train loss=0.631, avg val loss=0.578, auc=0.606\n",
      "Iter=75, avg train loss=0.658, avg val loss=0.602, auc=0.592\n",
      "Iter=80, avg train loss=0.692, avg val loss=0.611, auc=0.599\n",
      "Iter=85, avg train loss=0.704, avg val loss=0.621, auc=0.604\n",
      "Iter=90, avg train loss=0.681, avg val loss=0.624, auc=0.606\n",
      "Iter=95, avg train loss=0.624, avg val loss=0.580, auc=0.616\n",
      "Best model saved.\n",
      "Iter=100, avg train loss=0.655, avg val loss=0.608, auc=0.616\n",
      "Best model saved.\n",
      "Iter=105, avg train loss=0.699, avg val loss=0.603, auc=0.617\n",
      "Best model saved.\n",
      "Iter=110, avg train loss=0.680, avg val loss=0.635, auc=0.610\n",
      "Iter=115, avg train loss=0.669, avg val loss=0.604, auc=0.608\n",
      "Iter=120, avg train loss=0.645, avg val loss=0.626, auc=0.603\n",
      "Iter=125, avg train loss=0.621, avg val loss=0.590, auc=0.610\n",
      "Iter=130, avg train loss=0.679, avg val loss=0.579, auc=0.618\n",
      "Best model saved.\n",
      "Iter=135, avg train loss=0.640, avg val loss=0.567, auc=0.616\n",
      "Iter=140, avg train loss=0.622, avg val loss=0.596, auc=0.612\n",
      "Iter=145, avg train loss=0.710, avg val loss=0.585, auc=0.612\n",
      "Iter=150, avg train loss=0.701, avg val loss=0.569, auc=0.614\n",
      "Iter=155, avg train loss=0.651, avg val loss=0.599, auc=0.613\n",
      "Iter=160, avg train loss=0.673, avg val loss=0.589, auc=0.616\n",
      "Iter=165, avg train loss=0.631, avg val loss=0.614, auc=0.631\n",
      "Best model saved.\n",
      "Iter=170, avg train loss=0.684, avg val loss=0.579, auc=0.645\n",
      "Best model saved.\n",
      "Iter=175, avg train loss=0.585, avg val loss=0.569, auc=0.648\n",
      "Best model saved.\n",
      "Iter=180, avg train loss=0.698, avg val loss=0.575, auc=0.648\n",
      "Iter=185, avg train loss=0.673, avg val loss=0.570, auc=0.645\n",
      "Iter=190, avg train loss=0.638, avg val loss=0.579, auc=0.648\n",
      "Iter=195, avg train loss=0.622, avg val loss=0.554, auc=0.660\n",
      "Best model saved.\n",
      "Iter=200, avg train loss=0.650, avg val loss=0.554, auc=0.659\n",
      "Iter=205, avg train loss=0.625, avg val loss=0.556, auc=0.661\n",
      "Best model saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=210, avg train loss=0.672, avg val loss=0.560, auc=0.660\n",
      "Iter=215, avg train loss=0.632, avg val loss=0.581, auc=0.646\n",
      "Iter=220, avg train loss=0.589, avg val loss=0.592, auc=0.645\n",
      "Iter=225, avg train loss=0.743, avg val loss=0.566, auc=0.651\n",
      "Iter=230, avg train loss=0.703, avg val loss=0.576, auc=0.651\n",
      "Iter=235, avg train loss=0.695, avg val loss=0.586, auc=0.647\n",
      "Iter=240, avg train loss=0.704, avg val loss=0.610, auc=0.636\n",
      "Iter=245, avg train loss=0.659, avg val loss=0.664, auc=0.605\n",
      "Iter=250, avg train loss=0.630, avg val loss=0.612, auc=0.610\n",
      "Iter=255, avg train loss=0.644, avg val loss=0.610, auc=0.627\n",
      "Iter=260, avg train loss=0.583, avg val loss=0.610, auc=0.613\n",
      "Iter=265, avg train loss=0.621, avg val loss=0.580, auc=0.636\n",
      "Iter=270, avg train loss=0.635, avg val loss=0.568, auc=0.642\n",
      "Iter=275, avg train loss=0.580, avg val loss=0.552, auc=0.649\n",
      "Iter=280, avg train loss=0.685, avg val loss=0.558, auc=0.627\n",
      "Iter=285, avg train loss=0.622, avg val loss=0.560, auc=0.655\n",
      "Iter=290, avg train loss=0.728, avg val loss=0.577, auc=0.651\n",
      "Iter=295, avg train loss=0.651, avg val loss=0.640, auc=0.635\n",
      "Iter=300, avg train loss=0.666, avg val loss=0.651, auc=0.643\n",
      "Iter=305, avg train loss=0.626, avg val loss=0.588, auc=0.664\n",
      "Best model saved.\n",
      "Iter=310, avg train loss=0.638, avg val loss=0.629, auc=0.648\n",
      "Iter=315, avg train loss=0.631, avg val loss=0.580, auc=0.655\n",
      "Iter=320, avg train loss=0.605, avg val loss=0.555, auc=0.644\n",
      "Iter=325, avg train loss=0.661, avg val loss=0.556, auc=0.640\n",
      "Iter=330, avg train loss=0.691, avg val loss=0.558, auc=0.650\n",
      "Iter=335, avg train loss=0.632, avg val loss=0.552, auc=0.640\n",
      "Iter=340, avg train loss=0.537, avg val loss=0.567, auc=0.635\n",
      "Iter=345, avg train loss=0.715, avg val loss=0.554, auc=0.641\n",
      "Iter=350, avg train loss=0.491, avg val loss=0.571, auc=0.646\n",
      "Iter=355, avg train loss=0.626, avg val loss=0.564, auc=0.643\n",
      "Iter=360, avg train loss=0.670, avg val loss=0.548, auc=0.631\n",
      "Iter=365, avg train loss=0.635, avg val loss=0.556, auc=0.630\n",
      "Iter=370, avg train loss=0.615, avg val loss=0.572, auc=0.623\n",
      "Iter=375, avg train loss=0.574, avg val loss=0.569, auc=0.636\n",
      "Iter=380, avg train loss=0.696, avg val loss=0.560, auc=0.641\n",
      "Iter=385, avg train loss=0.671, avg val loss=0.582, auc=0.637\n",
      "Iter=390, avg train loss=0.609, avg val loss=0.595, auc=0.649\n",
      "Iter=395, avg train loss=0.591, avg val loss=0.588, auc=0.658\n",
      "Iter=400, avg train loss=0.658, avg val loss=0.616, auc=0.670\n",
      "Best model saved.\n",
      "Iter=405, avg train loss=0.622, avg val loss=0.605, auc=0.667\n",
      "Iter=410, avg train loss=0.633, avg val loss=0.590, auc=0.672\n",
      "Best model saved.\n",
      "Iter=415, avg train loss=0.631, avg val loss=0.575, auc=0.677\n",
      "Best model saved.\n",
      "Iter=420, avg train loss=0.576, avg val loss=0.595, auc=0.675\n",
      "Iter=425, avg train loss=0.582, avg val loss=0.558, auc=0.668\n",
      "Iter=430, avg train loss=0.509, avg val loss=0.557, auc=0.677\n",
      "Iter=435, avg train loss=0.597, avg val loss=0.569, auc=0.668\n",
      "Iter=440, avg train loss=0.548, avg val loss=0.586, auc=0.660\n",
      "Iter=445, avg train loss=0.614, avg val loss=0.672, auc=0.654\n",
      "Iter=450, avg train loss=0.691, avg val loss=0.594, auc=0.655\n",
      "Iter=455, avg train loss=0.596, avg val loss=0.585, auc=0.652\n",
      "Iter=460, avg train loss=0.630, avg val loss=0.602, auc=0.658\n",
      "Iter=465, avg train loss=0.587, avg val loss=0.636, auc=0.653\n",
      "Iter=470, avg train loss=0.693, avg val loss=0.562, auc=0.654\n",
      "Iter=475, avg train loss=0.650, avg val loss=0.575, auc=0.658\n",
      "Iter=480, avg train loss=0.687, avg val loss=0.587, auc=0.649\n",
      "Iter=485, avg train loss=0.503, avg val loss=0.602, auc=0.646\n",
      "Iter=490, avg train loss=0.645, avg val loss=0.601, auc=0.643\n",
      "Iter=495, avg train loss=0.664, avg val loss=0.625, auc=0.650\n",
      "Iter=500, avg train loss=0.586, avg val loss=0.646, auc=0.645\n",
      "Iter=505, avg train loss=0.638, avg val loss=0.587, auc=0.646\n",
      "Iter=510, avg train loss=0.620, avg val loss=0.576, auc=0.643\n",
      "Iter=515, avg train loss=0.626, avg val loss=0.550, auc=0.659\n",
      "Iter=520, avg train loss=0.526, avg val loss=0.546, auc=0.658\n",
      "Iter=525, avg train loss=0.585, avg val loss=0.548, auc=0.636\n",
      "Iter=530, avg train loss=0.603, avg val loss=0.543, auc=0.632\n",
      "Iter=535, avg train loss=0.683, avg val loss=0.568, auc=0.636\n",
      "Iter=540, avg train loss=0.570, avg val loss=0.564, auc=0.636\n",
      "Iter=545, avg train loss=0.604, avg val loss=0.565, auc=0.633\n",
      "Iter=550, avg train loss=0.599, avg val loss=0.584, auc=0.632\n",
      "Iter=555, avg train loss=0.570, avg val loss=0.589, auc=0.610\n",
      "Iter=560, avg train loss=0.620, avg val loss=0.590, auc=0.630\n",
      "Iter=565, avg train loss=0.598, avg val loss=0.586, auc=0.631\n",
      "Iter=570, avg train loss=0.543, avg val loss=0.578, auc=0.635\n",
      "Iter=575, avg train loss=0.604, avg val loss=0.598, auc=0.637\n",
      "Iter=580, avg train loss=0.546, avg val loss=0.583, auc=0.646\n",
      "Iter=585, avg train loss=0.507, avg val loss=0.579, auc=0.654\n",
      "Iter=590, avg train loss=0.656, avg val loss=0.573, auc=0.660\n",
      "Iter=595, avg train loss=0.583, avg val loss=0.559, auc=0.664\n",
      "Iter=600, avg train loss=0.534, avg val loss=0.568, auc=0.661\n",
      "Iter=605, avg train loss=0.665, avg val loss=0.547, auc=0.668\n",
      "Iter=610, avg train loss=0.522, avg val loss=0.551, auc=0.661\n",
      "Iter=615, avg train loss=0.577, avg val loss=0.544, auc=0.651\n",
      "Iter=620, avg train loss=0.601, avg val loss=0.562, auc=0.667\n",
      "Iter=625, avg train loss=0.754, avg val loss=0.565, auc=0.661\n",
      "Iter=630, avg train loss=0.521, avg val loss=0.556, auc=0.661\n",
      "Iter=635, avg train loss=0.478, avg val loss=0.539, auc=0.652\n",
      "Iter=640, avg train loss=0.537, avg val loss=0.543, auc=0.650\n",
      "Iter=645, avg train loss=0.575, avg val loss=0.543, auc=0.652\n",
      "Iter=650, avg train loss=0.563, avg val loss=0.555, auc=0.665\n",
      "Iter=655, avg train loss=0.548, avg val loss=0.565, auc=0.670\n",
      "Iter=660, avg train loss=0.626, avg val loss=0.656, auc=0.678\n",
      "Best model saved.\n",
      "Iter=665, avg train loss=0.616, avg val loss=0.623, auc=0.676\n",
      "Iter=670, avg train loss=0.659, avg val loss=0.570, auc=0.672\n",
      "Iter=675, avg train loss=0.593, avg val loss=0.586, auc=0.685\n",
      "Best model saved.\n",
      "Iter=680, avg train loss=0.500, avg val loss=0.648, auc=0.683\n",
      "Iter=685, avg train loss=0.585, avg val loss=0.566, auc=0.685\n",
      "Iter=690, avg train loss=0.590, avg val loss=0.550, auc=0.679\n",
      "Iter=695, avg train loss=0.530, avg val loss=0.558, auc=0.679\n",
      "Iter=700, avg train loss=0.640, avg val loss=0.588, auc=0.672\n",
      "Iter=705, avg train loss=0.537, avg val loss=0.558, auc=0.674\n",
      "Iter=710, avg train loss=0.515, avg val loss=0.554, auc=0.661\n",
      "Iter=715, avg train loss=0.563, avg val loss=0.565, auc=0.657\n",
      "Iter=720, avg train loss=0.662, avg val loss=0.560, auc=0.659\n",
      "Iter=725, avg train loss=0.535, avg val loss=0.565, auc=0.656\n",
      "Iter=730, avg train loss=0.546, avg val loss=0.584, auc=0.653\n",
      "Iter=735, avg train loss=0.508, avg val loss=0.575, auc=0.635\n",
      "Iter=740, avg train loss=0.619, avg val loss=0.584, auc=0.640\n",
      "Iter=745, avg train loss=0.527, avg val loss=0.619, auc=0.635\n",
      "Iter=750, avg train loss=0.578, avg val loss=0.654, auc=0.639\n",
      "Iter=755, avg train loss=0.625, avg val loss=0.630, auc=0.633\n",
      "Iter=760, avg train loss=0.575, avg val loss=0.594, auc=0.637\n",
      "Iter=765, avg train loss=0.485, avg val loss=0.578, auc=0.638\n",
      "Iter=770, avg train loss=0.618, avg val loss=0.577, auc=0.632\n",
      "Iter=775, avg train loss=0.524, avg val loss=0.564, auc=0.612\n",
      "Iter=780, avg train loss=0.505, avg val loss=0.550, auc=0.626\n",
      "Iter=785, avg train loss=0.536, avg val loss=0.584, auc=0.623\n",
      "Iter=790, avg train loss=0.535, avg val loss=0.600, auc=0.624\n",
      "Iter=795, avg train loss=0.530, avg val loss=0.635, auc=0.625\n",
      "Iter=800, avg train loss=0.620, avg val loss=0.603, auc=0.614\n",
      "Iter=805, avg train loss=0.528, avg val loss=0.581, auc=0.616\n",
      "Iter=810, avg train loss=0.558, avg val loss=0.596, auc=0.611\n",
      "Iter=815, avg train loss=0.713, avg val loss=0.583, auc=0.603\n",
      "Iter=820, avg train loss=0.506, avg val loss=0.597, auc=0.620\n",
      "Iter=825, avg train loss=0.471, avg val loss=0.588, auc=0.626\n",
      "Iter=830, avg train loss=0.610, avg val loss=0.609, auc=0.624\n",
      "Iter=835, avg train loss=0.524, avg val loss=0.613, auc=0.641\n",
      "Iter=840, avg train loss=0.489, avg val loss=0.577, auc=0.628\n",
      "Iter=845, avg train loss=0.584, avg val loss=0.580, auc=0.625\n",
      "Iter=850, avg train loss=0.553, avg val loss=0.611, auc=0.624\n",
      "Iter=855, avg train loss=0.518, avg val loss=0.621, auc=0.617\n",
      "Iter=860, avg train loss=0.627, avg val loss=0.615, auc=0.616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=865, avg train loss=0.661, avg val loss=0.581, auc=0.636\n",
      "Iter=870, avg train loss=0.527, avg val loss=0.615, auc=0.633\n",
      "Iter=875, avg train loss=0.493, avg val loss=0.618, auc=0.628\n",
      "Iter=880, avg train loss=0.477, avg val loss=0.590, auc=0.618\n",
      "Iter=885, avg train loss=0.562, avg val loss=0.579, auc=0.623\n",
      "Iter=890, avg train loss=0.653, avg val loss=0.576, auc=0.619\n",
      "Iter=895, avg train loss=0.545, avg val loss=0.650, auc=0.642\n",
      "Iter=900, avg train loss=0.492, avg val loss=0.710, auc=0.645\n",
      "Iter=905, avg train loss=0.536, avg val loss=0.734, auc=0.657\n",
      "Iter=910, avg train loss=0.594, avg val loss=0.649, auc=0.667\n",
      "Iter=915, avg train loss=0.559, avg val loss=0.704, auc=0.671\n",
      "Iter=920, avg train loss=0.544, avg val loss=0.652, auc=0.655\n",
      "Iter=925, avg train loss=0.607, avg val loss=0.590, auc=0.651\n",
      "Iter=930, avg train loss=0.605, avg val loss=0.565, auc=0.647\n",
      "Iter=935, avg train loss=0.586, avg val loss=0.570, auc=0.651\n",
      "Iter=940, avg train loss=0.516, avg val loss=0.629, auc=0.652\n",
      "Iter=945, avg train loss=0.531, avg val loss=0.639, auc=0.650\n",
      "Iter=950, avg train loss=0.584, avg val loss=0.603, auc=0.648\n",
      "Iter=955, avg train loss=0.554, avg val loss=0.573, auc=0.636\n",
      "Iter=960, avg train loss=0.557, avg val loss=0.566, auc=0.632\n",
      "Iter=965, avg train loss=0.586, avg val loss=0.584, auc=0.626\n",
      "Iter=970, avg train loss=0.416, avg val loss=0.573, auc=0.621\n",
      "Iter=975, avg train loss=0.639, avg val loss=0.585, auc=0.616\n",
      "Iter=980, avg train loss=0.523, avg val loss=0.628, auc=0.623\n",
      "Iter=985, avg train loss=0.455, avg val loss=0.677, auc=0.629\n",
      "Iter=990, avg train loss=0.435, avg val loss=0.755, auc=0.624\n",
      "Iter=995, avg train loss=0.397, avg val loss=0.673, auc=0.628\n",
      "Iter=1000, avg train loss=0.732, avg val loss=0.657, auc=0.642\n",
      "Iter=1005, avg train loss=0.521, avg val loss=0.609, auc=0.639\n",
      "Iter=1010, avg train loss=0.545, avg val loss=0.644, auc=0.639\n",
      "Iter=1015, avg train loss=0.477, avg val loss=0.652, auc=0.635\n",
      "Iter=1020, avg train loss=0.590, avg val loss=0.606, auc=0.618\n",
      "Iter=1025, avg train loss=0.517, avg val loss=0.606, auc=0.616\n",
      "Iter=1030, avg train loss=0.584, avg val loss=0.598, auc=0.608\n",
      "Iter=1035, avg train loss=0.546, avg val loss=0.613, auc=0.616\n",
      "Iter=1040, avg train loss=0.559, avg val loss=0.629, auc=0.614\n",
      "Iter=1045, avg train loss=0.439, avg val loss=0.633, auc=0.614\n",
      "Iter=1050, avg train loss=0.595, avg val loss=0.624, auc=0.628\n",
      "Iter=1055, avg train loss=0.500, avg val loss=0.622, auc=0.621\n",
      "Iter=1060, avg train loss=0.606, avg val loss=0.621, auc=0.614\n",
      "Iter=1065, avg train loss=0.627, avg val loss=0.624, auc=0.616\n",
      "Iter=1070, avg train loss=0.480, avg val loss=0.647, auc=0.631\n",
      "Iter=1075, avg train loss=0.494, avg val loss=0.654, auc=0.628\n",
      "Iter=1080, avg train loss=0.547, avg val loss=0.603, auc=0.629\n",
      "Iter=1085, avg train loss=0.747, avg val loss=0.670, auc=0.642\n",
      "Iter=1090, avg train loss=0.565, avg val loss=0.770, auc=0.646\n",
      "Iter=1095, avg train loss=0.493, avg val loss=0.684, auc=0.644\n",
      "Iter=1100, avg train loss=0.558, avg val loss=0.642, auc=0.646\n",
      "Iter=1105, avg train loss=0.577, avg val loss=0.621, auc=0.652\n",
      "Iter=1110, avg train loss=0.525, avg val loss=0.588, auc=0.650\n",
      "Iter=1115, avg train loss=0.391, avg val loss=0.612, auc=0.652\n",
      "Iter=1120, avg train loss=0.544, avg val loss=0.619, auc=0.654\n",
      "Iter=1125, avg train loss=0.541, avg val loss=0.627, auc=0.644\n",
      "Iter=1130, avg train loss=0.578, avg val loss=0.583, auc=0.640\n",
      "Iter=1135, avg train loss=0.536, avg val loss=0.596, auc=0.628\n",
      "Iter=1140, avg train loss=0.588, avg val loss=0.590, auc=0.625\n",
      "Iter=1145, avg train loss=0.531, avg val loss=0.638, auc=0.636\n",
      "Iter=1150, avg train loss=0.464, avg val loss=0.630, auc=0.649\n",
      "Iter=1155, avg train loss=0.641, avg val loss=0.615, auc=0.653\n",
      "Iter=1160, avg train loss=0.618, avg val loss=0.577, auc=0.650\n",
      "Iter=1165, avg train loss=0.587, avg val loss=0.575, auc=0.652\n",
      "Iter=1170, avg train loss=0.520, avg val loss=0.568, auc=0.653\n",
      "Iter=1175, avg train loss=0.491, avg val loss=0.607, auc=0.653\n",
      "Iter=1180, avg train loss=0.607, avg val loss=0.640, auc=0.652\n",
      "Iter=1185, avg train loss=0.503, avg val loss=0.648, auc=0.648\n",
      "Iter=1190, avg train loss=0.595, avg val loss=0.611, auc=0.643\n",
      "Iter=1195, avg train loss=0.584, avg val loss=0.628, auc=0.634\n",
      "Iter=1200, avg train loss=0.496, avg val loss=0.633, auc=0.638\n",
      "Iter=1205, avg train loss=0.481, avg val loss=0.627, auc=0.648\n",
      "Iter=1210, avg train loss=0.583, avg val loss=0.622, auc=0.652\n",
      "Iter=1215, avg train loss=0.531, avg val loss=0.632, auc=0.659\n",
      "Iter=1220, avg train loss=0.514, avg val loss=0.621, auc=0.653\n",
      "Iter=1225, avg train loss=0.417, avg val loss=0.616, auc=0.656\n",
      "Iter=1230, avg train loss=0.559, avg val loss=0.603, auc=0.648\n",
      "Iter=1235, avg train loss=0.653, avg val loss=0.604, auc=0.642\n",
      "Iter=1240, avg train loss=0.441, avg val loss=0.607, auc=0.651\n",
      "Iter=1245, avg train loss=0.528, avg val loss=0.719, auc=0.657\n",
      "Iter=1250, avg train loss=0.569, avg val loss=0.649, auc=0.651\n",
      "Iter=1255, avg train loss=0.397, avg val loss=0.620, auc=0.649\n",
      "Iter=1260, avg train loss=0.419, avg val loss=0.636, auc=0.658\n",
      "Iter=1265, avg train loss=0.557, avg val loss=0.623, auc=0.650\n",
      "Iter=1270, avg train loss=0.448, avg val loss=0.621, auc=0.649\n",
      "Iter=1275, avg train loss=0.436, avg val loss=0.649, auc=0.648\n",
      "Iter=1280, avg train loss=0.635, avg val loss=0.649, auc=0.646\n",
      "Iter=1285, avg train loss=0.487, avg val loss=0.603, auc=0.624\n",
      "Iter=1290, avg train loss=0.457, avg val loss=0.580, auc=0.615\n",
      "Iter=1295, avg train loss=0.422, avg val loss=0.589, auc=0.616\n",
      "Iter=1300, avg train loss=0.517, avg val loss=0.581, auc=0.626\n",
      "Iter=1305, avg train loss=0.384, avg val loss=0.591, auc=0.634\n",
      "Iter=1310, avg train loss=0.571, avg val loss=0.692, auc=0.629\n",
      "Iter=1315, avg train loss=0.474, avg val loss=0.702, auc=0.636\n",
      "Iter=1320, avg train loss=0.473, avg val loss=0.646, auc=0.642\n",
      "Iter=1325, avg train loss=0.527, avg val loss=0.724, auc=0.639\n",
      "Iter=1330, avg train loss=0.403, avg val loss=0.663, auc=0.652\n",
      "Iter=1335, avg train loss=0.570, avg val loss=0.645, auc=0.648\n",
      "Iter=1340, avg train loss=0.383, avg val loss=0.579, auc=0.640\n",
      "Iter=1345, avg train loss=0.357, avg val loss=0.624, auc=0.644\n",
      "Iter=1350, avg train loss=0.495, avg val loss=0.582, auc=0.644\n",
      "Iter=1355, avg train loss=0.489, avg val loss=0.591, auc=0.630\n",
      "Iter=1360, avg train loss=0.531, avg val loss=0.653, auc=0.630\n",
      "Iter=1365, avg train loss=0.433, avg val loss=0.727, auc=0.624\n",
      "Iter=1370, avg train loss=0.395, avg val loss=0.848, auc=0.618\n",
      "Iter=1375, avg train loss=0.529, avg val loss=0.758, auc=0.615\n",
      "Iter=1380, avg train loss=0.565, avg val loss=0.620, auc=0.617\n",
      "Iter=1385, avg train loss=0.497, avg val loss=0.604, auc=0.610\n",
      "Iter=1390, avg train loss=0.515, avg val loss=0.586, auc=0.630\n",
      "Iter=1395, avg train loss=0.468, avg val loss=0.645, auc=0.617\n",
      "Iter=1400, avg train loss=0.424, avg val loss=0.773, auc=0.600\n",
      "Iter=1405, avg train loss=0.539, avg val loss=0.822, auc=0.615\n",
      "Iter=1410, avg train loss=0.533, avg val loss=0.661, auc=0.629\n",
      "Iter=1415, avg train loss=0.561, avg val loss=0.630, auc=0.626\n",
      "Iter=1420, avg train loss=0.462, avg val loss=0.610, auc=0.620\n",
      "Iter=1425, avg train loss=0.601, avg val loss=0.590, auc=0.620\n",
      "Iter=1430, avg train loss=0.517, avg val loss=0.602, auc=0.628\n",
      "Iter=1435, avg train loss=0.546, avg val loss=0.731, auc=0.637\n",
      "Iter=1440, avg train loss=0.472, avg val loss=0.709, auc=0.637\n",
      "Iter=1445, avg train loss=0.462, avg val loss=0.857, auc=0.653\n",
      "Iter=1450, avg train loss=0.520, avg val loss=0.609, auc=0.658\n",
      "Iter=1455, avg train loss=0.521, avg val loss=0.586, auc=0.647\n",
      "Iter=1460, avg train loss=0.458, avg val loss=0.599, auc=0.641\n",
      "Iter=1465, avg train loss=0.484, avg val loss=0.589, auc=0.655\n",
      "Iter=1470, avg train loss=0.450, avg val loss=0.555, auc=0.645\n",
      "Iter=1475, avg train loss=0.565, avg val loss=0.575, auc=0.642\n",
      "Iter=1480, avg train loss=0.300, avg val loss=0.604, auc=0.632\n",
      "Iter=1485, avg train loss=0.493, avg val loss=0.594, auc=0.637\n",
      "Iter=1490, avg train loss=0.519, avg val loss=0.587, auc=0.651\n",
      "Iter=1495, avg train loss=0.483, avg val loss=0.654, auc=0.663\n",
      "Iter=1500, avg train loss=0.655, avg val loss=0.615, auc=0.672\n",
      "Iter=1505, avg train loss=0.483, avg val loss=0.617, auc=0.659\n",
      "Iter=1510, avg train loss=0.596, avg val loss=0.588, auc=0.650\n",
      "Iter=1515, avg train loss=0.511, avg val loss=0.580, auc=0.646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1520, avg train loss=0.495, avg val loss=0.570, auc=0.643\n",
      "Iter=1525, avg train loss=0.434, avg val loss=0.586, auc=0.652\n",
      "Iter=1530, avg train loss=0.327, avg val loss=0.623, auc=0.652\n",
      "Iter=1535, avg train loss=0.408, avg val loss=0.611, auc=0.648\n",
      "Iter=1540, avg train loss=0.518, avg val loss=0.570, auc=0.656\n",
      "Iter=1545, avg train loss=0.580, avg val loss=0.601, auc=0.664\n",
      "Iter=1550, avg train loss=0.502, avg val loss=0.579, auc=0.659\n",
      "Iter=1555, avg train loss=0.437, avg val loss=0.568, auc=0.652\n",
      "Iter=1560, avg train loss=0.466, avg val loss=0.631, auc=0.647\n",
      "Iter=1565, avg train loss=0.562, avg val loss=0.608, auc=0.645\n",
      "Iter=1570, avg train loss=0.406, avg val loss=0.619, auc=0.632\n",
      "Iter=1575, avg train loss=0.417, avg val loss=0.592, auc=0.634\n",
      "Iter=1580, avg train loss=0.545, avg val loss=0.662, auc=0.618\n",
      "Iter=1585, avg train loss=0.438, avg val loss=0.596, auc=0.608\n",
      "Iter=1590, avg train loss=0.466, avg val loss=0.640, auc=0.606\n",
      "Iter=1595, avg train loss=0.452, avg val loss=0.634, auc=0.612\n",
      "Iter=1600, avg train loss=0.464, avg val loss=0.732, auc=0.608\n",
      "Iter=1605, avg train loss=0.450, avg val loss=0.868, auc=0.610\n",
      "Iter=1610, avg train loss=0.407, avg val loss=0.709, auc=0.622\n",
      "Iter=1615, avg train loss=0.409, avg val loss=0.667, auc=0.623\n",
      "Iter=1620, avg train loss=0.435, avg val loss=0.656, auc=0.620\n",
      "Iter=1625, avg train loss=0.368, avg val loss=0.627, auc=0.606\n",
      "Iter=1630, avg train loss=0.500, avg val loss=0.641, auc=0.618\n",
      "Iter=1635, avg train loss=0.669, avg val loss=0.737, auc=0.626\n",
      "Iter=1640, avg train loss=0.438, avg val loss=0.656, auc=0.628\n",
      "Iter=1645, avg train loss=0.490, avg val loss=0.645, auc=0.645\n",
      "Iter=1650, avg train loss=0.351, avg val loss=0.635, auc=0.637\n",
      "Iter=1655, avg train loss=0.552, avg val loss=0.600, auc=0.627\n",
      "Iter=1660, avg train loss=0.377, avg val loss=0.586, auc=0.613\n",
      "Iter=1665, avg train loss=0.460, avg val loss=0.621, auc=0.621\n",
      "Iter=1670, avg train loss=0.589, avg val loss=0.821, auc=0.622\n",
      "Iter=1675, avg train loss=0.477, avg val loss=0.744, auc=0.631\n",
      "Iter=1680, avg train loss=0.487, avg val loss=0.611, auc=0.633\n",
      "Iter=1685, avg train loss=0.470, avg val loss=0.631, auc=0.624\n",
      "Iter=1690, avg train loss=0.463, avg val loss=0.679, auc=0.632\n",
      "Iter=1695, avg train loss=0.496, avg val loss=0.614, auc=0.637\n",
      "Iter=1700, avg train loss=0.476, avg val loss=0.621, auc=0.620\n",
      "Iter=1705, avg train loss=0.474, avg val loss=0.614, auc=0.620\n",
      "Iter=1710, avg train loss=0.344, avg val loss=0.627, auc=0.631\n",
      "Iter=1715, avg train loss=0.503, avg val loss=0.672, auc=0.625\n",
      "Iter=1720, avg train loss=0.351, avg val loss=0.627, auc=0.637\n",
      "Iter=1725, avg train loss=0.507, avg val loss=0.608, auc=0.625\n",
      "Iter=1730, avg train loss=0.441, avg val loss=0.620, auc=0.620\n",
      "Iter=1735, avg train loss=0.513, avg val loss=0.667, auc=0.630\n",
      "Iter=1740, avg train loss=0.298, avg val loss=0.641, auc=0.640\n",
      "Iter=1745, avg train loss=0.472, avg val loss=0.625, auc=0.639\n",
      "Iter=1750, avg train loss=0.403, avg val loss=0.588, auc=0.645\n",
      "Iter=1755, avg train loss=0.495, avg val loss=0.682, auc=0.641\n",
      "Iter=1760, avg train loss=0.523, avg val loss=0.642, auc=0.644\n",
      "Iter=1765, avg train loss=0.291, avg val loss=0.653, auc=0.642\n",
      "Iter=1770, avg train loss=0.357, avg val loss=0.655, auc=0.641\n",
      "Iter=1775, avg train loss=0.315, avg val loss=0.642, auc=0.650\n",
      "Iter=1780, avg train loss=0.364, avg val loss=0.681, auc=0.649\n",
      "Iter=1785, avg train loss=0.312, avg val loss=0.752, auc=0.646\n",
      "Iter=1790, avg train loss=0.550, avg val loss=0.629, auc=0.647\n",
      "Iter=1795, avg train loss=0.309, avg val loss=0.602, auc=0.644\n",
      "Iter=1800, avg train loss=0.380, avg val loss=0.595, auc=0.643\n",
      "Iter=1805, avg train loss=0.429, avg val loss=0.606, auc=0.637\n",
      "Iter=1810, avg train loss=0.347, avg val loss=0.636, auc=0.630\n",
      "Iter=1815, avg train loss=0.402, avg val loss=0.628, auc=0.635\n",
      "Iter=1820, avg train loss=0.472, avg val loss=0.613, auc=0.645\n",
      "Iter=1825, avg train loss=0.359, avg val loss=0.596, auc=0.640\n",
      "Iter=1830, avg train loss=0.412, avg val loss=0.627, auc=0.645\n",
      "Iter=1835, avg train loss=0.292, avg val loss=0.831, auc=0.638\n",
      "Iter=1840, avg train loss=0.472, avg val loss=0.645, auc=0.641\n",
      "Iter=1845, avg train loss=0.451, avg val loss=0.587, auc=0.635\n",
      "Iter=1850, avg train loss=0.355, avg val loss=0.570, auc=0.631\n",
      "Iter=1855, avg train loss=0.501, avg val loss=0.573, auc=0.638\n",
      "Iter=1860, avg train loss=0.520, avg val loss=0.607, auc=0.652\n",
      "Iter=1865, avg train loss=0.383, avg val loss=0.675, auc=0.659\n",
      "Iter=1870, avg train loss=0.389, avg val loss=0.600, auc=0.663\n",
      "Iter=1875, avg train loss=0.479, avg val loss=0.613, auc=0.664\n",
      "Iter=1880, avg train loss=0.434, avg val loss=0.590, auc=0.660\n",
      "Iter=1885, avg train loss=0.324, avg val loss=0.590, auc=0.662\n",
      "Iter=1890, avg train loss=0.478, avg val loss=0.635, auc=0.650\n",
      "Iter=1895, avg train loss=0.285, avg val loss=0.583, auc=0.645\n",
      "Iter=1900, avg train loss=0.373, avg val loss=0.578, auc=0.644\n",
      "Iter=1905, avg train loss=0.435, avg val loss=0.564, auc=0.650\n",
      "Iter=1910, avg train loss=0.358, avg val loss=0.583, auc=0.644\n",
      "Iter=1915, avg train loss=0.557, avg val loss=0.653, auc=0.647\n",
      "Iter=1920, avg train loss=0.401, avg val loss=0.751, auc=0.661\n",
      "Iter=1925, avg train loss=0.329, avg val loss=0.707, auc=0.656\n",
      "Iter=1930, avg train loss=0.482, avg val loss=0.662, auc=0.638\n",
      "Iter=1935, avg train loss=0.357, avg val loss=0.571, auc=0.633\n",
      "Iter=1940, avg train loss=0.358, avg val loss=0.576, auc=0.629\n",
      "Iter=1945, avg train loss=0.422, avg val loss=0.662, auc=0.638\n",
      "Iter=1950, avg train loss=0.418, avg val loss=0.696, auc=0.643\n",
      "Iter=1955, avg train loss=0.437, avg val loss=0.690, auc=0.631\n",
      "Iter=1960, avg train loss=0.526, avg val loss=0.708, auc=0.620\n",
      "Iter=1965, avg train loss=0.375, avg val loss=0.579, auc=0.615\n",
      "Iter=1970, avg train loss=0.479, avg val loss=0.634, auc=0.601\n",
      "Iter=1975, avg train loss=0.368, avg val loss=0.600, auc=0.614\n",
      "Iter=1980, avg train loss=0.383, avg val loss=0.652, auc=0.621\n",
      "Iter=1985, avg train loss=0.290, avg val loss=0.692, auc=0.611\n",
      "Iter=1990, avg train loss=0.352, avg val loss=0.760, auc=0.611\n",
      "Iter=1995, avg train loss=0.436, avg val loss=0.680, auc=0.607\n",
      "Iter=2000, avg train loss=0.228, avg val loss=0.606, auc=0.629\n",
      "Iter=2005, avg train loss=0.383, avg val loss=0.614, auc=0.635\n",
      "Iter=2010, avg train loss=0.414, avg val loss=0.657, auc=0.631\n",
      "Iter=2015, avg train loss=0.391, avg val loss=0.700, auc=0.635\n",
      "Iter=2020, avg train loss=0.349, avg val loss=0.672, auc=0.643\n",
      "Iter=2025, avg train loss=0.360, avg val loss=0.672, auc=0.644\n",
      "Iter=2030, avg train loss=0.406, avg val loss=0.691, auc=0.644\n",
      "Iter=2035, avg train loss=0.414, avg val loss=0.666, auc=0.633\n",
      "Iter=2040, avg train loss=0.341, avg val loss=0.609, auc=0.638\n",
      "Iter=2045, avg train loss=0.380, avg val loss=0.710, auc=0.642\n",
      "Iter=2050, avg train loss=0.291, avg val loss=0.591, auc=0.639\n",
      "Iter=2055, avg train loss=0.518, avg val loss=0.609, auc=0.628\n",
      "Iter=2060, avg train loss=0.282, avg val loss=0.634, auc=0.614\n",
      "Iter=2065, avg train loss=0.395, avg val loss=0.589, auc=0.630\n",
      "Iter=2070, avg train loss=0.289, avg val loss=0.606, auc=0.627\n",
      "Iter=2075, avg train loss=0.391, avg val loss=0.620, auc=0.628\n",
      "Iter=2080, avg train loss=0.459, avg val loss=0.638, auc=0.636\n",
      "Iter=2085, avg train loss=0.359, avg val loss=0.629, auc=0.628\n",
      "Iter=2090, avg train loss=0.278, avg val loss=0.638, auc=0.635\n",
      "Iter=2095, avg train loss=0.385, avg val loss=0.595, auc=0.641\n",
      "Iter=2100, avg train loss=0.324, avg val loss=0.615, auc=0.635\n",
      "Iter=2105, avg train loss=0.423, avg val loss=0.801, auc=0.624\n",
      "Iter=2110, avg train loss=0.296, avg val loss=0.643, auc=0.633\n",
      "Iter=2115, avg train loss=0.302, avg val loss=0.702, auc=0.633\n",
      "Iter=2120, avg train loss=0.252, avg val loss=0.638, auc=0.636\n",
      "Iter=2125, avg train loss=0.355, avg val loss=0.635, auc=0.630\n",
      "Iter=2130, avg train loss=0.381, avg val loss=0.604, auc=0.627\n",
      "Iter=2135, avg train loss=0.296, avg val loss=0.658, auc=0.638\n",
      "Iter=2140, avg train loss=0.327, avg val loss=0.670, auc=0.656\n",
      "Iter=2145, avg train loss=0.381, avg val loss=0.665, auc=0.649\n",
      "Iter=2150, avg train loss=0.341, avg val loss=0.627, auc=0.638\n",
      "Iter=2155, avg train loss=0.382, avg val loss=0.621, auc=0.648\n",
      "Iter=2160, avg train loss=0.488, avg val loss=0.596, auc=0.628\n",
      "Iter=2165, avg train loss=0.393, avg val loss=0.632, auc=0.644\n",
      "Iter=2170, avg train loss=0.443, avg val loss=0.696, auc=0.655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=2175, avg train loss=0.291, avg val loss=0.851, auc=0.643\n",
      "Iter=2180, avg train loss=0.511, avg val loss=0.777, auc=0.636\n",
      "Iter=2185, avg train loss=0.487, avg val loss=0.652, auc=0.620\n",
      "Iter=2190, avg train loss=0.527, avg val loss=0.605, auc=0.617\n",
      "Iter=2195, avg train loss=0.444, avg val loss=0.601, auc=0.607\n",
      "Iter=2200, avg train loss=0.238, avg val loss=0.602, auc=0.606\n",
      "Iter=2205, avg train loss=0.330, avg val loss=0.631, auc=0.612\n",
      "Iter=2210, avg train loss=0.424, avg val loss=0.626, auc=0.625\n",
      "Iter=2215, avg train loss=0.337, avg val loss=0.760, auc=0.632\n",
      "Iter=2220, avg train loss=0.443, avg val loss=0.856, auc=0.627\n",
      "Iter=2225, avg train loss=0.393, avg val loss=0.632, auc=0.621\n",
      "Iter=2230, avg train loss=0.251, avg val loss=0.605, auc=0.625\n",
      "Iter=2235, avg train loss=0.396, avg val loss=0.624, auc=0.620\n",
      "Iter=2240, avg train loss=0.377, avg val loss=0.607, auc=0.617\n",
      "Iter=2245, avg train loss=0.528, avg val loss=0.607, auc=0.610\n",
      "Iter=2250, avg train loss=0.379, avg val loss=0.619, auc=0.618\n",
      "Iter=2255, avg train loss=0.343, avg val loss=0.627, auc=0.632\n",
      "Iter=2260, avg train loss=0.400, avg val loss=0.682, auc=0.618\n",
      "Iter=2265, avg train loss=0.315, avg val loss=0.662, auc=0.634\n",
      "Iter=2270, avg train loss=0.264, avg val loss=0.593, auc=0.620\n",
      "Iter=2275, avg train loss=0.385, avg val loss=0.630, auc=0.611\n",
      "Iter=2280, avg train loss=0.317, avg val loss=0.643, auc=0.601\n",
      "Iter=2285, avg train loss=0.289, avg val loss=0.698, auc=0.609\n",
      "Iter=2290, avg train loss=0.321, avg val loss=0.716, auc=0.615\n",
      "Iter=2295, avg train loss=0.297, avg val loss=0.736, auc=0.626\n",
      "Iter=2300, avg train loss=0.394, avg val loss=0.736, auc=0.630\n",
      "Iter=2305, avg train loss=0.272, avg val loss=0.699, auc=0.637\n",
      "Iter=2310, avg train loss=0.309, avg val loss=0.726, auc=0.643\n",
      "Iter=2315, avg train loss=0.386, avg val loss=0.789, auc=0.635\n",
      "Iter=2320, avg train loss=0.485, avg val loss=0.618, auc=0.621\n",
      "Iter=2325, avg train loss=0.233, avg val loss=0.646, auc=0.620\n",
      "Iter=2330, avg train loss=0.279, avg val loss=0.611, auc=0.615\n",
      "Iter=2335, avg train loss=0.509, avg val loss=0.639, auc=0.637\n",
      "Iter=2340, avg train loss=0.375, avg val loss=0.678, auc=0.636\n",
      "Iter=2345, avg train loss=0.497, avg val loss=0.670, auc=0.613\n",
      "Iter=2350, avg train loss=0.460, avg val loss=0.719, auc=0.607\n",
      "Iter=2355, avg train loss=0.381, avg val loss=0.622, auc=0.616\n",
      "Iter=2360, avg train loss=0.225, avg val loss=0.598, auc=0.600\n",
      "Iter=2365, avg train loss=0.322, avg val loss=0.630, auc=0.599\n",
      "Iter=2370, avg train loss=0.417, avg val loss=0.659, auc=0.600\n",
      "Iter=2375, avg train loss=0.358, avg val loss=0.817, auc=0.607\n",
      "Iter=2380, avg train loss=0.335, avg val loss=0.679, auc=0.619\n",
      "Iter=2385, avg train loss=0.440, avg val loss=0.641, auc=0.624\n",
      "Iter=2390, avg train loss=0.204, avg val loss=0.631, auc=0.623\n",
      "Iter=2395, avg train loss=0.477, avg val loss=0.599, auc=0.616\n",
      "Iter=2400, avg train loss=0.222, avg val loss=0.674, auc=0.618\n",
      "Iter=2405, avg train loss=0.297, avg val loss=0.710, auc=0.615\n",
      "Iter=2410, avg train loss=0.331, avg val loss=0.708, auc=0.615\n",
      "Iter=2415, avg train loss=0.280, avg val loss=0.683, auc=0.610\n",
      "Iter=2420, avg train loss=0.341, avg val loss=0.675, auc=0.625\n",
      "Iter=2425, avg train loss=0.471, avg val loss=0.668, auc=0.637\n",
      "Iter=2430, avg train loss=0.333, avg val loss=0.672, auc=0.612\n",
      "Iter=2435, avg train loss=0.558, avg val loss=0.698, auc=0.602\n",
      "Iter=2440, avg train loss=0.298, avg val loss=0.629, auc=0.637\n",
      "Iter=2445, avg train loss=0.340, avg val loss=0.638, auc=0.631\n",
      "Iter=2450, avg train loss=0.397, avg val loss=0.603, auc=0.638\n",
      "Iter=2455, avg train loss=0.291, avg val loss=0.588, auc=0.626\n",
      "Iter=2460, avg train loss=0.224, avg val loss=0.622, auc=0.622\n",
      "Iter=2465, avg train loss=0.392, avg val loss=0.603, auc=0.624\n",
      "Iter=2470, avg train loss=0.312, avg val loss=0.610, auc=0.621\n",
      "Iter=2475, avg train loss=0.279, avg val loss=0.655, auc=0.622\n",
      "Iter=2480, avg train loss=0.326, avg val loss=0.612, auc=0.627\n",
      "Iter=2485, avg train loss=0.424, avg val loss=0.621, auc=0.623\n",
      "Iter=2490, avg train loss=0.360, avg val loss=0.613, auc=0.639\n",
      "Iter=2495, avg train loss=0.269, avg val loss=0.617, auc=0.652\n",
      "Iter=2500, avg train loss=0.295, avg val loss=0.647, auc=0.655\n",
      "Iter=2505, avg train loss=0.383, avg val loss=0.623, auc=0.643\n",
      "Iter=2510, avg train loss=0.215, avg val loss=0.636, auc=0.639\n",
      "Iter=2515, avg train loss=0.260, avg val loss=0.627, auc=0.635\n",
      "Iter=2520, avg train loss=0.357, avg val loss=0.649, auc=0.632\n",
      "Iter=2525, avg train loss=0.348, avg val loss=0.665, auc=0.619\n",
      "Iter=2530, avg train loss=0.212, avg val loss=0.652, auc=0.623\n",
      "Iter=2535, avg train loss=0.306, avg val loss=0.664, auc=0.622\n",
      "Iter=2540, avg train loss=0.210, avg val loss=0.658, auc=0.633\n",
      "Iter=2545, avg train loss=0.320, avg val loss=0.730, auc=0.629\n",
      "Iter=2550, avg train loss=0.365, avg val loss=0.704, auc=0.622\n",
      "Iter=2555, avg train loss=0.226, avg val loss=0.744, auc=0.617\n",
      "Iter=2560, avg train loss=0.230, avg val loss=0.604, auc=0.621\n",
      "Iter=2565, avg train loss=0.272, avg val loss=0.665, auc=0.636\n",
      "Iter=2570, avg train loss=0.340, avg val loss=0.750, auc=0.634\n",
      "Iter=2575, avg train loss=0.318, avg val loss=0.703, auc=0.638\n",
      "Iter=2580, avg train loss=0.278, avg val loss=0.656, auc=0.641\n",
      "Iter=2585, avg train loss=0.291, avg val loss=0.639, auc=0.640\n",
      "Iter=2590, avg train loss=0.367, avg val loss=0.672, auc=0.629\n",
      "Iter=2595, avg train loss=0.288, avg val loss=0.706, auc=0.635\n",
      "Iter=2600, avg train loss=0.202, avg val loss=0.648, auc=0.615\n",
      "Iter=2605, avg train loss=0.200, avg val loss=0.615, auc=0.628\n",
      "Iter=2610, avg train loss=0.302, avg val loss=0.587, auc=0.634\n",
      "Iter=2615, avg train loss=0.253, avg val loss=0.643, auc=0.630\n",
      "Iter=2620, avg train loss=0.271, avg val loss=0.655, auc=0.629\n",
      "Iter=2625, avg train loss=0.346, avg val loss=0.704, auc=0.617\n",
      "Iter=2630, avg train loss=0.306, avg val loss=0.625, auc=0.629\n",
      "Iter=2635, avg train loss=0.310, avg val loss=0.609, auc=0.632\n",
      "Iter=2640, avg train loss=0.347, avg val loss=0.642, auc=0.642\n",
      "Iter=2645, avg train loss=0.444, avg val loss=0.595, auc=0.646\n",
      "Iter=2650, avg train loss=0.276, avg val loss=0.593, auc=0.627\n",
      "Iter=2655, avg train loss=0.334, avg val loss=0.660, auc=0.623\n",
      "Iter=2660, avg train loss=0.264, avg val loss=0.614, auc=0.619\n",
      "Iter=2665, avg train loss=0.232, avg val loss=0.653, auc=0.616\n",
      "Iter=2670, avg train loss=0.312, avg val loss=0.711, auc=0.609\n",
      "Iter=2675, avg train loss=0.210, avg val loss=0.637, auc=0.621\n",
      "Iter=2680, avg train loss=0.312, avg val loss=0.612, auc=0.607\n",
      "Iter=2685, avg train loss=0.399, avg val loss=0.635, auc=0.588\n",
      "Iter=2690, avg train loss=0.238, avg val loss=0.685, auc=0.604\n",
      "Iter=2695, avg train loss=0.278, avg val loss=0.693, auc=0.610\n",
      "Iter=2700, avg train loss=0.207, avg val loss=0.682, auc=0.607\n",
      "Iter=2705, avg train loss=0.314, avg val loss=0.735, auc=0.615\n",
      "Iter=2710, avg train loss=0.260, avg val loss=0.683, auc=0.613\n",
      "Iter=2715, avg train loss=0.402, avg val loss=0.710, auc=0.595\n",
      "Iter=2720, avg train loss=0.249, avg val loss=0.630, auc=0.590\n",
      "Iter=2725, avg train loss=0.312, avg val loss=0.648, auc=0.591\n",
      "Iter=2730, avg train loss=0.212, avg val loss=0.652, auc=0.611\n",
      "Iter=2735, avg train loss=0.278, avg val loss=0.719, auc=0.609\n",
      "Iter=2740, avg train loss=0.377, avg val loss=0.680, auc=0.609\n",
      "Iter=2745, avg train loss=0.332, avg val loss=0.674, auc=0.625\n",
      "Iter=2750, avg train loss=0.217, avg val loss=0.683, auc=0.611\n",
      "Iter=2755, avg train loss=0.291, avg val loss=0.602, auc=0.623\n",
      "Iter=2760, avg train loss=0.290, avg val loss=0.610, auc=0.614\n",
      "Iter=2765, avg train loss=0.240, avg val loss=0.679, auc=0.613\n",
      "Iter=2770, avg train loss=0.246, avg val loss=0.636, auc=0.627\n",
      "Iter=2775, avg train loss=0.376, avg val loss=0.608, auc=0.632\n",
      "Iter=2780, avg train loss=0.439, avg val loss=0.678, auc=0.640\n",
      "Iter=2785, avg train loss=0.239, avg val loss=0.624, auc=0.629\n",
      "Iter=2790, avg train loss=0.269, avg val loss=0.633, auc=0.637\n",
      "Iter=2795, avg train loss=0.241, avg val loss=0.682, auc=0.628\n",
      "Iter=2800, avg train loss=0.253, avg val loss=0.637, auc=0.635\n",
      "Iter=2805, avg train loss=0.261, avg val loss=0.676, auc=0.631\n",
      "Iter=2810, avg train loss=0.452, avg val loss=0.672, auc=0.618\n",
      "Iter=2815, avg train loss=0.288, avg val loss=0.679, auc=0.620\n",
      "Iter=2820, avg train loss=0.266, avg val loss=0.615, auc=0.613\n",
      "Iter=2825, avg train loss=0.287, avg val loss=0.609, auc=0.614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=2830, avg train loss=0.191, avg val loss=0.667, auc=0.624\n",
      "Iter=2835, avg train loss=0.263, avg val loss=0.666, auc=0.645\n",
      "Iter=2840, avg train loss=0.479, avg val loss=0.664, auc=0.623\n",
      "Iter=2845, avg train loss=0.240, avg val loss=0.600, auc=0.612\n",
      "Iter=2850, avg train loss=0.248, avg val loss=0.632, auc=0.608\n",
      "Iter=2855, avg train loss=0.232, avg val loss=0.625, auc=0.635\n",
      "Iter=2860, avg train loss=0.196, avg val loss=0.614, auc=0.629\n",
      "Iter=2865, avg train loss=0.218, avg val loss=0.627, auc=0.633\n",
      "Iter=2870, avg train loss=0.348, avg val loss=0.612, auc=0.630\n",
      "Iter=2875, avg train loss=0.320, avg val loss=0.639, auc=0.630\n",
      "Iter=2880, avg train loss=0.232, avg val loss=0.653, auc=0.644\n",
      "Iter=2885, avg train loss=0.307, avg val loss=0.692, auc=0.651\n",
      "Iter=2890, avg train loss=0.297, avg val loss=0.675, auc=0.674\n",
      "Iter=2895, avg train loss=0.344, avg val loss=0.637, auc=0.671\n",
      "Iter=2900, avg train loss=0.179, avg val loss=0.620, auc=0.646\n",
      "Iter=2905, avg train loss=0.274, avg val loss=0.620, auc=0.646\n",
      "Iter=2910, avg train loss=0.239, avg val loss=0.620, auc=0.653\n",
      "Iter=2915, avg train loss=0.312, avg val loss=0.665, auc=0.657\n",
      "Iter=2920, avg train loss=0.200, avg val loss=0.669, auc=0.635\n",
      "Iter=2925, avg train loss=0.275, avg val loss=0.635, auc=0.630\n",
      "Iter=2930, avg train loss=0.454, avg val loss=0.657, auc=0.613\n",
      "Iter=2935, avg train loss=0.273, avg val loss=0.668, auc=0.626\n",
      "Iter=2940, avg train loss=0.200, avg val loss=0.673, auc=0.635\n",
      "Iter=2945, avg train loss=0.198, avg val loss=0.705, auc=0.641\n",
      "Iter=2950, avg train loss=0.349, avg val loss=0.696, auc=0.669\n",
      "Iter=2955, avg train loss=0.178, avg val loss=0.696, auc=0.671\n",
      "Iter=2960, avg train loss=0.301, avg val loss=0.761, auc=0.651\n",
      "Iter=2965, avg train loss=0.305, avg val loss=0.637, auc=0.644\n",
      "Iter=2970, avg train loss=0.196, avg val loss=0.766, auc=0.624\n",
      "Iter=2975, avg train loss=0.166, avg val loss=0.684, auc=0.613\n",
      "Iter=2980, avg train loss=0.274, avg val loss=0.646, auc=0.612\n",
      "Iter=2985, avg train loss=0.208, avg val loss=0.641, auc=0.618\n",
      "Iter=2990, avg train loss=0.244, avg val loss=0.649, auc=0.611\n",
      "Iter=2995, avg train loss=0.190, avg val loss=0.681, auc=0.609\n",
      "Iter=3000, avg train loss=0.240, avg val loss=0.717, auc=0.622\n",
      "Iter=3005, avg train loss=0.191, avg val loss=0.689, auc=0.630\n",
      "Iter=3010, avg train loss=0.200, avg val loss=0.646, auc=0.628\n",
      "Iter=3015, avg train loss=0.199, avg val loss=0.722, auc=0.629\n",
      "Iter=3020, avg train loss=0.304, avg val loss=0.681, auc=0.648\n",
      "Iter=3025, avg train loss=0.197, avg val loss=0.646, auc=0.652\n",
      "Iter=3030, avg train loss=0.305, avg val loss=0.653, auc=0.640\n",
      "Iter=3035, avg train loss=0.299, avg val loss=0.631, auc=0.637\n",
      "Iter=3040, avg train loss=0.301, avg val loss=0.631, auc=0.652\n",
      "Iter=3045, avg train loss=0.228, avg val loss=0.627, auc=0.642\n",
      "Iter=3050, avg train loss=0.133, avg val loss=0.633, auc=0.626\n",
      "Iter=3055, avg train loss=0.204, avg val loss=0.734, auc=0.623\n",
      "Iter=3060, avg train loss=0.196, avg val loss=0.789, auc=0.623\n",
      "Iter=3065, avg train loss=0.188, avg val loss=0.735, auc=0.628\n",
      "Iter=3070, avg train loss=0.260, avg val loss=0.753, auc=0.621\n",
      "Iter=3075, avg train loss=0.234, avg val loss=0.707, auc=0.602\n",
      "Iter=3080, avg train loss=0.220, avg val loss=0.723, auc=0.594\n",
      "Iter=3085, avg train loss=0.312, avg val loss=0.728, auc=0.600\n",
      "Iter=3090, avg train loss=0.192, avg val loss=0.754, auc=0.613\n",
      "Iter=3095, avg train loss=0.193, avg val loss=0.843, auc=0.627\n",
      "Iter=3100, avg train loss=0.309, avg val loss=0.647, auc=0.633\n",
      "Iter=3105, avg train loss=0.237, avg val loss=0.707, auc=0.605\n",
      "Iter=3110, avg train loss=0.342, avg val loss=0.726, auc=0.592\n",
      "Iter=3115, avg train loss=0.324, avg val loss=0.629, auc=0.592\n",
      "Iter=3120, avg train loss=0.176, avg val loss=0.693, auc=0.592\n",
      "Iter=3125, avg train loss=0.204, avg val loss=0.690, auc=0.594\n",
      "Iter=3130, avg train loss=0.168, avg val loss=0.634, auc=0.621\n",
      "Iter=3135, avg train loss=0.247, avg val loss=0.658, auc=0.631\n",
      "Iter=3140, avg train loss=0.275, avg val loss=0.709, auc=0.636\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.707, max-score-based AUC=0.708\n",
      "\n",
      "\n",
      "========== Fold 4 ==========\n",
      "Test AUC at start=0.680, max-score-based AUC=0.686\n",
      "Iter=5, avg train loss=0.754, avg val loss=0.543, auc=0.624\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.808, avg val loss=0.541, auc=0.631\n",
      "Best model saved.\n",
      "Iter=15, avg train loss=0.841, avg val loss=0.541, auc=0.636\n",
      "Best model saved.\n",
      "Iter=20, avg train loss=0.775, avg val loss=0.541, auc=0.639\n",
      "Best model saved.\n",
      "Iter=25, avg train loss=0.604, avg val loss=0.541, auc=0.644\n",
      "Best model saved.\n",
      "Iter=30, avg train loss=0.840, avg val loss=0.545, auc=0.640\n",
      "Iter=35, avg train loss=0.713, avg val loss=0.540, auc=0.642\n",
      "Iter=40, avg train loss=0.644, avg val loss=0.558, auc=0.646\n",
      "Best model saved.\n",
      "Iter=45, avg train loss=0.501, avg val loss=0.557, auc=0.657\n",
      "Best model saved.\n",
      "Iter=50, avg train loss=0.844, avg val loss=0.548, auc=0.664\n",
      "Best model saved.\n",
      "Iter=55, avg train loss=0.708, avg val loss=0.550, auc=0.674\n",
      "Best model saved.\n",
      "Iter=60, avg train loss=0.712, avg val loss=0.539, auc=0.674\n",
      "Best model saved.\n",
      "Iter=65, avg train loss=0.678, avg val loss=0.553, auc=0.674\n",
      "Iter=70, avg train loss=0.742, avg val loss=0.556, auc=0.664\n",
      "Iter=75, avg train loss=0.633, avg val loss=0.550, auc=0.659\n",
      "Iter=80, avg train loss=0.685, avg val loss=0.560, auc=0.677\n",
      "Best model saved.\n",
      "Iter=85, avg train loss=0.702, avg val loss=0.568, auc=0.678\n",
      "Best model saved.\n",
      "Iter=90, avg train loss=0.707, avg val loss=0.564, auc=0.677\n",
      "Iter=95, avg train loss=0.671, avg val loss=0.577, auc=0.691\n",
      "Best model saved.\n",
      "Iter=100, avg train loss=0.717, avg val loss=0.566, auc=0.676\n",
      "Iter=105, avg train loss=0.743, avg val loss=0.601, auc=0.694\n",
      "Best model saved.\n",
      "Iter=110, avg train loss=0.649, avg val loss=0.611, auc=0.691\n",
      "Iter=115, avg train loss=0.560, avg val loss=0.590, auc=0.684\n",
      "Iter=120, avg train loss=0.606, avg val loss=0.604, auc=0.685\n",
      "Iter=125, avg train loss=0.686, avg val loss=0.577, auc=0.678\n",
      "Iter=130, avg train loss=0.735, avg val loss=0.569, auc=0.675\n",
      "Iter=135, avg train loss=0.618, avg val loss=0.611, auc=0.691\n",
      "Iter=140, avg train loss=0.685, avg val loss=0.585, auc=0.687\n",
      "Iter=145, avg train loss=0.654, avg val loss=0.609, auc=0.696\n",
      "Best model saved.\n",
      "Iter=150, avg train loss=0.612, avg val loss=0.563, auc=0.665\n",
      "Iter=155, avg train loss=0.686, avg val loss=0.566, auc=0.680\n",
      "Iter=160, avg train loss=0.575, avg val loss=0.558, auc=0.678\n",
      "Iter=165, avg train loss=0.669, avg val loss=0.555, auc=0.665\n",
      "Iter=170, avg train loss=0.688, avg val loss=0.563, auc=0.679\n",
      "Iter=175, avg train loss=0.635, avg val loss=0.555, auc=0.661\n",
      "Iter=180, avg train loss=0.703, avg val loss=0.597, auc=0.682\n",
      "Iter=185, avg train loss=0.579, avg val loss=0.576, auc=0.681\n",
      "Iter=190, avg train loss=0.658, avg val loss=0.586, auc=0.686\n",
      "Iter=195, avg train loss=0.659, avg val loss=0.599, auc=0.688\n",
      "Iter=200, avg train loss=0.814, avg val loss=0.601, auc=0.677\n",
      "Iter=205, avg train loss=0.628, avg val loss=0.577, auc=0.660\n",
      "Iter=210, avg train loss=0.665, avg val loss=0.563, auc=0.655\n",
      "Iter=215, avg train loss=0.708, avg val loss=0.564, auc=0.654\n",
      "Iter=220, avg train loss=0.732, avg val loss=0.589, auc=0.663\n",
      "Iter=225, avg train loss=0.655, avg val loss=0.622, auc=0.671\n",
      "Iter=230, avg train loss=0.617, avg val loss=0.632, auc=0.668\n",
      "Iter=235, avg train loss=0.640, avg val loss=0.601, auc=0.683\n",
      "Iter=240, avg train loss=0.671, avg val loss=0.586, auc=0.677\n",
      "Iter=245, avg train loss=0.579, avg val loss=0.587, auc=0.673\n",
      "Iter=250, avg train loss=0.679, avg val loss=0.565, auc=0.675\n",
      "Iter=255, avg train loss=0.673, avg val loss=0.567, auc=0.664\n",
      "Iter=260, avg train loss=0.680, avg val loss=0.612, auc=0.669\n",
      "Iter=265, avg train loss=0.666, avg val loss=0.607, auc=0.673\n",
      "Iter=270, avg train loss=0.599, avg val loss=0.565, auc=0.670\n",
      "Iter=275, avg train loss=0.645, avg val loss=0.568, auc=0.653\n",
      "Iter=280, avg train loss=0.630, avg val loss=0.550, auc=0.659\n",
      "Iter=285, avg train loss=0.747, avg val loss=0.573, auc=0.667\n",
      "Iter=290, avg train loss=0.682, avg val loss=0.589, auc=0.676\n",
      "Iter=295, avg train loss=0.713, avg val loss=0.599, auc=0.683\n",
      "Iter=300, avg train loss=0.699, avg val loss=0.584, auc=0.695\n",
      "Iter=305, avg train loss=0.614, avg val loss=0.599, auc=0.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=310, avg train loss=0.634, avg val loss=0.600, auc=0.693\n",
      "Iter=315, avg train loss=0.670, avg val loss=0.593, auc=0.692\n",
      "Iter=320, avg train loss=0.607, avg val loss=0.590, auc=0.691\n",
      "Iter=325, avg train loss=0.710, avg val loss=0.594, auc=0.688\n",
      "Iter=330, avg train loss=0.610, avg val loss=0.625, auc=0.701\n",
      "Best model saved.\n",
      "Iter=335, avg train loss=0.692, avg val loss=0.619, auc=0.711\n",
      "Best model saved.\n",
      "Iter=340, avg train loss=0.588, avg val loss=0.627, auc=0.701\n",
      "Iter=345, avg train loss=0.615, avg val loss=0.594, auc=0.715\n",
      "Best model saved.\n",
      "Iter=350, avg train loss=0.560, avg val loss=0.589, auc=0.714\n",
      "Iter=355, avg train loss=0.617, avg val loss=0.569, auc=0.707\n",
      "Iter=360, avg train loss=0.588, avg val loss=0.575, auc=0.707\n",
      "Iter=365, avg train loss=0.679, avg val loss=0.562, auc=0.700\n",
      "Iter=370, avg train loss=0.650, avg val loss=0.607, auc=0.711\n",
      "Iter=375, avg train loss=0.589, avg val loss=0.607, auc=0.705\n",
      "Iter=380, avg train loss=0.689, avg val loss=0.603, auc=0.708\n",
      "Iter=385, avg train loss=0.570, avg val loss=0.562, auc=0.693\n",
      "Iter=390, avg train loss=0.584, avg val loss=0.552, auc=0.686\n",
      "Iter=395, avg train loss=0.580, avg val loss=0.555, auc=0.685\n",
      "Iter=400, avg train loss=0.618, avg val loss=0.569, auc=0.688\n",
      "Iter=405, avg train loss=0.638, avg val loss=0.589, auc=0.683\n",
      "Iter=410, avg train loss=0.585, avg val loss=0.598, auc=0.680\n",
      "Iter=415, avg train loss=0.629, avg val loss=0.610, auc=0.677\n",
      "Iter=420, avg train loss=0.674, avg val loss=0.610, auc=0.678\n",
      "Iter=425, avg train loss=0.621, avg val loss=0.606, auc=0.681\n",
      "Iter=430, avg train loss=0.630, avg val loss=0.610, auc=0.682\n",
      "Iter=435, avg train loss=0.643, avg val loss=0.590, auc=0.663\n",
      "Iter=440, avg train loss=0.640, avg val loss=0.629, auc=0.685\n",
      "Iter=445, avg train loss=0.598, avg val loss=0.600, auc=0.687\n",
      "Iter=450, avg train loss=0.660, avg val loss=0.593, auc=0.677\n",
      "Iter=455, avg train loss=0.586, avg val loss=0.588, auc=0.685\n",
      "Iter=460, avg train loss=0.625, avg val loss=0.544, auc=0.666\n",
      "Iter=465, avg train loss=0.657, avg val loss=0.540, auc=0.651\n",
      "Iter=470, avg train loss=0.706, avg val loss=0.557, auc=0.674\n",
      "Iter=475, avg train loss=0.657, avg val loss=0.556, auc=0.678\n",
      "Iter=480, avg train loss=0.571, avg val loss=0.555, auc=0.680\n",
      "Iter=485, avg train loss=0.551, avg val loss=0.581, auc=0.690\n",
      "Iter=490, avg train loss=0.693, avg val loss=0.621, auc=0.705\n",
      "Iter=495, avg train loss=0.658, avg val loss=0.629, auc=0.701\n",
      "Iter=500, avg train loss=0.594, avg val loss=0.626, auc=0.694\n",
      "Iter=505, avg train loss=0.663, avg val loss=0.597, auc=0.687\n",
      "Iter=510, avg train loss=0.669, avg val loss=0.579, auc=0.670\n",
      "Iter=515, avg train loss=0.643, avg val loss=0.568, auc=0.680\n",
      "Iter=520, avg train loss=0.588, avg val loss=0.544, auc=0.660\n",
      "Iter=525, avg train loss=0.587, avg val loss=0.553, auc=0.667\n",
      "Iter=530, avg train loss=0.787, avg val loss=0.575, auc=0.691\n",
      "Iter=535, avg train loss=0.619, avg val loss=0.582, auc=0.702\n",
      "Iter=540, avg train loss=0.629, avg val loss=0.601, auc=0.698\n",
      "Iter=545, avg train loss=0.647, avg val loss=0.601, auc=0.704\n",
      "Iter=550, avg train loss=0.665, avg val loss=0.614, auc=0.702\n",
      "Iter=555, avg train loss=0.665, avg val loss=0.621, auc=0.697\n",
      "Iter=560, avg train loss=0.719, avg val loss=0.619, auc=0.694\n",
      "Iter=565, avg train loss=0.567, avg val loss=0.548, auc=0.687\n",
      "Iter=570, avg train loss=0.589, avg val loss=0.581, auc=0.683\n",
      "Iter=575, avg train loss=0.517, avg val loss=0.581, auc=0.678\n",
      "Iter=580, avg train loss=0.585, avg val loss=0.552, auc=0.678\n",
      "Iter=585, avg train loss=0.595, avg val loss=0.562, auc=0.677\n",
      "Iter=590, avg train loss=0.536, avg val loss=0.555, auc=0.692\n",
      "Iter=595, avg train loss=0.638, avg val loss=0.574, auc=0.704\n",
      "Iter=600, avg train loss=0.661, avg val loss=0.610, auc=0.697\n",
      "Iter=605, avg train loss=0.626, avg val loss=0.607, auc=0.702\n",
      "Iter=610, avg train loss=0.636, avg val loss=0.559, auc=0.700\n",
      "Iter=615, avg train loss=0.634, avg val loss=0.555, auc=0.704\n",
      "Iter=620, avg train loss=0.594, avg val loss=0.559, auc=0.701\n",
      "Iter=625, avg train loss=0.631, avg val loss=0.589, auc=0.713\n",
      "Iter=630, avg train loss=0.642, avg val loss=0.603, auc=0.713\n",
      "Iter=635, avg train loss=0.608, avg val loss=0.616, auc=0.708\n",
      "Iter=640, avg train loss=0.598, avg val loss=0.642, auc=0.715\n",
      "Iter=645, avg train loss=0.577, avg val loss=0.574, auc=0.704\n",
      "Iter=650, avg train loss=0.628, avg val loss=0.566, auc=0.699\n",
      "Iter=655, avg train loss=0.623, avg val loss=0.538, auc=0.681\n",
      "Iter=660, avg train loss=0.711, avg val loss=0.550, auc=0.683\n",
      "Iter=665, avg train loss=0.593, avg val loss=0.593, auc=0.674\n",
      "Iter=670, avg train loss=0.609, avg val loss=0.612, auc=0.699\n",
      "Iter=675, avg train loss=0.566, avg val loss=0.680, auc=0.717\n",
      "Best model saved.\n",
      "Iter=680, avg train loss=0.604, avg val loss=0.632, auc=0.695\n",
      "Iter=685, avg train loss=0.623, avg val loss=0.646, auc=0.703\n",
      "Iter=690, avg train loss=0.526, avg val loss=0.648, auc=0.706\n",
      "Iter=695, avg train loss=0.602, avg val loss=0.647, auc=0.694\n",
      "Iter=700, avg train loss=0.537, avg val loss=0.618, auc=0.714\n",
      "Iter=705, avg train loss=0.677, avg val loss=0.605, auc=0.703\n",
      "Iter=710, avg train loss=0.682, avg val loss=0.601, auc=0.714\n",
      "Iter=715, avg train loss=0.653, avg val loss=0.642, auc=0.715\n",
      "Iter=720, avg train loss=0.603, avg val loss=0.677, auc=0.718\n",
      "Best model saved.\n",
      "Iter=725, avg train loss=0.556, avg val loss=0.635, auc=0.704\n",
      "Iter=730, avg train loss=0.625, avg val loss=0.599, auc=0.705\n",
      "Iter=735, avg train loss=0.546, avg val loss=0.567, auc=0.683\n",
      "Iter=740, avg train loss=0.668, avg val loss=0.580, auc=0.689\n",
      "Iter=745, avg train loss=0.478, avg val loss=0.554, auc=0.676\n",
      "Iter=750, avg train loss=0.623, avg val loss=0.553, auc=0.675\n",
      "Iter=755, avg train loss=0.612, avg val loss=0.561, auc=0.680\n",
      "Iter=760, avg train loss=0.578, avg val loss=0.589, auc=0.694\n",
      "Iter=765, avg train loss=0.574, avg val loss=0.565, auc=0.673\n",
      "Iter=770, avg train loss=0.556, avg val loss=0.602, auc=0.696\n",
      "Iter=775, avg train loss=0.551, avg val loss=0.626, auc=0.706\n",
      "Iter=780, avg train loss=0.538, avg val loss=0.623, auc=0.704\n",
      "Iter=785, avg train loss=0.600, avg val loss=0.608, auc=0.696\n",
      "Iter=790, avg train loss=0.568, avg val loss=0.554, auc=0.684\n",
      "Iter=795, avg train loss=0.539, avg val loss=0.567, auc=0.683\n",
      "Iter=800, avg train loss=0.423, avg val loss=0.562, auc=0.682\n",
      "Iter=805, avg train loss=0.578, avg val loss=0.579, auc=0.687\n",
      "Iter=810, avg train loss=0.612, avg val loss=0.579, auc=0.692\n",
      "Iter=815, avg train loss=0.557, avg val loss=0.639, auc=0.703\n",
      "Iter=820, avg train loss=0.575, avg val loss=0.678, auc=0.707\n",
      "Iter=825, avg train loss=0.520, avg val loss=0.649, auc=0.705\n",
      "Iter=830, avg train loss=0.579, avg val loss=0.624, auc=0.702\n",
      "Iter=835, avg train loss=0.628, avg val loss=0.599, auc=0.689\n",
      "Iter=840, avg train loss=0.624, avg val loss=0.610, auc=0.685\n",
      "Iter=845, avg train loss=0.550, avg val loss=0.583, auc=0.688\n",
      "Iter=850, avg train loss=0.565, avg val loss=0.567, auc=0.686\n",
      "Iter=855, avg train loss=0.561, avg val loss=0.571, auc=0.682\n",
      "Iter=860, avg train loss=0.661, avg val loss=0.608, auc=0.693\n",
      "Iter=865, avg train loss=0.592, avg val loss=0.620, auc=0.706\n",
      "Iter=870, avg train loss=0.666, avg val loss=0.613, auc=0.697\n",
      "Iter=875, avg train loss=0.542, avg val loss=0.559, auc=0.697\n",
      "Iter=880, avg train loss=0.603, avg val loss=0.572, auc=0.682\n",
      "Iter=885, avg train loss=0.511, avg val loss=0.546, auc=0.669\n",
      "Iter=890, avg train loss=0.599, avg val loss=0.548, auc=0.680\n",
      "Iter=895, avg train loss=0.513, avg val loss=0.586, auc=0.681\n",
      "Iter=900, avg train loss=0.526, avg val loss=0.669, auc=0.708\n",
      "Iter=905, avg train loss=0.551, avg val loss=0.626, auc=0.691\n",
      "Iter=910, avg train loss=0.573, avg val loss=0.615, auc=0.681\n",
      "Iter=915, avg train loss=0.620, avg val loss=0.588, auc=0.688\n",
      "Iter=920, avg train loss=0.561, avg val loss=0.571, auc=0.681\n",
      "Iter=925, avg train loss=0.639, avg val loss=0.567, auc=0.674\n",
      "Iter=930, avg train loss=0.522, avg val loss=0.583, auc=0.678\n",
      "Iter=935, avg train loss=0.529, avg val loss=0.550, auc=0.664\n",
      "Iter=940, avg train loss=0.584, avg val loss=0.572, auc=0.671\n",
      "Iter=945, avg train loss=0.531, avg val loss=0.612, auc=0.691\n",
      "Iter=950, avg train loss=0.557, avg val loss=0.651, auc=0.707\n",
      "Iter=955, avg train loss=0.539, avg val loss=0.623, auc=0.709\n",
      "Iter=960, avg train loss=0.720, avg val loss=0.666, auc=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=965, avg train loss=0.648, avg val loss=0.606, auc=0.699\n",
      "Iter=970, avg train loss=0.572, avg val loss=0.586, auc=0.690\n",
      "Iter=975, avg train loss=0.499, avg val loss=0.580, auc=0.686\n",
      "Iter=980, avg train loss=0.603, avg val loss=0.558, auc=0.667\n",
      "Iter=985, avg train loss=0.521, avg val loss=0.586, auc=0.662\n",
      "Iter=990, avg train loss=0.588, avg val loss=0.598, auc=0.682\n",
      "Iter=995, avg train loss=0.414, avg val loss=0.573, auc=0.677\n",
      "Iter=1000, avg train loss=0.500, avg val loss=0.543, auc=0.674\n",
      "Iter=1005, avg train loss=0.601, avg val loss=0.548, auc=0.676\n",
      "Iter=1010, avg train loss=0.414, avg val loss=0.568, auc=0.684\n",
      "Iter=1015, avg train loss=0.522, avg val loss=0.567, auc=0.675\n",
      "Iter=1020, avg train loss=0.518, avg val loss=0.589, auc=0.682\n",
      "Iter=1025, avg train loss=0.514, avg val loss=0.628, auc=0.697\n",
      "Iter=1030, avg train loss=0.519, avg val loss=0.579, auc=0.687\n",
      "Iter=1035, avg train loss=0.616, avg val loss=0.603, auc=0.695\n",
      "Iter=1040, avg train loss=0.555, avg val loss=0.588, auc=0.691\n",
      "Iter=1045, avg train loss=0.535, avg val loss=0.546, auc=0.680\n",
      "Iter=1050, avg train loss=0.505, avg val loss=0.560, auc=0.672\n",
      "Iter=1055, avg train loss=0.513, avg val loss=0.561, auc=0.669\n",
      "Iter=1060, avg train loss=0.604, avg val loss=0.527, auc=0.653\n",
      "Iter=1065, avg train loss=0.583, avg val loss=0.537, auc=0.666\n",
      "Iter=1070, avg train loss=0.514, avg val loss=0.536, auc=0.668\n",
      "Iter=1075, avg train loss=0.532, avg val loss=0.569, auc=0.676\n",
      "Iter=1080, avg train loss=0.541, avg val loss=0.556, auc=0.673\n",
      "Iter=1085, avg train loss=0.615, avg val loss=0.591, auc=0.691\n",
      "Iter=1090, avg train loss=0.584, avg val loss=0.616, auc=0.667\n",
      "Iter=1095, avg train loss=0.549, avg val loss=0.613, auc=0.683\n",
      "Iter=1100, avg train loss=0.698, avg val loss=0.570, auc=0.674\n",
      "Iter=1105, avg train loss=0.512, avg val loss=0.543, auc=0.664\n",
      "Iter=1110, avg train loss=0.639, avg val loss=0.551, auc=0.672\n",
      "Iter=1115, avg train loss=0.530, avg val loss=0.565, auc=0.656\n",
      "Iter=1120, avg train loss=0.505, avg val loss=0.613, auc=0.681\n",
      "Iter=1125, avg train loss=0.481, avg val loss=0.604, auc=0.672\n",
      "Iter=1130, avg train loss=0.600, avg val loss=0.617, auc=0.701\n",
      "Iter=1135, avg train loss=0.582, avg val loss=0.573, auc=0.681\n",
      "Iter=1140, avg train loss=0.591, avg val loss=0.549, auc=0.673\n",
      "Iter=1145, avg train loss=0.484, avg val loss=0.548, auc=0.676\n",
      "Iter=1150, avg train loss=0.573, avg val loss=0.572, auc=0.682\n",
      "Iter=1155, avg train loss=0.450, avg val loss=0.559, auc=0.665\n",
      "Iter=1160, avg train loss=0.504, avg val loss=0.588, auc=0.675\n",
      "Iter=1165, avg train loss=0.498, avg val loss=0.602, auc=0.692\n",
      "Iter=1170, avg train loss=0.585, avg val loss=0.636, auc=0.686\n",
      "Iter=1175, avg train loss=0.569, avg val loss=0.585, auc=0.681\n",
      "Iter=1180, avg train loss=0.531, avg val loss=0.572, auc=0.672\n",
      "Iter=1185, avg train loss=0.507, avg val loss=0.558, auc=0.676\n",
      "Iter=1190, avg train loss=0.505, avg val loss=0.544, auc=0.673\n",
      "Iter=1195, avg train loss=0.529, avg val loss=0.541, auc=0.678\n",
      "Iter=1200, avg train loss=0.487, avg val loss=0.564, auc=0.676\n",
      "Iter=1205, avg train loss=0.521, avg val loss=0.553, auc=0.670\n",
      "Iter=1210, avg train loss=0.441, avg val loss=0.588, auc=0.670\n",
      "Iter=1215, avg train loss=0.480, avg val loss=0.624, auc=0.682\n",
      "Iter=1220, avg train loss=0.507, avg val loss=0.614, auc=0.679\n",
      "Iter=1225, avg train loss=0.499, avg val loss=0.577, auc=0.690\n",
      "Iter=1230, avg train loss=0.475, avg val loss=0.558, auc=0.684\n",
      "Iter=1235, avg train loss=0.530, avg val loss=0.556, auc=0.675\n",
      "Iter=1240, avg train loss=0.648, avg val loss=0.576, auc=0.694\n",
      "Iter=1245, avg train loss=0.427, avg val loss=0.567, auc=0.665\n",
      "Iter=1250, avg train loss=0.411, avg val loss=0.580, auc=0.672\n",
      "Iter=1255, avg train loss=0.419, avg val loss=0.600, auc=0.676\n",
      "Iter=1260, avg train loss=0.519, avg val loss=0.603, auc=0.663\n",
      "Iter=1265, avg train loss=0.619, avg val loss=0.705, auc=0.685\n",
      "Iter=1270, avg train loss=0.527, avg val loss=0.736, auc=0.702\n",
      "Iter=1275, avg train loss=0.539, avg val loss=0.662, auc=0.699\n",
      "Iter=1280, avg train loss=0.547, avg val loss=0.595, auc=0.681\n",
      "Iter=1285, avg train loss=0.505, avg val loss=0.574, auc=0.673\n",
      "Iter=1290, avg train loss=0.430, avg val loss=0.549, auc=0.657\n",
      "Iter=1295, avg train loss=0.541, avg val loss=0.562, auc=0.655\n",
      "Iter=1300, avg train loss=0.545, avg val loss=0.553, auc=0.682\n",
      "Iter=1305, avg train loss=0.527, avg val loss=0.584, auc=0.677\n",
      "Iter=1310, avg train loss=0.554, avg val loss=0.570, auc=0.671\n",
      "Iter=1315, avg train loss=0.438, avg val loss=0.567, auc=0.674\n",
      "Iter=1320, avg train loss=0.426, avg val loss=0.544, auc=0.669\n",
      "Iter=1325, avg train loss=0.585, avg val loss=0.548, auc=0.680\n",
      "Iter=1330, avg train loss=0.513, avg val loss=0.546, auc=0.683\n",
      "Iter=1335, avg train loss=0.437, avg val loss=0.587, auc=0.691\n",
      "Iter=1340, avg train loss=0.439, avg val loss=0.592, auc=0.688\n",
      "Iter=1345, avg train loss=0.400, avg val loss=0.567, auc=0.681\n",
      "Iter=1350, avg train loss=0.521, avg val loss=0.547, auc=0.675\n",
      "Iter=1355, avg train loss=0.527, avg val loss=0.550, auc=0.681\n",
      "Iter=1360, avg train loss=0.611, avg val loss=0.569, auc=0.685\n",
      "Iter=1365, avg train loss=0.551, avg val loss=0.683, auc=0.690\n",
      "Iter=1370, avg train loss=0.484, avg val loss=0.618, auc=0.670\n",
      "Iter=1375, avg train loss=0.428, avg val loss=0.684, auc=0.674\n",
      "Iter=1380, avg train loss=0.564, avg val loss=0.659, auc=0.674\n",
      "Iter=1385, avg train loss=0.533, avg val loss=0.659, auc=0.679\n",
      "Iter=1390, avg train loss=0.351, avg val loss=0.563, auc=0.649\n",
      "Iter=1395, avg train loss=0.492, avg val loss=0.580, auc=0.662\n",
      "Iter=1400, avg train loss=0.570, avg val loss=0.605, auc=0.669\n",
      "Iter=1405, avg train loss=0.512, avg val loss=0.625, auc=0.660\n",
      "Iter=1410, avg train loss=0.603, avg val loss=0.621, auc=0.666\n",
      "Iter=1415, avg train loss=0.622, avg val loss=0.666, auc=0.685\n",
      "Iter=1420, avg train loss=0.514, avg val loss=0.705, auc=0.690\n",
      "Iter=1425, avg train loss=0.419, avg val loss=0.659, auc=0.691\n",
      "Iter=1430, avg train loss=0.372, avg val loss=0.575, auc=0.677\n",
      "Iter=1435, avg train loss=0.450, avg val loss=0.581, auc=0.680\n",
      "Iter=1440, avg train loss=0.558, avg val loss=0.578, auc=0.677\n",
      "Iter=1445, avg train loss=0.482, avg val loss=0.545, auc=0.667\n",
      "Iter=1450, avg train loss=0.429, avg val loss=0.566, auc=0.676\n",
      "Iter=1455, avg train loss=0.487, avg val loss=0.558, auc=0.674\n",
      "Iter=1460, avg train loss=0.576, avg val loss=0.554, auc=0.694\n",
      "Iter=1465, avg train loss=0.361, avg val loss=0.570, auc=0.702\n",
      "Iter=1470, avg train loss=0.424, avg val loss=0.560, auc=0.700\n",
      "Iter=1475, avg train loss=0.368, avg val loss=0.588, auc=0.714\n",
      "Iter=1480, avg train loss=0.523, avg val loss=0.587, auc=0.708\n",
      "Iter=1485, avg train loss=0.380, avg val loss=0.597, auc=0.713\n",
      "Iter=1490, avg train loss=0.421, avg val loss=0.611, auc=0.725\n",
      "Best model saved.\n",
      "Iter=1495, avg train loss=0.463, avg val loss=0.543, auc=0.692\n",
      "Iter=1500, avg train loss=0.610, avg val loss=0.550, auc=0.701\n",
      "Iter=1505, avg train loss=0.461, avg val loss=0.534, auc=0.676\n",
      "Iter=1510, avg train loss=0.400, avg val loss=0.547, auc=0.680\n",
      "Iter=1515, avg train loss=0.430, avg val loss=0.555, auc=0.679\n",
      "Iter=1520, avg train loss=0.389, avg val loss=0.569, auc=0.689\n",
      "Iter=1525, avg train loss=0.428, avg val loss=0.685, auc=0.697\n",
      "Iter=1530, avg train loss=0.584, avg val loss=0.748, auc=0.703\n",
      "Iter=1535, avg train loss=0.501, avg val loss=0.613, auc=0.701\n",
      "Iter=1540, avg train loss=0.460, avg val loss=0.563, auc=0.691\n",
      "Iter=1545, avg train loss=0.551, avg val loss=0.536, auc=0.681\n",
      "Iter=1550, avg train loss=0.348, avg val loss=0.540, auc=0.685\n",
      "Iter=1555, avg train loss=0.508, avg val loss=0.573, auc=0.694\n",
      "Iter=1560, avg train loss=0.534, avg val loss=0.570, auc=0.685\n",
      "Iter=1565, avg train loss=0.438, avg val loss=0.551, auc=0.694\n",
      "Iter=1570, avg train loss=0.493, avg val loss=0.537, auc=0.688\n",
      "Iter=1575, avg train loss=0.376, avg val loss=0.632, auc=0.690\n",
      "Iter=1580, avg train loss=0.443, avg val loss=0.623, auc=0.681\n",
      "Iter=1585, avg train loss=0.513, avg val loss=0.632, auc=0.678\n",
      "Iter=1590, avg train loss=0.495, avg val loss=0.583, auc=0.682\n",
      "Iter=1595, avg train loss=0.366, avg val loss=0.616, auc=0.674\n",
      "Iter=1600, avg train loss=0.366, avg val loss=0.604, auc=0.670\n",
      "Iter=1605, avg train loss=0.447, avg val loss=0.580, auc=0.659\n",
      "Iter=1610, avg train loss=0.575, avg val loss=0.565, auc=0.658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1615, avg train loss=0.472, avg val loss=0.565, auc=0.659\n",
      "Iter=1620, avg train loss=0.433, avg val loss=0.584, auc=0.669\n",
      "Iter=1625, avg train loss=0.499, avg val loss=0.685, auc=0.669\n",
      "Iter=1630, avg train loss=0.456, avg val loss=0.614, auc=0.672\n",
      "Iter=1635, avg train loss=0.470, avg val loss=0.536, auc=0.649\n",
      "Iter=1640, avg train loss=0.378, avg val loss=0.541, auc=0.668\n",
      "Iter=1645, avg train loss=0.498, avg val loss=0.543, auc=0.669\n",
      "Iter=1650, avg train loss=0.508, avg val loss=0.672, auc=0.692\n",
      "Iter=1655, avg train loss=0.400, avg val loss=0.722, auc=0.695\n",
      "Iter=1660, avg train loss=0.379, avg val loss=0.606, auc=0.674\n",
      "Iter=1665, avg train loss=0.396, avg val loss=0.599, auc=0.659\n",
      "Iter=1670, avg train loss=0.441, avg val loss=0.617, auc=0.674\n",
      "Iter=1675, avg train loss=0.452, avg val loss=0.560, auc=0.656\n",
      "Iter=1680, avg train loss=0.350, avg val loss=0.546, auc=0.644\n",
      "Iter=1685, avg train loss=0.518, avg val loss=0.531, auc=0.645\n",
      "Iter=1690, avg train loss=0.393, avg val loss=0.541, auc=0.632\n",
      "Iter=1695, avg train loss=0.498, avg val loss=0.568, auc=0.649\n",
      "Iter=1700, avg train loss=0.469, avg val loss=0.582, auc=0.654\n",
      "Iter=1705, avg train loss=0.366, avg val loss=0.667, auc=0.659\n",
      "Iter=1710, avg train loss=0.499, avg val loss=0.728, auc=0.661\n",
      "Iter=1715, avg train loss=0.540, avg val loss=0.671, auc=0.660\n",
      "Iter=1720, avg train loss=0.409, avg val loss=0.607, auc=0.662\n",
      "Iter=1725, avg train loss=0.482, avg val loss=0.560, auc=0.642\n",
      "Iter=1730, avg train loss=0.469, avg val loss=0.591, auc=0.660\n",
      "Iter=1735, avg train loss=0.418, avg val loss=0.560, auc=0.660\n",
      "Iter=1740, avg train loss=0.401, avg val loss=0.535, auc=0.667\n",
      "Iter=1745, avg train loss=0.298, avg val loss=0.566, auc=0.676\n",
      "Iter=1750, avg train loss=0.318, avg val loss=0.546, auc=0.669\n",
      "Iter=1755, avg train loss=0.443, avg val loss=0.540, auc=0.665\n",
      "Iter=1760, avg train loss=0.394, avg val loss=0.589, auc=0.676\n",
      "Iter=1765, avg train loss=0.320, avg val loss=0.591, auc=0.679\n",
      "Iter=1770, avg train loss=0.417, avg val loss=0.554, auc=0.678\n",
      "Iter=1775, avg train loss=0.584, avg val loss=0.599, auc=0.671\n",
      "Iter=1780, avg train loss=0.519, avg val loss=0.567, auc=0.652\n",
      "Iter=1785, avg train loss=0.460, avg val loss=0.562, auc=0.656\n",
      "Iter=1790, avg train loss=0.407, avg val loss=0.599, auc=0.669\n",
      "Iter=1795, avg train loss=0.383, avg val loss=0.654, auc=0.677\n",
      "Iter=1800, avg train loss=0.347, avg val loss=0.565, auc=0.663\n",
      "Iter=1805, avg train loss=0.453, avg val loss=0.568, auc=0.661\n",
      "Iter=1810, avg train loss=0.373, avg val loss=0.562, auc=0.649\n",
      "Iter=1815, avg train loss=0.408, avg val loss=0.550, auc=0.646\n",
      "Iter=1820, avg train loss=0.324, avg val loss=0.536, auc=0.638\n",
      "Iter=1825, avg train loss=0.374, avg val loss=0.571, auc=0.669\n",
      "Iter=1830, avg train loss=0.274, avg val loss=0.585, auc=0.672\n",
      "Iter=1835, avg train loss=0.364, avg val loss=0.592, auc=0.673\n",
      "Iter=1840, avg train loss=0.370, avg val loss=0.621, auc=0.683\n",
      "Iter=1845, avg train loss=0.430, avg val loss=0.588, auc=0.676\n",
      "Iter=1850, avg train loss=0.475, avg val loss=0.613, auc=0.668\n",
      "Iter=1855, avg train loss=0.358, avg val loss=0.561, auc=0.647\n",
      "Iter=1860, avg train loss=0.480, avg val loss=0.557, auc=0.647\n",
      "Iter=1865, avg train loss=0.337, avg val loss=0.632, auc=0.672\n",
      "Iter=1870, avg train loss=0.432, avg val loss=0.687, auc=0.675\n",
      "Iter=1875, avg train loss=0.367, avg val loss=0.666, auc=0.679\n",
      "Iter=1880, avg train loss=0.322, avg val loss=0.598, auc=0.667\n",
      "Iter=1885, avg train loss=0.450, avg val loss=0.551, auc=0.654\n",
      "Iter=1890, avg train loss=0.404, avg val loss=0.550, auc=0.648\n",
      "Iter=1895, avg train loss=0.428, avg val loss=0.584, auc=0.652\n",
      "Iter=1900, avg train loss=0.384, avg val loss=0.589, auc=0.648\n",
      "Iter=1905, avg train loss=0.274, avg val loss=0.599, auc=0.639\n",
      "Iter=1910, avg train loss=0.494, avg val loss=0.553, auc=0.631\n",
      "Iter=1915, avg train loss=0.339, avg val loss=0.569, auc=0.639\n",
      "Iter=1920, avg train loss=0.425, avg val loss=0.560, auc=0.637\n",
      "Iter=1925, avg train loss=0.348, avg val loss=0.593, auc=0.662\n",
      "Iter=1930, avg train loss=0.321, avg val loss=0.561, auc=0.648\n",
      "Iter=1935, avg train loss=0.402, avg val loss=0.537, auc=0.629\n",
      "Iter=1940, avg train loss=0.490, avg val loss=0.575, auc=0.649\n",
      "Iter=1945, avg train loss=0.402, avg val loss=0.590, auc=0.650\n",
      "Iter=1950, avg train loss=0.477, avg val loss=0.670, auc=0.654\n",
      "Iter=1955, avg train loss=0.440, avg val loss=0.680, auc=0.651\n",
      "Iter=1960, avg train loss=0.565, avg val loss=0.639, auc=0.664\n",
      "Iter=1965, avg train loss=0.488, avg val loss=0.591, auc=0.651\n",
      "Iter=1970, avg train loss=0.334, avg val loss=0.581, auc=0.633\n",
      "Iter=1975, avg train loss=0.431, avg val loss=0.550, auc=0.607\n",
      "Iter=1980, avg train loss=0.332, avg val loss=0.566, auc=0.611\n",
      "Iter=1985, avg train loss=0.414, avg val loss=0.588, auc=0.606\n",
      "Iter=1990, avg train loss=0.380, avg val loss=0.630, auc=0.615\n",
      "Iter=1995, avg train loss=0.430, avg val loss=0.634, auc=0.611\n",
      "Iter=2000, avg train loss=0.430, avg val loss=0.680, auc=0.613\n",
      "Iter=2005, avg train loss=0.306, avg val loss=0.746, auc=0.627\n",
      "Iter=2010, avg train loss=0.347, avg val loss=0.774, auc=0.653\n",
      "Iter=2015, avg train loss=0.467, avg val loss=0.762, auc=0.656\n",
      "Iter=2020, avg train loss=0.364, avg val loss=0.618, auc=0.646\n",
      "Iter=2025, avg train loss=0.520, avg val loss=0.571, auc=0.622\n",
      "Iter=2030, avg train loss=0.412, avg val loss=0.588, auc=0.636\n",
      "Iter=2035, avg train loss=0.420, avg val loss=0.689, auc=0.648\n",
      "Iter=2040, avg train loss=0.308, avg val loss=0.679, auc=0.652\n",
      "Iter=2045, avg train loss=0.447, avg val loss=0.605, auc=0.647\n",
      "Iter=2050, avg train loss=0.304, avg val loss=0.580, auc=0.639\n",
      "Iter=2055, avg train loss=0.309, avg val loss=0.577, auc=0.647\n",
      "Iter=2060, avg train loss=0.443, avg val loss=0.607, auc=0.647\n",
      "Iter=2065, avg train loss=0.404, avg val loss=0.587, auc=0.650\n",
      "Iter=2070, avg train loss=0.413, avg val loss=0.616, auc=0.659\n",
      "Iter=2075, avg train loss=0.505, avg val loss=0.692, auc=0.675\n",
      "Iter=2080, avg train loss=0.497, avg val loss=0.631, auc=0.671\n",
      "Iter=2085, avg train loss=0.347, avg val loss=0.567, auc=0.649\n",
      "Iter=2090, avg train loss=0.427, avg val loss=0.574, auc=0.651\n",
      "Iter=2095, avg train loss=0.351, avg val loss=0.583, auc=0.642\n",
      "Iter=2100, avg train loss=0.341, avg val loss=0.574, auc=0.655\n",
      "Iter=2105, avg train loss=0.484, avg val loss=0.566, auc=0.647\n",
      "Iter=2110, avg train loss=0.344, avg val loss=0.651, auc=0.653\n",
      "Iter=2115, avg train loss=0.441, avg val loss=0.607, auc=0.650\n",
      "Iter=2120, avg train loss=0.310, avg val loss=0.616, auc=0.657\n",
      "Iter=2125, avg train loss=0.249, avg val loss=0.641, auc=0.647\n",
      "Iter=2130, avg train loss=0.351, avg val loss=0.680, auc=0.655\n",
      "Iter=2135, avg train loss=0.427, avg val loss=0.733, auc=0.671\n",
      "Iter=2140, avg train loss=0.317, avg val loss=0.749, auc=0.672\n",
      "Iter=2145, avg train loss=0.463, avg val loss=0.582, auc=0.675\n",
      "Iter=2150, avg train loss=0.393, avg val loss=0.601, auc=0.673\n",
      "Iter=2155, avg train loss=0.407, avg val loss=0.558, auc=0.657\n",
      "Iter=2160, avg train loss=0.415, avg val loss=0.551, auc=0.644\n",
      "Iter=2165, avg train loss=0.433, avg val loss=0.583, auc=0.645\n",
      "Iter=2170, avg train loss=0.354, avg val loss=0.613, auc=0.648\n",
      "Iter=2175, avg train loss=0.434, avg val loss=0.590, auc=0.631\n",
      "Iter=2180, avg train loss=0.422, avg val loss=0.594, auc=0.637\n",
      "Iter=2185, avg train loss=0.461, avg val loss=0.585, auc=0.646\n",
      "Iter=2190, avg train loss=0.299, avg val loss=0.605, auc=0.655\n",
      "Iter=2195, avg train loss=0.307, avg val loss=0.644, auc=0.667\n",
      "Iter=2200, avg train loss=0.294, avg val loss=0.624, auc=0.652\n",
      "Iter=2205, avg train loss=0.412, avg val loss=0.671, auc=0.665\n",
      "Iter=2210, avg train loss=0.367, avg val loss=0.669, auc=0.672\n",
      "Iter=2215, avg train loss=0.322, avg val loss=0.616, auc=0.649\n",
      "Iter=2220, avg train loss=0.348, avg val loss=0.573, auc=0.630\n",
      "Iter=2225, avg train loss=0.305, avg val loss=0.559, auc=0.631\n",
      "Iter=2230, avg train loss=0.405, avg val loss=0.587, auc=0.651\n",
      "Iter=2235, avg train loss=0.260, avg val loss=0.667, auc=0.681\n",
      "Iter=2240, avg train loss=0.460, avg val loss=0.770, auc=0.693\n",
      "Iter=2245, avg train loss=0.286, avg val loss=0.566, auc=0.660\n",
      "Iter=2250, avg train loss=0.414, avg val loss=0.570, auc=0.654\n",
      "Iter=2255, avg train loss=0.358, avg val loss=0.580, auc=0.658\n",
      "Iter=2260, avg train loss=0.430, avg val loss=0.610, auc=0.669\n",
      "Iter=2265, avg train loss=0.452, avg val loss=0.582, auc=0.676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=2270, avg train loss=0.355, avg val loss=0.549, auc=0.648\n",
      "Iter=2275, avg train loss=0.271, avg val loss=0.584, auc=0.651\n",
      "Iter=2280, avg train loss=0.283, avg val loss=0.572, auc=0.652\n",
      "Iter=2285, avg train loss=0.273, avg val loss=0.592, auc=0.646\n",
      "Iter=2290, avg train loss=0.270, avg val loss=0.647, auc=0.668\n",
      "Iter=2295, avg train loss=0.321, avg val loss=0.682, auc=0.666\n",
      "Iter=2300, avg train loss=0.328, avg val loss=0.814, auc=0.666\n",
      "Iter=2305, avg train loss=0.352, avg val loss=0.611, auc=0.636\n",
      "Iter=2310, avg train loss=0.190, avg val loss=0.658, auc=0.638\n",
      "Iter=2315, avg train loss=0.301, avg val loss=0.668, auc=0.620\n",
      "Iter=2320, avg train loss=0.394, avg val loss=0.607, auc=0.645\n",
      "Iter=2325, avg train loss=0.299, avg val loss=0.575, auc=0.640\n",
      "Iter=2330, avg train loss=0.363, avg val loss=0.575, auc=0.633\n",
      "Iter=2335, avg train loss=0.221, avg val loss=0.623, auc=0.648\n",
      "Iter=2340, avg train loss=0.234, avg val loss=0.596, auc=0.638\n",
      "Iter=2345, avg train loss=0.387, avg val loss=0.678, auc=0.664\n",
      "Iter=2350, avg train loss=0.233, avg val loss=0.684, auc=0.673\n",
      "Iter=2355, avg train loss=0.401, avg val loss=0.740, auc=0.680\n",
      "Iter=2360, avg train loss=0.325, avg val loss=0.660, auc=0.669\n",
      "Iter=2365, avg train loss=0.358, avg val loss=0.589, auc=0.664\n",
      "Iter=2370, avg train loss=0.244, avg val loss=0.576, auc=0.645\n",
      "Iter=2375, avg train loss=0.270, avg val loss=0.584, auc=0.645\n",
      "Iter=2380, avg train loss=0.298, avg val loss=0.596, auc=0.652\n",
      "Iter=2385, avg train loss=0.351, avg val loss=0.593, auc=0.644\n",
      "Iter=2390, avg train loss=0.287, avg val loss=0.607, auc=0.633\n",
      "Iter=2395, avg train loss=0.206, avg val loss=0.645, auc=0.647\n",
      "Iter=2400, avg train loss=0.232, avg val loss=0.694, auc=0.658\n",
      "Iter=2405, avg train loss=0.246, avg val loss=0.624, auc=0.666\n",
      "Iter=2410, avg train loss=0.418, avg val loss=0.672, auc=0.654\n",
      "Iter=2415, avg train loss=0.401, avg val loss=0.600, auc=0.652\n",
      "Iter=2420, avg train loss=0.253, avg val loss=0.613, auc=0.648\n",
      "Iter=2425, avg train loss=0.409, avg val loss=0.651, auc=0.649\n",
      "Iter=2430, avg train loss=0.275, avg val loss=0.659, auc=0.645\n",
      "Iter=2435, avg train loss=0.304, avg val loss=0.641, auc=0.634\n",
      "Iter=2440, avg train loss=0.500, avg val loss=0.666, auc=0.650\n",
      "Iter=2445, avg train loss=0.306, avg val loss=0.655, auc=0.642\n",
      "Iter=2450, avg train loss=0.254, avg val loss=0.606, auc=0.647\n",
      "Iter=2455, avg train loss=0.296, avg val loss=0.666, auc=0.660\n",
      "Iter=2460, avg train loss=0.355, avg val loss=0.605, auc=0.662\n",
      "Iter=2465, avg train loss=0.234, avg val loss=0.578, auc=0.650\n",
      "Iter=2470, avg train loss=0.320, avg val loss=0.610, auc=0.642\n",
      "Iter=2475, avg train loss=0.235, avg val loss=0.591, auc=0.635\n",
      "Iter=2480, avg train loss=0.309, avg val loss=0.567, auc=0.650\n",
      "Iter=2485, avg train loss=0.307, avg val loss=0.659, auc=0.662\n",
      "Iter=2490, avg train loss=0.283, avg val loss=0.717, auc=0.663\n",
      "Iter=2495, avg train loss=0.395, avg val loss=0.645, auc=0.660\n",
      "Iter=2500, avg train loss=0.286, avg val loss=0.591, auc=0.663\n",
      "Iter=2505, avg train loss=0.183, avg val loss=0.567, auc=0.653\n",
      "Iter=2510, avg train loss=0.236, avg val loss=0.564, auc=0.644\n",
      "Iter=2515, avg train loss=0.351, avg val loss=0.739, auc=0.674\n",
      "Iter=2520, avg train loss=0.374, avg val loss=0.721, auc=0.677\n",
      "Iter=2525, avg train loss=0.246, avg val loss=0.629, auc=0.669\n",
      "Iter=2530, avg train loss=0.278, avg val loss=0.645, auc=0.674\n",
      "Iter=2535, avg train loss=0.173, avg val loss=0.621, auc=0.664\n",
      "Iter=2540, avg train loss=0.288, avg val loss=0.579, auc=0.665\n",
      "Iter=2545, avg train loss=0.300, avg val loss=0.622, auc=0.674\n",
      "Iter=2550, avg train loss=0.286, avg val loss=0.658, auc=0.670\n",
      "Iter=2555, avg train loss=0.315, avg val loss=0.623, auc=0.675\n",
      "Iter=2560, avg train loss=0.234, avg val loss=0.632, auc=0.664\n",
      "Iter=2565, avg train loss=0.404, avg val loss=0.565, auc=0.654\n",
      "Iter=2570, avg train loss=0.239, avg val loss=0.580, auc=0.652\n",
      "Iter=2575, avg train loss=0.381, avg val loss=0.581, auc=0.652\n",
      "Iter=2580, avg train loss=0.175, avg val loss=0.654, auc=0.669\n",
      "Iter=2585, avg train loss=0.228, avg val loss=0.624, auc=0.664\n",
      "Iter=2590, avg train loss=0.279, avg val loss=0.695, auc=0.667\n",
      "Iter=2595, avg train loss=0.229, avg val loss=0.630, auc=0.678\n",
      "Iter=2600, avg train loss=0.317, avg val loss=0.604, auc=0.660\n",
      "Iter=2605, avg train loss=0.174, avg val loss=0.574, auc=0.651\n",
      "Iter=2610, avg train loss=0.337, avg val loss=0.613, auc=0.667\n",
      "Iter=2615, avg train loss=0.288, avg val loss=0.639, auc=0.655\n",
      "Iter=2620, avg train loss=0.243, avg val loss=0.628, auc=0.637\n",
      "Iter=2625, avg train loss=0.190, avg val loss=0.631, auc=0.635\n",
      "Iter=2630, avg train loss=0.374, avg val loss=0.567, auc=0.631\n",
      "Iter=2635, avg train loss=0.219, avg val loss=0.593, auc=0.637\n",
      "Iter=2640, avg train loss=0.250, avg val loss=0.621, auc=0.622\n",
      "Iter=2645, avg train loss=0.445, avg val loss=0.678, auc=0.634\n",
      "Iter=2650, avg train loss=0.530, avg val loss=0.991, auc=0.671\n",
      "Iter=2655, avg train loss=0.459, avg val loss=0.732, auc=0.674\n",
      "Iter=2660, avg train loss=0.225, avg val loss=0.601, auc=0.654\n",
      "Iter=2665, avg train loss=0.391, avg val loss=0.605, auc=0.672\n",
      "Iter=2670, avg train loss=0.271, avg val loss=0.652, auc=0.674\n",
      "Iter=2675, avg train loss=0.202, avg val loss=0.648, auc=0.677\n",
      "Iter=2680, avg train loss=0.292, avg val loss=0.633, auc=0.680\n",
      "Iter=2685, avg train loss=0.241, avg val loss=0.607, auc=0.675\n",
      "Iter=2690, avg train loss=0.205, avg val loss=0.600, auc=0.675\n",
      "Iter=2695, avg train loss=0.192, avg val loss=0.598, auc=0.670\n",
      "Iter=2700, avg train loss=0.257, avg val loss=0.561, auc=0.673\n",
      "Iter=2705, avg train loss=0.341, avg val loss=0.584, auc=0.675\n",
      "Iter=2710, avg train loss=0.268, avg val loss=0.577, auc=0.658\n",
      "Iter=2715, avg train loss=0.308, avg val loss=0.592, auc=0.656\n",
      "Iter=2720, avg train loss=0.232, avg val loss=0.648, auc=0.674\n",
      "Iter=2725, avg train loss=0.248, avg val loss=0.679, auc=0.687\n",
      "Iter=2730, avg train loss=0.241, avg val loss=0.660, auc=0.687\n",
      "Iter=2735, avg train loss=0.645, avg val loss=0.645, auc=0.683\n",
      "Iter=2740, avg train loss=0.303, avg val loss=0.623, auc=0.643\n",
      "Iter=2745, avg train loss=0.460, avg val loss=0.628, auc=0.612\n",
      "Iter=2750, avg train loss=0.207, avg val loss=0.697, auc=0.619\n",
      "Iter=2755, avg train loss=0.262, avg val loss=0.725, auc=0.631\n",
      "Iter=2760, avg train loss=0.303, avg val loss=0.771, auc=0.646\n",
      "Iter=2765, avg train loss=0.298, avg val loss=0.850, auc=0.646\n",
      "Iter=2770, avg train loss=0.256, avg val loss=0.717, auc=0.622\n",
      "Iter=2775, avg train loss=0.239, avg val loss=0.611, auc=0.600\n",
      "Iter=2780, avg train loss=0.347, avg val loss=0.623, auc=0.595\n",
      "Iter=2785, avg train loss=0.403, avg val loss=0.643, auc=0.609\n",
      "Iter=2790, avg train loss=0.322, avg val loss=0.713, auc=0.645\n",
      "Iter=2795, avg train loss=0.276, avg val loss=0.882, auc=0.672\n",
      "Iter=2800, avg train loss=0.306, avg val loss=0.664, auc=0.655\n",
      "Iter=2805, avg train loss=0.196, avg val loss=0.619, auc=0.638\n",
      "Iter=2810, avg train loss=0.168, avg val loss=0.639, auc=0.625\n",
      "Iter=2815, avg train loss=0.431, avg val loss=0.790, auc=0.663\n",
      "Iter=2820, avg train loss=0.252, avg val loss=0.652, auc=0.671\n",
      "Iter=2825, avg train loss=0.204, avg val loss=0.620, auc=0.676\n",
      "Iter=2830, avg train loss=0.208, avg val loss=0.607, auc=0.670\n",
      "Iter=2835, avg train loss=0.283, avg val loss=0.693, auc=0.672\n",
      "Iter=2840, avg train loss=0.360, avg val loss=0.619, auc=0.670\n",
      "Iter=2845, avg train loss=0.192, avg val loss=0.606, auc=0.672\n",
      "Iter=2850, avg train loss=0.167, avg val loss=0.626, auc=0.674\n",
      "Iter=2855, avg train loss=0.384, avg val loss=0.584, auc=0.647\n",
      "Iter=2860, avg train loss=0.449, avg val loss=0.585, auc=0.657\n",
      "Iter=2865, avg train loss=0.161, avg val loss=0.620, auc=0.663\n",
      "Iter=2870, avg train loss=0.189, avg val loss=0.641, auc=0.674\n",
      "Iter=2875, avg train loss=0.282, avg val loss=0.744, auc=0.675\n",
      "Iter=2880, avg train loss=0.428, avg val loss=0.563, auc=0.657\n",
      "Iter=2885, avg train loss=0.259, avg val loss=0.623, auc=0.658\n",
      "Iter=2890, avg train loss=0.355, avg val loss=0.594, auc=0.644\n",
      "Iter=2895, avg train loss=0.353, avg val loss=0.622, auc=0.633\n",
      "Iter=2900, avg train loss=0.433, avg val loss=0.644, auc=0.622\n",
      "Iter=2905, avg train loss=0.288, avg val loss=0.712, auc=0.638\n",
      "Iter=2910, avg train loss=0.309, avg val loss=0.744, auc=0.646\n",
      "Iter=2915, avg train loss=0.396, avg val loss=0.616, auc=0.648\n",
      "Iter=2920, avg train loss=0.241, avg val loss=0.627, auc=0.659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=2925, avg train loss=0.213, avg val loss=0.559, auc=0.635\n",
      "Iter=2930, avg train loss=0.227, avg val loss=0.614, auc=0.651\n",
      "Iter=2935, avg train loss=0.295, avg val loss=0.698, auc=0.651\n",
      "Iter=2940, avg train loss=0.341, avg val loss=0.701, auc=0.657\n",
      "Iter=2945, avg train loss=0.316, avg val loss=0.626, auc=0.650\n",
      "Iter=2950, avg train loss=0.216, avg val loss=0.582, auc=0.654\n",
      "Iter=2955, avg train loss=0.247, avg val loss=0.560, auc=0.647\n",
      "Iter=2960, avg train loss=0.233, avg val loss=0.570, auc=0.662\n",
      "Iter=2965, avg train loss=0.184, avg val loss=0.592, auc=0.643\n",
      "Iter=2970, avg train loss=0.260, avg val loss=0.616, auc=0.651\n",
      "Iter=2975, avg train loss=0.268, avg val loss=0.679, auc=0.669\n",
      "Iter=2980, avg train loss=0.247, avg val loss=0.591, auc=0.657\n",
      "Iter=2985, avg train loss=0.183, avg val loss=0.627, auc=0.662\n",
      "Iter=2990, avg train loss=0.209, avg val loss=0.624, auc=0.660\n",
      "Iter=2995, avg train loss=0.236, avg val loss=0.631, auc=0.656\n",
      "Iter=3000, avg train loss=0.229, avg val loss=0.619, auc=0.651\n",
      "Iter=3005, avg train loss=0.204, avg val loss=0.601, auc=0.647\n",
      "Iter=3010, avg train loss=0.191, avg val loss=0.677, auc=0.666\n",
      "Iter=3015, avg train loss=0.189, avg val loss=0.686, auc=0.675\n",
      "Iter=3020, avg train loss=0.183, avg val loss=0.665, auc=0.672\n",
      "Iter=3025, avg train loss=0.184, avg val loss=0.589, auc=0.667\n",
      "Iter=3030, avg train loss=0.165, avg val loss=0.612, auc=0.674\n",
      "Iter=3035, avg train loss=0.165, avg val loss=0.625, auc=0.673\n",
      "Iter=3040, avg train loss=0.276, avg val loss=0.647, auc=0.668\n",
      "Iter=3045, avg train loss=0.194, avg val loss=0.604, auc=0.648\n",
      "Iter=3050, avg train loss=0.279, avg val loss=0.630, auc=0.658\n",
      "Iter=3055, avg train loss=0.155, avg val loss=0.661, auc=0.654\n",
      "Iter=3060, avg train loss=0.200, avg val loss=0.644, auc=0.645\n",
      "Iter=3065, avg train loss=0.306, avg val loss=0.618, auc=0.647\n",
      "Iter=3070, avg train loss=0.218, avg val loss=0.665, auc=0.662\n",
      "Iter=3075, avg train loss=0.229, avg val loss=0.615, auc=0.666\n",
      "Iter=3080, avg train loss=0.345, avg val loss=0.641, auc=0.676\n",
      "Iter=3085, avg train loss=0.247, avg val loss=0.632, auc=0.672\n",
      "Iter=3090, avg train loss=0.133, avg val loss=0.608, auc=0.673\n",
      "Iter=3095, avg train loss=0.215, avg val loss=0.582, auc=0.656\n",
      "Iter=3100, avg train loss=0.141, avg val loss=0.644, auc=0.667\n",
      "Iter=3105, avg train loss=0.352, avg val loss=0.584, auc=0.660\n",
      "Iter=3110, avg train loss=0.204, avg val loss=0.623, auc=0.678\n",
      "Iter=3115, avg train loss=0.133, avg val loss=0.637, auc=0.685\n",
      "Iter=3120, avg train loss=0.284, avg val loss=0.595, auc=0.656\n",
      "Iter=3125, avg train loss=0.227, avg val loss=0.640, auc=0.643\n",
      "Iter=3130, avg train loss=0.160, avg val loss=0.620, auc=0.654\n",
      "Iter=3135, avg train loss=0.233, avg val loss=0.681, auc=0.667\n",
      "Iter=3140, avg train loss=0.163, avg val loss=0.780, auc=0.673\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.696, max-score-based AUC=0.687\n",
      "\n",
      "\n",
      "========== Fold 5 ==========\n",
      "Test AUC at start=0.597, max-score-based AUC=0.598\n",
      "Iter=5, avg train loss=0.901, avg val loss=0.515, auc=0.693\n",
      "Best model saved.\n",
      "Iter=10, avg train loss=0.599, avg val loss=0.514, auc=0.707\n",
      "Best model saved.\n",
      "Iter=15, avg train loss=0.870, avg val loss=0.520, auc=0.683\n",
      "Iter=20, avg train loss=0.987, avg val loss=0.513, auc=0.709\n",
      "Best model saved.\n",
      "Iter=25, avg train loss=0.843, avg val loss=0.515, auc=0.714\n",
      "Best model saved.\n",
      "Iter=30, avg train loss=0.683, avg val loss=0.520, auc=0.706\n",
      "Iter=35, avg train loss=0.818, avg val loss=0.529, auc=0.688\n",
      "Iter=40, avg train loss=0.728, avg val loss=0.543, auc=0.682\n",
      "Iter=45, avg train loss=0.673, avg val loss=0.538, auc=0.683\n",
      "Iter=50, avg train loss=0.587, avg val loss=0.539, auc=0.697\n",
      "Iter=55, avg train loss=0.759, avg val loss=0.534, auc=0.714\n",
      "Iter=60, avg train loss=0.808, avg val loss=0.555, auc=0.720\n",
      "Best model saved.\n",
      "Iter=65, avg train loss=0.667, avg val loss=0.543, auc=0.724\n",
      "Best model saved.\n",
      "Iter=70, avg train loss=0.692, avg val loss=0.556, auc=0.728\n",
      "Best model saved.\n",
      "Iter=75, avg train loss=0.734, avg val loss=0.558, auc=0.738\n",
      "Best model saved.\n",
      "Iter=80, avg train loss=0.680, avg val loss=0.577, auc=0.729\n",
      "Iter=85, avg train loss=0.704, avg val loss=0.597, auc=0.713\n",
      "Iter=90, avg train loss=0.660, avg val loss=0.589, auc=0.726\n",
      "Iter=95, avg train loss=0.638, avg val loss=0.581, auc=0.723\n",
      "Iter=100, avg train loss=0.664, avg val loss=0.556, auc=0.729\n",
      "Iter=105, avg train loss=0.646, avg val loss=0.551, auc=0.724\n",
      "Iter=110, avg train loss=0.689, avg val loss=0.566, auc=0.729\n",
      "Iter=115, avg train loss=0.675, avg val loss=0.561, auc=0.717\n",
      "Iter=120, avg train loss=0.690, avg val loss=0.557, auc=0.721\n",
      "Iter=125, avg train loss=0.621, avg val loss=0.573, auc=0.733\n",
      "Iter=130, avg train loss=0.719, avg val loss=0.595, auc=0.721\n",
      "Iter=135, avg train loss=0.570, avg val loss=0.570, auc=0.733\n",
      "Iter=140, avg train loss=0.690, avg val loss=0.593, auc=0.742\n",
      "Best model saved.\n",
      "Iter=145, avg train loss=0.681, avg val loss=0.618, auc=0.736\n",
      "Iter=150, avg train loss=0.649, avg val loss=0.570, auc=0.740\n",
      "Iter=155, avg train loss=0.672, avg val loss=0.588, auc=0.735\n",
      "Iter=160, avg train loss=0.645, avg val loss=0.575, auc=0.718\n",
      "Iter=165, avg train loss=0.618, avg val loss=0.593, auc=0.699\n",
      "Iter=170, avg train loss=0.659, avg val loss=0.555, auc=0.705\n",
      "Iter=175, avg train loss=0.684, avg val loss=0.567, auc=0.694\n",
      "Iter=180, avg train loss=0.645, avg val loss=0.562, auc=0.692\n",
      "Iter=185, avg train loss=0.733, avg val loss=0.578, auc=0.682\n",
      "Iter=190, avg train loss=0.703, avg val loss=0.573, auc=0.671\n",
      "Iter=195, avg train loss=0.625, avg val loss=0.563, auc=0.692\n",
      "Iter=200, avg train loss=0.627, avg val loss=0.579, auc=0.699\n",
      "Iter=205, avg train loss=0.741, avg val loss=0.578, auc=0.707\n",
      "Iter=210, avg train loss=0.591, avg val loss=0.557, auc=0.719\n",
      "Iter=215, avg train loss=0.575, avg val loss=0.567, auc=0.716\n",
      "Iter=220, avg train loss=0.718, avg val loss=0.561, auc=0.722\n",
      "Iter=225, avg train loss=0.606, avg val loss=0.555, auc=0.721\n",
      "Iter=230, avg train loss=0.648, avg val loss=0.559, auc=0.716\n",
      "Iter=235, avg train loss=0.665, avg val loss=0.563, auc=0.721\n",
      "Iter=240, avg train loss=0.689, avg val loss=0.570, auc=0.713\n",
      "Iter=245, avg train loss=0.669, avg val loss=0.603, auc=0.711\n",
      "Iter=250, avg train loss=0.671, avg val loss=0.576, auc=0.714\n",
      "Iter=255, avg train loss=0.636, avg val loss=0.573, auc=0.710\n",
      "Iter=260, avg train loss=0.633, avg val loss=0.582, auc=0.697\n",
      "Iter=265, avg train loss=0.625, avg val loss=0.587, auc=0.698\n",
      "Iter=270, avg train loss=0.649, avg val loss=0.603, auc=0.705\n",
      "Iter=275, avg train loss=0.666, avg val loss=0.568, auc=0.713\n",
      "Iter=280, avg train loss=0.669, avg val loss=0.601, auc=0.707\n",
      "Iter=285, avg train loss=0.573, avg val loss=0.630, auc=0.708\n",
      "Iter=290, avg train loss=0.567, avg val loss=0.618, auc=0.715\n",
      "Iter=295, avg train loss=0.719, avg val loss=0.599, auc=0.720\n",
      "Iter=300, avg train loss=0.621, avg val loss=0.573, auc=0.720\n",
      "Iter=305, avg train loss=0.639, avg val loss=0.574, auc=0.714\n",
      "Iter=310, avg train loss=0.657, avg val loss=0.574, auc=0.724\n",
      "Iter=315, avg train loss=0.615, avg val loss=0.566, auc=0.710\n",
      "Iter=320, avg train loss=0.662, avg val loss=0.578, auc=0.711\n",
      "Iter=325, avg train loss=0.640, avg val loss=0.605, auc=0.715\n",
      "Iter=330, avg train loss=0.581, avg val loss=0.622, auc=0.702\n",
      "Iter=335, avg train loss=0.669, avg val loss=0.574, auc=0.703\n",
      "Iter=340, avg train loss=0.639, avg val loss=0.565, auc=0.710\n",
      "Iter=345, avg train loss=0.688, avg val loss=0.571, auc=0.708\n",
      "Iter=350, avg train loss=0.636, avg val loss=0.610, auc=0.706\n",
      "Iter=355, avg train loss=0.615, avg val loss=0.621, auc=0.708\n",
      "Iter=360, avg train loss=0.648, avg val loss=0.591, auc=0.697\n",
      "Iter=365, avg train loss=0.592, avg val loss=0.639, auc=0.695\n",
      "Iter=370, avg train loss=0.653, avg val loss=0.657, auc=0.688\n",
      "Iter=375, avg train loss=0.732, avg val loss=0.685, auc=0.685\n",
      "Iter=380, avg train loss=0.656, avg val loss=0.637, auc=0.675\n",
      "Iter=385, avg train loss=0.672, avg val loss=0.628, auc=0.671\n",
      "Iter=390, avg train loss=0.604, avg val loss=0.648, auc=0.671\n",
      "Iter=395, avg train loss=0.645, avg val loss=0.625, auc=0.686\n",
      "Iter=400, avg train loss=0.675, avg val loss=0.633, auc=0.672\n",
      "Iter=405, avg train loss=0.625, avg val loss=0.628, auc=0.674\n",
      "Iter=410, avg train loss=0.555, avg val loss=0.625, auc=0.671\n",
      "Iter=415, avg train loss=0.648, avg val loss=0.642, auc=0.674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=420, avg train loss=0.627, avg val loss=0.638, auc=0.676\n",
      "Iter=425, avg train loss=0.614, avg val loss=0.626, auc=0.678\n",
      "Iter=430, avg train loss=0.621, avg val loss=0.620, auc=0.661\n",
      "Iter=435, avg train loss=0.565, avg val loss=0.589, auc=0.662\n",
      "Iter=440, avg train loss=0.627, avg val loss=0.570, auc=0.665\n",
      "Iter=445, avg train loss=0.584, avg val loss=0.595, auc=0.677\n",
      "Iter=450, avg train loss=0.625, avg val loss=0.631, auc=0.682\n",
      "Iter=455, avg train loss=0.603, avg val loss=0.639, auc=0.699\n",
      "Iter=460, avg train loss=0.690, avg val loss=0.635, auc=0.696\n",
      "Iter=465, avg train loss=0.624, avg val loss=0.636, auc=0.677\n",
      "Iter=470, avg train loss=0.605, avg val loss=0.618, auc=0.674\n",
      "Iter=475, avg train loss=0.638, avg val loss=0.590, auc=0.668\n",
      "Iter=480, avg train loss=0.677, avg val loss=0.578, auc=0.666\n",
      "Iter=485, avg train loss=0.655, avg val loss=0.590, auc=0.670\n",
      "Iter=490, avg train loss=0.596, avg val loss=0.595, auc=0.672\n",
      "Iter=495, avg train loss=0.694, avg val loss=0.618, auc=0.673\n",
      "Iter=500, avg train loss=0.588, avg val loss=0.644, auc=0.650\n",
      "Iter=505, avg train loss=0.574, avg val loss=0.679, auc=0.635\n",
      "Iter=510, avg train loss=0.686, avg val loss=0.627, auc=0.646\n",
      "Iter=515, avg train loss=0.651, avg val loss=0.583, auc=0.659\n",
      "Iter=520, avg train loss=0.680, avg val loss=0.635, auc=0.652\n",
      "Iter=525, avg train loss=0.639, avg val loss=0.668, auc=0.657\n",
      "Iter=530, avg train loss=0.654, avg val loss=0.682, auc=0.669\n",
      "Iter=535, avg train loss=0.574, avg val loss=0.717, auc=0.676\n",
      "Iter=540, avg train loss=0.714, avg val loss=0.613, auc=0.667\n",
      "Iter=545, avg train loss=0.572, avg val loss=0.600, auc=0.644\n",
      "Iter=550, avg train loss=0.599, avg val loss=0.601, auc=0.650\n",
      "Iter=555, avg train loss=0.616, avg val loss=0.586, auc=0.666\n",
      "Iter=560, avg train loss=0.514, avg val loss=0.552, auc=0.671\n",
      "Iter=565, avg train loss=0.683, avg val loss=0.533, auc=0.677\n",
      "Iter=570, avg train loss=0.606, avg val loss=0.569, auc=0.680\n",
      "Iter=575, avg train loss=0.607, avg val loss=0.567, auc=0.688\n",
      "Iter=580, avg train loss=0.643, avg val loss=0.572, auc=0.682\n",
      "Iter=585, avg train loss=0.609, avg val loss=0.584, auc=0.665\n",
      "Iter=590, avg train loss=0.603, avg val loss=0.596, auc=0.647\n",
      "Iter=595, avg train loss=0.632, avg val loss=0.620, auc=0.641\n",
      "Iter=600, avg train loss=0.593, avg val loss=0.626, auc=0.624\n",
      "Iter=605, avg train loss=0.704, avg val loss=0.669, auc=0.616\n",
      "Iter=610, avg train loss=0.531, avg val loss=0.678, auc=0.629\n",
      "Iter=615, avg train loss=0.708, avg val loss=0.713, auc=0.606\n",
      "Iter=620, avg train loss=0.575, avg val loss=0.703, auc=0.612\n",
      "Iter=625, avg train loss=0.693, avg val loss=0.676, auc=0.616\n",
      "Iter=630, avg train loss=0.588, avg val loss=0.651, auc=0.629\n",
      "Iter=635, avg train loss=0.621, avg val loss=0.627, auc=0.650\n",
      "Iter=640, avg train loss=0.636, avg val loss=0.641, auc=0.669\n",
      "Iter=645, avg train loss=0.522, avg val loss=0.614, auc=0.694\n",
      "Iter=650, avg train loss=0.590, avg val loss=0.581, auc=0.704\n",
      "Iter=655, avg train loss=0.486, avg val loss=0.563, auc=0.710\n",
      "Iter=660, avg train loss=0.599, avg val loss=0.558, auc=0.700\n",
      "Iter=665, avg train loss=0.577, avg val loss=0.558, auc=0.711\n",
      "Iter=670, avg train loss=0.684, avg val loss=0.555, auc=0.712\n",
      "Iter=675, avg train loss=0.541, avg val loss=0.582, auc=0.701\n",
      "Iter=680, avg train loss=0.574, avg val loss=0.569, auc=0.699\n",
      "Iter=685, avg train loss=0.648, avg val loss=0.571, auc=0.689\n",
      "Iter=690, avg train loss=0.540, avg val loss=0.560, auc=0.693\n",
      "Iter=695, avg train loss=0.542, avg val loss=0.555, auc=0.692\n",
      "Iter=700, avg train loss=0.675, avg val loss=0.564, auc=0.684\n",
      "Iter=705, avg train loss=0.659, avg val loss=0.571, auc=0.691\n",
      "Iter=710, avg train loss=0.557, avg val loss=0.554, auc=0.695\n",
      "Iter=715, avg train loss=0.530, avg val loss=0.566, auc=0.694\n",
      "Iter=720, avg train loss=0.486, avg val loss=0.560, auc=0.687\n",
      "Iter=725, avg train loss=0.624, avg val loss=0.573, auc=0.696\n",
      "Iter=730, avg train loss=0.554, avg val loss=0.567, auc=0.693\n",
      "Iter=735, avg train loss=0.533, avg val loss=0.573, auc=0.690\n",
      "Iter=740, avg train loss=0.533, avg val loss=0.656, auc=0.705\n",
      "Iter=745, avg train loss=0.676, avg val loss=0.637, auc=0.712\n",
      "Iter=750, avg train loss=0.638, avg val loss=0.628, auc=0.702\n",
      "Iter=755, avg train loss=0.586, avg val loss=0.606, auc=0.692\n",
      "Iter=760, avg train loss=0.571, avg val loss=0.594, auc=0.687\n",
      "Iter=765, avg train loss=0.607, avg val loss=0.663, auc=0.687\n",
      "Iter=770, avg train loss=0.707, avg val loss=0.600, auc=0.681\n",
      "Iter=775, avg train loss=0.587, avg val loss=0.562, auc=0.686\n",
      "Iter=780, avg train loss=0.552, avg val loss=0.540, auc=0.686\n",
      "Iter=785, avg train loss=0.584, avg val loss=0.558, auc=0.691\n",
      "Iter=790, avg train loss=0.654, avg val loss=0.552, auc=0.690\n",
      "Iter=795, avg train loss=0.588, avg val loss=0.608, auc=0.681\n",
      "Iter=800, avg train loss=0.474, avg val loss=0.639, auc=0.677\n",
      "Iter=805, avg train loss=0.597, avg val loss=0.586, auc=0.684\n",
      "Iter=810, avg train loss=0.592, avg val loss=0.591, auc=0.681\n",
      "Iter=815, avg train loss=0.655, avg val loss=0.635, auc=0.680\n",
      "Iter=820, avg train loss=0.611, avg val loss=0.729, auc=0.670\n",
      "Iter=825, avg train loss=0.656, avg val loss=0.616, auc=0.673\n",
      "Iter=830, avg train loss=0.583, avg val loss=0.586, auc=0.672\n",
      "Iter=835, avg train loss=0.544, avg val loss=0.554, auc=0.686\n",
      "Iter=840, avg train loss=0.538, avg val loss=0.554, auc=0.692\n",
      "Iter=845, avg train loss=0.578, avg val loss=0.543, auc=0.687\n",
      "Iter=850, avg train loss=0.453, avg val loss=0.549, auc=0.674\n",
      "Iter=855, avg train loss=0.694, avg val loss=0.565, auc=0.666\n",
      "Iter=860, avg train loss=0.598, avg val loss=0.593, auc=0.668\n",
      "Iter=865, avg train loss=0.596, avg val loss=0.634, auc=0.668\n",
      "Iter=870, avg train loss=0.582, avg val loss=0.667, auc=0.650\n",
      "Iter=875, avg train loss=0.513, avg val loss=0.676, auc=0.649\n",
      "Iter=880, avg train loss=0.639, avg val loss=0.680, auc=0.648\n",
      "Iter=885, avg train loss=0.563, avg val loss=0.663, auc=0.658\n",
      "Iter=890, avg train loss=0.561, avg val loss=0.633, auc=0.652\n",
      "Iter=895, avg train loss=0.558, avg val loss=0.587, auc=0.622\n",
      "Iter=900, avg train loss=0.551, avg val loss=0.587, auc=0.590\n",
      "Iter=905, avg train loss=0.559, avg val loss=0.565, auc=0.593\n",
      "Iter=910, avg train loss=0.535, avg val loss=0.593, auc=0.587\n",
      "Iter=915, avg train loss=0.612, avg val loss=0.590, auc=0.597\n",
      "Iter=920, avg train loss=0.472, avg val loss=0.646, auc=0.598\n",
      "Iter=925, avg train loss=0.564, avg val loss=0.673, auc=0.608\n",
      "Iter=930, avg train loss=0.552, avg val loss=0.663, auc=0.595\n",
      "Iter=935, avg train loss=0.556, avg val loss=0.703, auc=0.615\n",
      "Iter=940, avg train loss=0.488, avg val loss=0.650, auc=0.614\n",
      "Iter=945, avg train loss=0.556, avg val loss=0.649, auc=0.623\n",
      "Iter=950, avg train loss=0.633, avg val loss=0.715, auc=0.629\n",
      "Iter=955, avg train loss=0.599, avg val loss=0.706, auc=0.630\n",
      "Iter=960, avg train loss=0.502, avg val loss=0.729, auc=0.625\n",
      "Iter=965, avg train loss=0.528, avg val loss=0.735, auc=0.622\n",
      "Iter=970, avg train loss=0.540, avg val loss=0.698, auc=0.636\n",
      "Iter=975, avg train loss=0.621, avg val loss=0.729, auc=0.638\n",
      "Iter=980, avg train loss=0.504, avg val loss=0.669, auc=0.621\n",
      "Iter=985, avg train loss=0.534, avg val loss=0.646, auc=0.625\n",
      "Iter=990, avg train loss=0.501, avg val loss=0.586, auc=0.623\n",
      "Iter=995, avg train loss=0.532, avg val loss=0.549, auc=0.628\n",
      "Iter=1000, avg train loss=0.528, avg val loss=0.555, auc=0.640\n",
      "Iter=1005, avg train loss=0.417, avg val loss=0.545, auc=0.641\n",
      "Iter=1010, avg train loss=0.596, avg val loss=0.576, auc=0.656\n",
      "Iter=1015, avg train loss=0.578, avg val loss=0.586, auc=0.666\n",
      "Iter=1020, avg train loss=0.516, avg val loss=0.635, auc=0.678\n",
      "Iter=1025, avg train loss=0.569, avg val loss=0.619, auc=0.669\n",
      "Iter=1030, avg train loss=0.534, avg val loss=0.648, auc=0.669\n",
      "Iter=1035, avg train loss=0.538, avg val loss=0.575, auc=0.674\n",
      "Iter=1040, avg train loss=0.530, avg val loss=0.552, auc=0.673\n",
      "Iter=1045, avg train loss=0.584, avg val loss=0.563, auc=0.670\n",
      "Iter=1050, avg train loss=0.634, avg val loss=0.532, auc=0.652\n",
      "Iter=1055, avg train loss=0.603, avg val loss=0.548, auc=0.658\n",
      "Iter=1060, avg train loss=0.643, avg val loss=0.599, auc=0.643\n",
      "Iter=1065, avg train loss=0.536, avg val loss=0.593, auc=0.620\n",
      "Iter=1070, avg train loss=0.601, avg val loss=0.578, auc=0.583\n",
      "Iter=1075, avg train loss=0.630, avg val loss=0.584, auc=0.573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1080, avg train loss=0.548, avg val loss=0.635, auc=0.582\n",
      "Iter=1085, avg train loss=0.466, avg val loss=0.642, auc=0.582\n",
      "Iter=1090, avg train loss=0.450, avg val loss=0.688, auc=0.621\n",
      "Iter=1095, avg train loss=0.465, avg val loss=0.704, auc=0.616\n",
      "Iter=1100, avg train loss=0.523, avg val loss=0.644, auc=0.627\n",
      "Iter=1105, avg train loss=0.464, avg val loss=0.631, auc=0.617\n",
      "Iter=1110, avg train loss=0.605, avg val loss=0.675, auc=0.636\n",
      "Iter=1115, avg train loss=0.524, avg val loss=0.699, auc=0.634\n",
      "Iter=1120, avg train loss=0.563, avg val loss=0.605, auc=0.615\n",
      "Iter=1125, avg train loss=0.547, avg val loss=0.649, auc=0.616\n",
      "Iter=1130, avg train loss=0.626, avg val loss=0.625, auc=0.589\n",
      "Iter=1135, avg train loss=0.529, avg val loss=0.613, auc=0.611\n",
      "Iter=1140, avg train loss=0.442, avg val loss=0.621, auc=0.638\n",
      "Iter=1145, avg train loss=0.582, avg val loss=0.574, auc=0.667\n",
      "Iter=1150, avg train loss=0.548, avg val loss=0.547, auc=0.674\n",
      "Iter=1155, avg train loss=0.592, avg val loss=0.532, auc=0.639\n",
      "Iter=1160, avg train loss=0.489, avg val loss=0.553, auc=0.671\n",
      "Iter=1165, avg train loss=0.479, avg val loss=0.539, auc=0.653\n",
      "Iter=1170, avg train loss=0.535, avg val loss=0.597, auc=0.670\n",
      "Iter=1175, avg train loss=0.468, avg val loss=0.587, auc=0.673\n",
      "Iter=1180, avg train loss=0.519, avg val loss=0.603, auc=0.672\n",
      "Iter=1185, avg train loss=0.600, avg val loss=0.594, auc=0.657\n",
      "Iter=1190, avg train loss=0.393, avg val loss=0.617, auc=0.639\n",
      "Iter=1195, avg train loss=0.547, avg val loss=0.656, auc=0.645\n",
      "Iter=1200, avg train loss=0.486, avg val loss=0.781, auc=0.649\n",
      "Iter=1205, avg train loss=0.481, avg val loss=0.695, auc=0.658\n",
      "Iter=1210, avg train loss=0.705, avg val loss=0.594, auc=0.646\n",
      "Iter=1215, avg train loss=0.580, avg val loss=0.576, auc=0.632\n",
      "Iter=1220, avg train loss=0.514, avg val loss=0.569, auc=0.635\n",
      "Iter=1225, avg train loss=0.524, avg val loss=0.555, auc=0.623\n",
      "Iter=1230, avg train loss=0.400, avg val loss=0.565, auc=0.631\n",
      "Iter=1235, avg train loss=0.509, avg val loss=0.601, auc=0.652\n",
      "Iter=1240, avg train loss=0.546, avg val loss=0.673, auc=0.652\n",
      "Iter=1245, avg train loss=0.478, avg val loss=0.669, auc=0.665\n",
      "Iter=1250, avg train loss=0.633, avg val loss=0.588, auc=0.670\n",
      "Iter=1255, avg train loss=0.411, avg val loss=0.629, auc=0.679\n",
      "Iter=1260, avg train loss=0.530, avg val loss=0.666, auc=0.685\n",
      "Iter=1265, avg train loss=0.682, avg val loss=0.615, auc=0.673\n",
      "Iter=1270, avg train loss=0.454, avg val loss=0.569, auc=0.671\n",
      "Iter=1275, avg train loss=0.449, avg val loss=0.545, auc=0.678\n",
      "Iter=1280, avg train loss=0.388, avg val loss=0.539, auc=0.682\n",
      "Iter=1285, avg train loss=0.509, avg val loss=0.526, auc=0.661\n",
      "Iter=1290, avg train loss=0.604, avg val loss=0.540, auc=0.674\n",
      "Iter=1295, avg train loss=0.465, avg val loss=0.701, auc=0.663\n",
      "Iter=1300, avg train loss=0.611, avg val loss=0.612, auc=0.665\n",
      "Iter=1305, avg train loss=0.461, avg val loss=0.570, auc=0.674\n",
      "Iter=1310, avg train loss=0.520, avg val loss=0.573, auc=0.674\n",
      "Iter=1315, avg train loss=0.549, avg val loss=0.543, auc=0.684\n",
      "Iter=1320, avg train loss=0.389, avg val loss=0.548, auc=0.685\n",
      "Iter=1325, avg train loss=0.379, avg val loss=0.554, auc=0.683\n",
      "Iter=1330, avg train loss=0.747, avg val loss=0.537, auc=0.675\n",
      "Iter=1335, avg train loss=0.556, avg val loss=0.551, auc=0.672\n",
      "Iter=1340, avg train loss=0.519, avg val loss=0.565, auc=0.671\n",
      "Iter=1345, avg train loss=0.374, avg val loss=0.620, auc=0.649\n",
      "Iter=1350, avg train loss=0.592, avg val loss=0.597, auc=0.655\n",
      "Iter=1355, avg train loss=0.417, avg val loss=0.671, auc=0.665\n",
      "Iter=1360, avg train loss=0.495, avg val loss=0.607, auc=0.660\n",
      "Iter=1365, avg train loss=0.348, avg val loss=0.607, auc=0.668\n",
      "Iter=1370, avg train loss=0.684, avg val loss=0.603, auc=0.672\n",
      "Iter=1375, avg train loss=0.557, avg val loss=0.630, auc=0.667\n",
      "Iter=1380, avg train loss=0.433, avg val loss=0.595, auc=0.651\n",
      "Iter=1385, avg train loss=0.386, avg val loss=0.574, auc=0.670\n",
      "Iter=1390, avg train loss=0.463, avg val loss=0.550, auc=0.659\n",
      "Iter=1395, avg train loss=0.497, avg val loss=0.534, auc=0.661\n",
      "Iter=1400, avg train loss=0.647, avg val loss=0.558, auc=0.679\n",
      "Iter=1405, avg train loss=0.499, avg val loss=0.635, auc=0.671\n",
      "Iter=1410, avg train loss=0.628, avg val loss=0.623, auc=0.660\n",
      "Iter=1415, avg train loss=0.568, avg val loss=0.574, auc=0.634\n",
      "Iter=1420, avg train loss=0.451, avg val loss=0.593, auc=0.623\n",
      "Iter=1425, avg train loss=0.558, avg val loss=0.569, auc=0.609\n",
      "Iter=1430, avg train loss=0.587, avg val loss=0.535, auc=0.644\n",
      "Iter=1435, avg train loss=0.491, avg val loss=0.547, auc=0.667\n",
      "Iter=1440, avg train loss=0.598, avg val loss=0.574, auc=0.684\n",
      "Iter=1445, avg train loss=0.407, avg val loss=0.543, auc=0.685\n",
      "Iter=1450, avg train loss=0.422, avg val loss=0.579, auc=0.686\n",
      "Iter=1455, avg train loss=0.451, avg val loss=0.609, auc=0.669\n",
      "Iter=1460, avg train loss=0.607, avg val loss=0.591, auc=0.679\n",
      "Iter=1465, avg train loss=0.466, avg val loss=0.661, auc=0.688\n",
      "Iter=1470, avg train loss=0.514, avg val loss=0.675, auc=0.689\n",
      "Iter=1475, avg train loss=0.461, avg val loss=0.553, auc=0.680\n",
      "Iter=1480, avg train loss=0.399, avg val loss=0.558, auc=0.670\n",
      "Iter=1485, avg train loss=0.510, avg val loss=0.580, auc=0.670\n",
      "Iter=1490, avg train loss=0.536, avg val loss=0.566, auc=0.675\n",
      "Iter=1495, avg train loss=0.593, avg val loss=0.596, auc=0.675\n",
      "Iter=1500, avg train loss=0.484, avg val loss=0.600, auc=0.680\n",
      "Iter=1505, avg train loss=0.463, avg val loss=0.600, auc=0.683\n",
      "Iter=1510, avg train loss=0.504, avg val loss=0.576, auc=0.683\n",
      "Iter=1515, avg train loss=0.453, avg val loss=0.563, auc=0.677\n",
      "Iter=1520, avg train loss=0.469, avg val loss=0.579, auc=0.678\n",
      "Iter=1525, avg train loss=0.452, avg val loss=0.590, auc=0.666\n",
      "Iter=1530, avg train loss=0.582, avg val loss=0.576, auc=0.679\n",
      "Iter=1535, avg train loss=0.423, avg val loss=0.555, auc=0.688\n",
      "Iter=1540, avg train loss=0.494, avg val loss=0.596, auc=0.690\n",
      "Iter=1545, avg train loss=0.490, avg val loss=0.583, auc=0.689\n",
      "Iter=1550, avg train loss=0.462, avg val loss=0.579, auc=0.687\n",
      "Iter=1555, avg train loss=0.475, avg val loss=0.650, auc=0.656\n",
      "Iter=1560, avg train loss=0.583, avg val loss=0.623, auc=0.652\n",
      "Iter=1565, avg train loss=0.449, avg val loss=0.601, auc=0.630\n",
      "Iter=1570, avg train loss=0.464, avg val loss=0.607, auc=0.646\n",
      "Iter=1575, avg train loss=0.611, avg val loss=0.565, auc=0.639\n",
      "Iter=1580, avg train loss=0.417, avg val loss=0.550, auc=0.628\n",
      "Iter=1585, avg train loss=0.450, avg val loss=0.552, auc=0.628\n",
      "Iter=1590, avg train loss=0.428, avg val loss=0.555, auc=0.639\n",
      "Iter=1595, avg train loss=0.566, avg val loss=0.555, auc=0.636\n",
      "Iter=1600, avg train loss=0.413, avg val loss=0.560, auc=0.662\n",
      "Iter=1605, avg train loss=0.467, avg val loss=0.594, auc=0.674\n",
      "Iter=1610, avg train loss=0.485, avg val loss=0.693, auc=0.674\n",
      "Iter=1615, avg train loss=0.424, avg val loss=0.647, auc=0.678\n",
      "Iter=1620, avg train loss=0.493, avg val loss=0.603, auc=0.676\n",
      "Iter=1625, avg train loss=0.525, avg val loss=0.560, auc=0.676\n",
      "Iter=1630, avg train loss=0.425, avg val loss=0.559, auc=0.672\n",
      "Iter=1635, avg train loss=0.445, avg val loss=0.545, auc=0.669\n",
      "Iter=1640, avg train loss=0.448, avg val loss=0.544, auc=0.686\n",
      "Iter=1645, avg train loss=0.338, avg val loss=0.551, auc=0.671\n",
      "Iter=1650, avg train loss=0.353, avg val loss=0.567, auc=0.652\n",
      "Iter=1655, avg train loss=0.543, avg val loss=0.584, auc=0.650\n",
      "Iter=1660, avg train loss=0.465, avg val loss=0.573, auc=0.650\n",
      "Iter=1665, avg train loss=0.442, avg val loss=0.593, auc=0.643\n",
      "Iter=1670, avg train loss=0.594, avg val loss=0.633, auc=0.669\n",
      "Iter=1675, avg train loss=0.604, avg val loss=0.646, auc=0.665\n",
      "Iter=1680, avg train loss=0.484, avg val loss=0.625, auc=0.658\n",
      "Iter=1685, avg train loss=0.377, avg val loss=0.713, auc=0.656\n",
      "Iter=1690, avg train loss=0.476, avg val loss=0.651, auc=0.674\n",
      "Iter=1695, avg train loss=0.508, avg val loss=0.586, auc=0.685\n",
      "Iter=1700, avg train loss=0.498, avg val loss=0.584, auc=0.686\n",
      "Iter=1705, avg train loss=0.324, avg val loss=0.565, auc=0.690\n",
      "Iter=1710, avg train loss=0.402, avg val loss=0.571, auc=0.697\n",
      "Iter=1715, avg train loss=0.480, avg val loss=0.594, auc=0.695\n",
      "Iter=1720, avg train loss=0.559, avg val loss=0.602, auc=0.676\n",
      "Iter=1725, avg train loss=0.526, avg val loss=0.631, auc=0.669\n",
      "Iter=1730, avg train loss=0.322, avg val loss=0.638, auc=0.658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1735, avg train loss=0.475, avg val loss=0.555, auc=0.681\n",
      "Iter=1740, avg train loss=0.378, avg val loss=0.573, auc=0.667\n",
      "Iter=1745, avg train loss=0.409, avg val loss=0.550, auc=0.676\n",
      "Iter=1750, avg train loss=0.418, avg val loss=0.624, auc=0.689\n",
      "Iter=1755, avg train loss=0.386, avg val loss=0.591, auc=0.712\n",
      "Iter=1760, avg train loss=0.464, avg val loss=0.570, auc=0.702\n",
      "Iter=1765, avg train loss=0.393, avg val loss=0.632, auc=0.693\n",
      "Iter=1770, avg train loss=0.523, avg val loss=0.551, auc=0.679\n",
      "Iter=1775, avg train loss=0.476, avg val loss=0.536, auc=0.671\n",
      "Iter=1780, avg train loss=0.380, avg val loss=0.552, auc=0.677\n",
      "Iter=1785, avg train loss=0.491, avg val loss=0.548, auc=0.656\n",
      "Iter=1790, avg train loss=0.438, avg val loss=0.567, auc=0.662\n",
      "Iter=1795, avg train loss=0.398, avg val loss=0.672, auc=0.664\n",
      "Iter=1800, avg train loss=0.452, avg val loss=0.588, auc=0.661\n",
      "Iter=1805, avg train loss=0.464, avg val loss=0.571, auc=0.656\n",
      "Iter=1810, avg train loss=0.468, avg val loss=0.583, auc=0.644\n",
      "Iter=1815, avg train loss=0.551, avg val loss=0.662, auc=0.630\n",
      "Iter=1820, avg train loss=0.306, avg val loss=0.676, auc=0.628\n",
      "Iter=1825, avg train loss=0.398, avg val loss=0.634, auc=0.612\n",
      "Iter=1830, avg train loss=0.433, avg val loss=0.671, auc=0.623\n",
      "Iter=1835, avg train loss=0.378, avg val loss=0.590, auc=0.627\n",
      "Iter=1840, avg train loss=0.386, avg val loss=0.541, auc=0.634\n",
      "Iter=1845, avg train loss=0.471, avg val loss=0.554, auc=0.664\n",
      "Iter=1850, avg train loss=0.378, avg val loss=0.676, auc=0.657\n",
      "Iter=1855, avg train loss=0.468, avg val loss=0.592, auc=0.645\n",
      "Iter=1860, avg train loss=0.475, avg val loss=0.570, auc=0.659\n",
      "Iter=1865, avg train loss=0.329, avg val loss=0.576, auc=0.653\n",
      "Iter=1870, avg train loss=0.451, avg val loss=0.591, auc=0.641\n",
      "Iter=1875, avg train loss=0.380, avg val loss=0.620, auc=0.663\n",
      "Iter=1880, avg train loss=0.447, avg val loss=0.683, auc=0.662\n",
      "Iter=1885, avg train loss=0.373, avg val loss=0.703, auc=0.661\n",
      "Iter=1890, avg train loss=0.391, avg val loss=0.605, auc=0.664\n",
      "Iter=1895, avg train loss=0.398, avg val loss=0.588, auc=0.665\n",
      "Iter=1900, avg train loss=0.560, avg val loss=0.667, auc=0.645\n",
      "Iter=1905, avg train loss=0.653, avg val loss=0.605, auc=0.640\n",
      "Iter=1910, avg train loss=0.319, avg val loss=0.570, auc=0.646\n",
      "Iter=1915, avg train loss=0.470, avg val loss=0.624, auc=0.649\n",
      "Iter=1920, avg train loss=0.509, avg val loss=0.647, auc=0.659\n",
      "Iter=1925, avg train loss=0.358, avg val loss=0.648, auc=0.630\n",
      "Iter=1930, avg train loss=0.282, avg val loss=0.636, auc=0.646\n",
      "Iter=1935, avg train loss=0.431, avg val loss=0.601, auc=0.663\n",
      "Iter=1940, avg train loss=0.434, avg val loss=0.640, auc=0.659\n",
      "Iter=1945, avg train loss=0.541, avg val loss=0.669, auc=0.657\n",
      "Iter=1950, avg train loss=0.387, avg val loss=0.593, auc=0.641\n",
      "Iter=1955, avg train loss=0.301, avg val loss=0.605, auc=0.651\n",
      "Iter=1960, avg train loss=0.412, avg val loss=0.558, auc=0.638\n",
      "Iter=1965, avg train loss=0.473, avg val loss=0.626, auc=0.622\n",
      "Iter=1970, avg train loss=0.518, avg val loss=0.607, auc=0.611\n",
      "Iter=1975, avg train loss=0.537, avg val loss=0.577, auc=0.600\n",
      "Iter=1980, avg train loss=0.457, avg val loss=0.578, auc=0.584\n",
      "Iter=1985, avg train loss=0.396, avg val loss=0.588, auc=0.614\n",
      "Iter=1990, avg train loss=0.424, avg val loss=0.613, auc=0.622\n",
      "Iter=1995, avg train loss=0.323, avg val loss=0.596, auc=0.632\n",
      "Iter=2000, avg train loss=0.433, avg val loss=0.629, auc=0.633\n",
      "Iter=2005, avg train loss=0.363, avg val loss=0.732, auc=0.645\n",
      "Iter=2010, avg train loss=0.397, avg val loss=0.709, auc=0.649\n",
      "Iter=2015, avg train loss=0.450, avg val loss=0.654, auc=0.645\n",
      "Iter=2020, avg train loss=0.404, avg val loss=0.718, auc=0.617\n",
      "Iter=2025, avg train loss=0.367, avg val loss=0.703, auc=0.577\n",
      "Iter=2030, avg train loss=0.429, avg val loss=0.686, auc=0.569\n",
      "Iter=2035, avg train loss=0.355, avg val loss=0.735, auc=0.573\n",
      "Iter=2040, avg train loss=0.325, avg val loss=0.670, auc=0.595\n",
      "Iter=2045, avg train loss=0.345, avg val loss=0.646, auc=0.613\n",
      "Iter=2050, avg train loss=0.370, avg val loss=0.678, auc=0.635\n",
      "Iter=2055, avg train loss=0.417, avg val loss=0.608, auc=0.624\n",
      "Iter=2060, avg train loss=0.377, avg val loss=0.562, auc=0.639\n",
      "Iter=2065, avg train loss=0.328, avg val loss=0.560, auc=0.640\n",
      "Iter=2070, avg train loss=0.370, avg val loss=0.571, auc=0.647\n",
      "Iter=2075, avg train loss=0.366, avg val loss=0.669, auc=0.646\n",
      "Iter=2080, avg train loss=0.425, avg val loss=0.653, auc=0.662\n",
      "Iter=2085, avg train loss=0.380, avg val loss=0.776, auc=0.662\n",
      "Iter=2090, avg train loss=0.405, avg val loss=0.731, auc=0.645\n",
      "Iter=2095, avg train loss=0.325, avg val loss=0.706, auc=0.630\n",
      "Iter=2100, avg train loss=0.344, avg val loss=0.626, auc=0.630\n",
      "Iter=2105, avg train loss=0.354, avg val loss=0.545, auc=0.671\n",
      "Iter=2110, avg train loss=0.308, avg val loss=0.551, auc=0.669\n",
      "Iter=2115, avg train loss=0.520, avg val loss=0.575, auc=0.679\n",
      "Iter=2120, avg train loss=0.325, avg val loss=0.613, auc=0.686\n",
      "Iter=2125, avg train loss=0.360, avg val loss=0.653, auc=0.662\n",
      "Iter=2130, avg train loss=0.437, avg val loss=0.595, auc=0.646\n",
      "Iter=2135, avg train loss=0.394, avg val loss=0.568, auc=0.620\n",
      "Iter=2140, avg train loss=0.300, avg val loss=0.585, auc=0.624\n",
      "Iter=2145, avg train loss=0.345, avg val loss=0.653, auc=0.615\n",
      "Iter=2150, avg train loss=0.410, avg val loss=0.659, auc=0.617\n",
      "Iter=2155, avg train loss=0.308, avg val loss=0.715, auc=0.636\n",
      "Iter=2160, avg train loss=0.454, avg val loss=0.634, auc=0.651\n",
      "Iter=2165, avg train loss=0.274, avg val loss=0.556, auc=0.668\n",
      "Iter=2170, avg train loss=0.297, avg val loss=0.543, auc=0.679\n",
      "Iter=2175, avg train loss=0.473, avg val loss=0.570, auc=0.681\n",
      "Iter=2180, avg train loss=0.384, avg val loss=0.587, auc=0.686\n",
      "Iter=2185, avg train loss=0.343, avg val loss=0.647, auc=0.687\n",
      "Iter=2190, avg train loss=0.381, avg val loss=0.576, auc=0.684\n",
      "Iter=2195, avg train loss=0.394, avg val loss=0.539, auc=0.679\n",
      "Iter=2200, avg train loss=0.385, avg val loss=0.564, auc=0.680\n",
      "Iter=2205, avg train loss=0.354, avg val loss=0.662, auc=0.665\n",
      "Iter=2210, avg train loss=0.488, avg val loss=0.641, auc=0.667\n",
      "Iter=2215, avg train loss=0.513, avg val loss=0.590, auc=0.670\n",
      "Iter=2220, avg train loss=0.328, avg val loss=0.557, auc=0.673\n",
      "Iter=2225, avg train loss=0.269, avg val loss=0.560, auc=0.689\n",
      "Iter=2230, avg train loss=0.409, avg val loss=0.572, auc=0.675\n",
      "Iter=2235, avg train loss=0.290, avg val loss=0.571, auc=0.691\n",
      "Iter=2240, avg train loss=0.497, avg val loss=0.649, auc=0.687\n",
      "Iter=2245, avg train loss=0.425, avg val loss=0.623, auc=0.684\n",
      "Iter=2250, avg train loss=0.410, avg val loss=0.594, auc=0.682\n",
      "Iter=2255, avg train loss=0.264, avg val loss=0.585, auc=0.679\n",
      "Iter=2260, avg train loss=0.450, avg val loss=0.574, auc=0.674\n",
      "Iter=2265, avg train loss=0.266, avg val loss=0.573, auc=0.661\n",
      "Iter=2270, avg train loss=0.418, avg val loss=0.562, auc=0.646\n",
      "Iter=2275, avg train loss=0.320, avg val loss=0.658, auc=0.661\n",
      "Iter=2280, avg train loss=0.339, avg val loss=0.673, auc=0.650\n",
      "Iter=2285, avg train loss=0.328, avg val loss=0.600, auc=0.646\n",
      "Iter=2290, avg train loss=0.343, avg val loss=0.582, auc=0.676\n",
      "Iter=2295, avg train loss=0.285, avg val loss=0.583, auc=0.661\n",
      "Iter=2300, avg train loss=0.402, avg val loss=0.591, auc=0.673\n",
      "Iter=2305, avg train loss=0.338, avg val loss=0.622, auc=0.673\n",
      "Iter=2310, avg train loss=0.329, avg val loss=0.573, auc=0.668\n",
      "Iter=2315, avg train loss=0.371, avg val loss=0.584, auc=0.653\n",
      "Iter=2320, avg train loss=0.327, avg val loss=0.738, auc=0.642\n",
      "Iter=2325, avg train loss=0.279, avg val loss=0.641, auc=0.642\n",
      "Iter=2330, avg train loss=0.399, avg val loss=0.623, auc=0.643\n",
      "Iter=2335, avg train loss=0.473, avg val loss=0.594, auc=0.659\n",
      "Iter=2340, avg train loss=0.541, avg val loss=0.550, auc=0.658\n",
      "Iter=2345, avg train loss=0.302, avg val loss=0.606, auc=0.652\n",
      "Iter=2350, avg train loss=0.281, avg val loss=0.617, auc=0.640\n",
      "Iter=2355, avg train loss=0.365, avg val loss=0.628, auc=0.620\n",
      "Iter=2360, avg train loss=0.382, avg val loss=0.627, auc=0.638\n",
      "Iter=2365, avg train loss=0.380, avg val loss=0.587, auc=0.639\n",
      "Iter=2370, avg train loss=0.365, avg val loss=0.632, auc=0.658\n",
      "Iter=2375, avg train loss=0.221, avg val loss=0.615, auc=0.662\n",
      "Iter=2380, avg train loss=0.261, avg val loss=0.646, auc=0.678\n",
      "Iter=2385, avg train loss=0.339, avg val loss=0.619, auc=0.667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=2390, avg train loss=0.261, avg val loss=0.603, auc=0.670\n",
      "Iter=2395, avg train loss=0.256, avg val loss=0.660, auc=0.659\n",
      "Iter=2400, avg train loss=0.407, avg val loss=0.604, auc=0.637\n",
      "Iter=2405, avg train loss=0.293, avg val loss=0.578, auc=0.636\n",
      "Iter=2410, avg train loss=0.281, avg val loss=0.649, auc=0.634\n",
      "Iter=2415, avg train loss=0.342, avg val loss=0.668, auc=0.653\n",
      "Iter=2420, avg train loss=0.322, avg val loss=0.613, auc=0.657\n",
      "Iter=2425, avg train loss=0.349, avg val loss=0.602, auc=0.643\n",
      "Iter=2430, avg train loss=0.380, avg val loss=0.580, auc=0.639\n",
      "Iter=2435, avg train loss=0.229, avg val loss=0.663, auc=0.636\n",
      "Iter=2440, avg train loss=0.459, avg val loss=0.614, auc=0.645\n",
      "Iter=2445, avg train loss=0.351, avg val loss=0.578, auc=0.642\n",
      "Iter=2450, avg train loss=0.316, avg val loss=0.604, auc=0.643\n",
      "Iter=2455, avg train loss=0.372, avg val loss=0.702, auc=0.646\n",
      "Iter=2460, avg train loss=0.357, avg val loss=0.681, auc=0.643\n",
      "Iter=2465, avg train loss=0.315, avg val loss=0.675, auc=0.635\n",
      "Iter=2470, avg train loss=0.360, avg val loss=0.674, auc=0.640\n",
      "Iter=2475, avg train loss=0.390, avg val loss=0.636, auc=0.658\n",
      "Iter=2480, avg train loss=0.263, avg val loss=0.668, auc=0.670\n",
      "Iter=2485, avg train loss=0.233, avg val loss=0.638, auc=0.669\n",
      "Iter=2490, avg train loss=0.279, avg val loss=0.575, auc=0.662\n",
      "Iter=2495, avg train loss=0.292, avg val loss=0.590, auc=0.652\n",
      "Iter=2500, avg train loss=0.278, avg val loss=0.600, auc=0.646\n",
      "Iter=2505, avg train loss=0.397, avg val loss=0.613, auc=0.674\n",
      "Iter=2510, avg train loss=0.383, avg val loss=0.761, auc=0.682\n",
      "Iter=2515, avg train loss=0.410, avg val loss=0.813, auc=0.665\n",
      "Iter=2520, avg train loss=0.343, avg val loss=0.595, auc=0.686\n",
      "Iter=2525, avg train loss=0.244, avg val loss=0.604, auc=0.674\n",
      "Iter=2530, avg train loss=0.310, avg val loss=0.596, auc=0.654\n",
      "Iter=2535, avg train loss=0.303, avg val loss=0.584, auc=0.664\n",
      "Iter=2540, avg train loss=0.310, avg val loss=0.587, auc=0.658\n",
      "Iter=2545, avg train loss=0.299, avg val loss=0.652, auc=0.671\n",
      "Iter=2550, avg train loss=0.281, avg val loss=0.791, auc=0.681\n",
      "Iter=2555, avg train loss=0.341, avg val loss=0.590, auc=0.665\n",
      "Iter=2560, avg train loss=0.259, avg val loss=0.583, auc=0.658\n",
      "Iter=2565, avg train loss=0.282, avg val loss=0.565, auc=0.660\n",
      "Iter=2570, avg train loss=0.258, avg val loss=0.573, auc=0.638\n",
      "Iter=2575, avg train loss=0.298, avg val loss=0.598, auc=0.645\n",
      "Iter=2580, avg train loss=0.247, avg val loss=0.679, auc=0.680\n",
      "Iter=2585, avg train loss=0.352, avg val loss=0.579, auc=0.668\n",
      "Iter=2590, avg train loss=0.223, avg val loss=0.560, auc=0.655\n",
      "Iter=2595, avg train loss=0.224, avg val loss=0.573, auc=0.635\n",
      "Iter=2600, avg train loss=0.356, avg val loss=0.591, auc=0.655\n",
      "Iter=2605, avg train loss=0.260, avg val loss=0.742, auc=0.657\n",
      "Iter=2610, avg train loss=0.253, avg val loss=0.738, auc=0.647\n",
      "Iter=2615, avg train loss=0.300, avg val loss=0.715, auc=0.627\n",
      "Iter=2620, avg train loss=0.410, avg val loss=0.671, auc=0.625\n",
      "Iter=2625, avg train loss=0.356, avg val loss=0.673, auc=0.623\n",
      "Iter=2630, avg train loss=0.338, avg val loss=0.588, auc=0.642\n",
      "Iter=2635, avg train loss=0.296, avg val loss=0.557, auc=0.635\n",
      "Iter=2640, avg train loss=0.357, avg val loss=0.597, auc=0.665\n",
      "Iter=2645, avg train loss=0.379, avg val loss=0.651, auc=0.681\n",
      "Iter=2650, avg train loss=0.269, avg val loss=0.638, auc=0.637\n",
      "Iter=2655, avg train loss=0.292, avg val loss=0.604, auc=0.639\n",
      "Iter=2660, avg train loss=0.229, avg val loss=0.647, auc=0.626\n",
      "Iter=2665, avg train loss=0.286, avg val loss=0.634, auc=0.640\n",
      "Iter=2670, avg train loss=0.266, avg val loss=0.623, auc=0.635\n",
      "Iter=2675, avg train loss=0.315, avg val loss=0.587, auc=0.638\n",
      "Iter=2680, avg train loss=0.397, avg val loss=0.610, auc=0.662\n",
      "Iter=2685, avg train loss=0.330, avg val loss=0.666, auc=0.656\n",
      "Iter=2690, avg train loss=0.347, avg val loss=0.717, auc=0.644\n",
      "Iter=2695, avg train loss=0.287, avg val loss=0.662, auc=0.646\n",
      "Iter=2700, avg train loss=0.288, avg val loss=0.639, auc=0.643\n",
      "Iter=2705, avg train loss=0.203, avg val loss=0.658, auc=0.652\n",
      "Iter=2710, avg train loss=0.205, avg val loss=0.623, auc=0.646\n",
      "Iter=2715, avg train loss=0.282, avg val loss=0.668, auc=0.643\n",
      "Iter=2720, avg train loss=0.253, avg val loss=0.711, auc=0.642\n",
      "Iter=2725, avg train loss=0.223, avg val loss=0.672, auc=0.615\n",
      "Iter=2730, avg train loss=0.258, avg val loss=0.603, auc=0.618\n",
      "Iter=2735, avg train loss=0.316, avg val loss=0.706, auc=0.628\n",
      "Iter=2740, avg train loss=0.294, avg val loss=0.682, auc=0.620\n",
      "Iter=2745, avg train loss=0.353, avg val loss=0.670, auc=0.625\n",
      "Iter=2750, avg train loss=0.417, avg val loss=0.616, auc=0.617\n",
      "Iter=2755, avg train loss=0.338, avg val loss=0.588, auc=0.641\n",
      "Iter=2760, avg train loss=0.236, avg val loss=0.588, auc=0.678\n",
      "Iter=2765, avg train loss=0.189, avg val loss=0.552, auc=0.683\n",
      "Iter=2770, avg train loss=0.316, avg val loss=0.607, auc=0.685\n",
      "Iter=2775, avg train loss=0.301, avg val loss=0.561, auc=0.668\n",
      "Iter=2780, avg train loss=0.303, avg val loss=0.613, auc=0.667\n",
      "Iter=2785, avg train loss=0.428, avg val loss=0.597, auc=0.650\n",
      "Iter=2790, avg train loss=0.298, avg val loss=0.611, auc=0.641\n",
      "Iter=2795, avg train loss=0.222, avg val loss=0.690, auc=0.639\n",
      "Iter=2800, avg train loss=0.183, avg val loss=0.694, auc=0.633\n",
      "Iter=2805, avg train loss=0.330, avg val loss=0.633, auc=0.621\n",
      "Iter=2810, avg train loss=0.282, avg val loss=0.611, auc=0.629\n",
      "Iter=2815, avg train loss=0.218, avg val loss=0.633, auc=0.629\n",
      "Iter=2820, avg train loss=0.265, avg val loss=0.712, auc=0.640\n",
      "Iter=2825, avg train loss=0.279, avg val loss=0.675, auc=0.639\n",
      "Iter=2830, avg train loss=0.285, avg val loss=0.724, auc=0.650\n",
      "Iter=2835, avg train loss=0.251, avg val loss=0.601, auc=0.651\n",
      "Iter=2840, avg train loss=0.211, avg val loss=0.595, auc=0.648\n",
      "Iter=2845, avg train loss=0.270, avg val loss=0.588, auc=0.653\n",
      "Iter=2850, avg train loss=0.226, avg val loss=0.581, auc=0.663\n",
      "Iter=2855, avg train loss=0.274, avg val loss=0.586, auc=0.663\n",
      "Iter=2860, avg train loss=0.329, avg val loss=0.565, auc=0.647\n",
      "Iter=2865, avg train loss=0.294, avg val loss=0.621, auc=0.631\n",
      "Iter=2870, avg train loss=0.269, avg val loss=0.673, auc=0.617\n",
      "Iter=2875, avg train loss=0.165, avg val loss=0.731, auc=0.593\n",
      "Iter=2880, avg train loss=0.260, avg val loss=0.655, auc=0.589\n",
      "Iter=2885, avg train loss=0.247, avg val loss=0.855, auc=0.596\n",
      "Iter=2890, avg train loss=0.249, avg val loss=0.666, auc=0.597\n",
      "Iter=2895, avg train loss=0.313, avg val loss=0.635, auc=0.602\n",
      "Iter=2900, avg train loss=0.251, avg val loss=0.578, auc=0.606\n",
      "Iter=2905, avg train loss=0.293, avg val loss=0.648, auc=0.611\n",
      "Iter=2910, avg train loss=0.262, avg val loss=0.747, auc=0.634\n",
      "Iter=2915, avg train loss=0.248, avg val loss=0.649, auc=0.648\n",
      "Iter=2920, avg train loss=0.267, avg val loss=0.596, auc=0.668\n",
      "Iter=2925, avg train loss=0.283, avg val loss=0.714, auc=0.676\n",
      "Iter=2930, avg train loss=0.198, avg val loss=0.644, auc=0.682\n",
      "Iter=2935, avg train loss=0.241, avg val loss=0.568, auc=0.643\n",
      "Iter=2940, avg train loss=0.267, avg val loss=0.594, auc=0.635\n",
      "Iter=2945, avg train loss=0.207, avg val loss=0.653, auc=0.648\n",
      "Iter=2950, avg train loss=0.269, avg val loss=0.666, auc=0.646\n",
      "Iter=2955, avg train loss=0.217, avg val loss=0.676, auc=0.643\n",
      "Iter=2960, avg train loss=0.188, avg val loss=0.830, auc=0.654\n",
      "Iter=2965, avg train loss=0.235, avg val loss=0.642, auc=0.623\n",
      "Iter=2970, avg train loss=0.281, avg val loss=0.622, auc=0.622\n",
      "Iter=2975, avg train loss=0.232, avg val loss=0.597, auc=0.605\n",
      "Iter=2980, avg train loss=0.274, avg val loss=0.609, auc=0.640\n",
      "Iter=2985, avg train loss=0.273, avg val loss=0.677, auc=0.642\n",
      "Iter=2990, avg train loss=0.289, avg val loss=0.649, auc=0.641\n",
      "Iter=2995, avg train loss=0.283, avg val loss=0.619, auc=0.620\n",
      "Iter=3000, avg train loss=0.178, avg val loss=0.639, auc=0.619\n",
      "Iter=3005, avg train loss=0.192, avg val loss=0.618, auc=0.613\n",
      "Iter=3010, avg train loss=0.180, avg val loss=0.621, auc=0.624\n",
      "Iter=3015, avg train loss=0.244, avg val loss=0.622, auc=0.623\n",
      "Iter=3020, avg train loss=0.196, avg val loss=0.648, auc=0.652\n",
      "Iter=3025, avg train loss=0.290, avg val loss=0.697, auc=0.637\n",
      "Iter=3030, avg train loss=0.236, avg val loss=0.667, auc=0.637\n",
      "Iter=3035, avg train loss=0.159, avg val loss=0.615, auc=0.646\n",
      "Iter=3040, avg train loss=0.224, avg val loss=0.589, auc=0.646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=3045, avg train loss=0.228, avg val loss=0.659, auc=0.660\n",
      "Iter=3050, avg train loss=0.315, avg val loss=0.727, auc=0.666\n",
      "Iter=3055, avg train loss=0.158, avg val loss=0.676, auc=0.659\n",
      "Iter=3060, avg train loss=0.149, avg val loss=0.700, auc=0.643\n",
      "Iter=3065, avg train loss=0.215, avg val loss=0.639, auc=0.650\n",
      "Iter=3070, avg train loss=0.281, avg val loss=0.700, auc=0.650\n",
      "Iter=3075, avg train loss=0.244, avg val loss=0.710, auc=0.651\n",
      "Iter=3080, avg train loss=0.252, avg val loss=0.716, auc=0.649\n",
      "Iter=3085, avg train loss=0.237, avg val loss=0.759, auc=0.675\n",
      "Iter=3090, avg train loss=0.227, avg val loss=0.664, auc=0.684\n",
      "Iter=3095, avg train loss=0.222, avg val loss=0.609, auc=0.671\n",
      "Iter=3100, avg train loss=0.253, avg val loss=0.583, auc=0.656\n",
      "Iter=3105, avg train loss=0.171, avg val loss=0.639, auc=0.649\n",
      "Iter=3110, avg train loss=0.186, avg val loss=0.630, auc=0.646\n",
      "Iter=3115, avg train loss=0.178, avg val loss=0.664, auc=0.630\n",
      "Iter=3120, avg train loss=0.213, avg val loss=0.810, auc=0.642\n",
      "Iter=3125, avg train loss=0.157, avg val loss=0.713, auc=0.640\n",
      "Iter=3130, avg train loss=0.244, avg val loss=0.623, auc=0.648\n",
      "Iter=3135, avg train loss=0.157, avg val loss=0.592, auc=0.644\n",
      "Iter=3140, avg train loss=0.320, avg val loss=0.690, auc=0.641\n",
      "Best model loaded.\n",
      "Predicting on the test set...Done\n",
      "Test AUC after training=0.662, max-score-based AUC=0.635\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "cpu_threads = 4\n",
    "batch_size = 4\n",
    "n_folds = 5\n",
    "epochs = 20\n",
    "subject_pool, exam_pool = [], []\n",
    "pred_pool, label_pool, machine_pool = [], [], []\n",
    "age_pool, race_pool, bmi_pool = [], [], []\n",
    "birads_pool, libra_pool = [], []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=12345)\n",
    "fold = 0\n",
    "\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t1), 1)), ys_t1):\n",
    "    fold += 1\n",
    "    # train-val-test idx.\n",
    "    train_y = ys_t1[train_ix_]\n",
    "    \n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(\n",
    "        train_ix_, test_size=test_prop, \n",
    "        stratify=train_y, random_state=12345)\n",
    "    \n",
    "    # subset.\n",
    "    train_dataset = Subset(risk_dataset_t1, train_ix)\n",
    "    val_dataset = Subset(risk_dataset_t1, val_ix)\n",
    "    test_dataset = Subset(risk_dataset_t1, test_ix)\n",
    "    train_y = ys_t1[train_ix]\n",
    "    val_y = ys_t1[val_ix]\n",
    "    test_y = ys_t1[test_ix]\n",
    "    \n",
    "    # weighted sampler.\n",
    "    f0, f1 = np.bincount(train_y)\n",
    "    \n",
    "    train_w = np.zeros_like(train_y, dtype='float')\n",
    "    train_w[train_y==0] = 1/f0\n",
    "    train_w[train_y==1] = 1/f1\n",
    "    \n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        train_w, len(train_y)//batch_size*batch_size, \n",
    "        replacement=True)\n",
    "    # data loaders.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "        collate_fn=mammo_collate)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    \n",
    "    # Reset model before training.\n",
    "    model, device_ = load_model(image_only_parameters)\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    # train & test.\n",
    "    best_name_ = 'best_model_{}_t1.pt'.format(fold)\n",
    "    print('='*10, 'Fold', fold, '='*10)\n",
    "    _, start_auc = val_loss(model, test_loader, device, return_auc=True)\n",
    "    start_auc_m = test_max_auc(model, test_loader, device)\n",
    "    \n",
    "    print('Test AUC at start={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        start_auc, start_auc_m))\n",
    "    \n",
    "    train(model, train_loader, val_loader, best_name_, device, \n",
    "          epochs=epochs, lr=1e-5, check_iters=5, log_name='finetune_t1.txt')\n",
    "    \n",
    "    print('Predicting on the test set...', end='')\n",
    "    subject_list, exam_list, \\\n",
    "    pred_list, label_list, machine_list, \\\n",
    "    age_list, race_list, bmi_list, \\\n",
    "    birads_list, libra_list = do_test(model, test_loader, device)\n",
    "    subject_pool.extend(subject_list)\n",
    "    exam_pool.extend(exam_list)\n",
    "    pred_pool.extend(pred_list)\n",
    "    label_pool.extend(label_list)\n",
    "    machine_pool.extend(machine_list)\n",
    "    age_pool.extend(age_list)\n",
    "    race_pool.extend(race_list)\n",
    "    bmi_pool.extend(bmi_list)\n",
    "    birads_pool.extend(birads_list)\n",
    "    libra_pool.extend(libra_list) \n",
    "    # test AUC.\n",
    "    _, fold_auc = val_loss(model, test_loader, device, return_auc=True)\n",
    "    fold_auc_m = test_max_auc(model, test_loader, device)\n",
    "    print('Done')\n",
    "    print('Test AUC after training={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        fold_auc, fold_auc_m))\n",
    "    \n",
    "    writer = open('finetune_t1.txt', 'a')\n",
    "    writer.write('Test AUC after training={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        fold_auc, fold_auc_m) + '\\n')\n",
    "    \n",
    "    if fold < n_folds:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1 = np.concatenate(subject_pool)\n",
    "all_exam_t1 = np.concatenate(exam_pool)\n",
    "all_preds_t1 = torch.cat(pred_pool)\n",
    "all_labels_t1 = torch.cat(label_pool)\n",
    "all_preds_t1 = all_preds_t1.cpu().numpy()\n",
    "all_labels_t1 = all_labels_t1.numpy()\n",
    "all_probs_max_t1 = all_preds_t1.max(1)\n",
    "all_machines_t1 = np.concatenate(machine_pool)\n",
    "all_ages_t1 = np.concatenate(age_pool)\n",
    "all_races_t1 = np.concatenate(race_pool)\n",
    "all_bmis_t1 = np.concatenate(bmi_pool)\n",
    "all_birads_t1 = np.concatenate(birads_pool)\n",
    "all_libras_t1 = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=1052\n",
      "4view max AUC=0.670\n",
      "4view mean AUC=0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of model failed: Traceback (most recent call last):\n",
      "  File \"/home/dasw/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/dasw/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/dasw/anaconda3/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/dasw/anaconda3/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 860, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/dasw/bc_risk_pred/breast_cancer_classifier/model.py\", line 50\n",
      "    def train_all_views()\n",
      "                        ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1, all_probs_max_t1)))\n",
    "print('4view mean AUC={:.3f}'.format(roc_auc_score(all_labels_t1, all_preds_t1.mean(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
