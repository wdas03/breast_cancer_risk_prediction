{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Adam Yala's code for model creation to Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../OncoNet_Public')\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MIT model and populate it with pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import onconet.utils.parsing_with_line as parsing\n",
    "import onconet.models.factory as model_factory\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "def init_model(multi_gpu=False):\n",
    "    # A long string to setup the Resnet model.\n",
    "    cmd_line = '''\n",
    "    --model_name custom_resnet --pool_name GlobalMaxPool \n",
    "    --block_layout BasicBlock,2 BasicBlock,2 BasicBlock,2 BasicBlock,2 \n",
    "    --dropout 0 --num_chan 3 --pretrained_on_imagenet\n",
    "    '''\n",
    "    args = parsing.parse_args(cmd_line)\n",
    "    model = model_factory.get_model(args)\n",
    "    model = model._model\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            'mods/mgh_mammo_cancer_5yr_risk_img_only_aug08_2018_state.pt'\n",
    "        ))\n",
    "    if multi_gpu:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained_weights for 100 out of 122 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = init_model(multi_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it with a dummy image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2048, 1664])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_img = torch.zeros(1, 3, 2048, 1664).to(device)\n",
    "dummy_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_, hidden_, x_ = model(dummy_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5265, -1.0262]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert logits to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3775, 0.6225]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "F.softmax(logit_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune MIT model on MSSM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are my custom functions to convert images to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mit_risk_data import MammoBCRiskDataset, Resize, FaceLeft, NormalizePix, ToTensor3D, mammo_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the 3+ yr dataset. There are three datasets in total: 1 yr, 1+ yr and 3+ yr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_dataset_t3 = torch.load('../time_set/risk_pred_GEHolo_4view_matchedSepCase_T3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(risk_dataset_t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = risk_dataset_t3[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject', 'exam', 'label', 'machine', 'age', 'race', 'bmi', 'birads', 'libra', 'years_before_dx', 'images'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get labels for all samples for stratified splitting. This can take a while, so I save the labels to load them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys_t3 = np.array([ sample['label'].item() for sample in risk_dataset_t3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('tmp/ys_t3.npy', ys_t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Store variables with pickle\n",
    "def load_pkl(filename):\n",
    "    var = pickle.load(open(filename, 'rb'))\n",
    "    print('Loaded data from:', filename)\n",
    "    return var\n",
    "\n",
    "def store_pkl(var, filename):\n",
    "    pickle.dump(var, open(filename, 'wb'), protocol=4)\n",
    "    print('Stored data in:', filename)\n",
    "\n",
    "# Display images\n",
    "def display_image(num):\n",
    "    return Image(filename='images_png_2048x1664/{0:08d}.png'.format(num)) \n",
    "\n",
    "def display_tensor(tensor):\n",
    "    plt.imshow(tensor.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored data in: ys_t3.pkl\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('ys_t3.pkl'):\n",
    "    ys_t3 = np.array([ sample['label'].item() for sample in risk_dataset_t3])\n",
    "    store_pkl(ys_t3, 'ys_t3.pkl')\n",
    "else:\n",
    "    ys_t3 = load_pkl('ys_t3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# of controls and cases, the size of control is exactly three times the size of case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([192,  64]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ys_t3, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test train-val-test split for K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - [64 21], val - [64 21], test - [64 22]\n",
      "train - [64 21], val - [64 22], test - [64 21]\n",
      "train - [64 21], val - [64 22], test - [64 21]\n"
     ]
    }
   ],
   "source": [
    "n_folds = 3\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=12345)\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t3), 1)), ys_t3):\n",
    "    train_y = ys_t3[train_ix_]\n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(train_ix_, test_size=test_prop, \n",
    "                                        stratify=train_y, \n",
    "                                        random_state=12345)\n",
    "    print('train - {}, val - {}, test - {}'.format(\n",
    "        np.bincount(ys_t3[train_ix]), np.bincount(ys_t3[val_ix]), np.bincount(ys_t3[test_ix])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(risk_dataset_t3, train_ix)\n",
    "val_dataset = Subset(risk_dataset_t3, val_ix)\n",
    "test_dataset = Subset(risk_dataset_t3, test_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = ys_t3[train_ix]\n",
    "val_y = ys_t3[val_ix]\n",
    "test_y = ys_t3[test_ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use weighted sampler for the train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0, f1 = np.bincount(train_y)\n",
    "train_w = np.zeros_like(train_y, dtype='float')\n",
    "train_w[train_y==0] = 1/f0\n",
    "train_w[train_y==1] = 1/f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cpu_threads = 4\n",
    "batch_size = 1\n",
    "\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    train_w, len(train_y)//batch_size*batch_size, \n",
    "    replacement=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, \n",
    "    num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "    collate_fn=mammo_collate)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, \n",
    "    num_workers=cpu_threads, drop_last=False,\n",
    "    collate_fn=mammo_collate)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, \n",
    "    num_workers=cpu_threads, drop_last=False,\n",
    "    collate_fn=mammo_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loading a batch of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_sample = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject', 'exam', 'machine', 'age', 'race', 'bmi', 'birads', 'libra', 'years_before_dx', 'label', 'images'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bat_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 2048, 1664]), torch.Size([1, 1, 2048, 1664]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bat_sample['images']['same-cc'].shape, bat_sample['images']['same-mlo'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 1, 2048, 1664] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-45ac033ef95a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbat_logit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbat_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'same-cc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bc_risk_pred/OncoNet_Public/onconet/models/resnet_base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, risk_factors)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bc_risk_pred/OncoNet_Public/onconet/models/resnet_base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 1, 2048, 1664] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    bat_logit, _, _ = model(bat_sample['images']['same-cc'].float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(bat_logit)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to calculate avg loss and AUC for one sweep of a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def val_loss(model, val_loader, return_auc=False):\n",
    "    '''Avg loss and AUC for an entire data loader\n",
    "    '''\n",
    "    criterion_ = nn.NLLLoss(reduction='sum').to(device)\n",
    "    model.eval()\n",
    "    val_loss, n = 0, 0\n",
    "    y_pool, p_pool = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            bat_X_1 = batch['images']['same-cc'].float().to(device)\n",
    "            bat_X_2 = batch['images']['same-mlo'].float().to(device)\n",
    "            bat_y = batch['label'].long().to(device)\n",
    "            bat_logit_1, _, _ = model(bat_X_1)\n",
    "            bat_logit_2, _, _ = model(bat_X_2)\n",
    "            bat_logp_1 = F.log_softmax(bat_logit_1)\n",
    "            bat_logp_2 = F.log_softmax(bat_logit_2)\n",
    "            loss_1 = criterion_(bat_logp_1, bat_y)\n",
    "            loss_2 = criterion_(bat_logp_2, bat_y)\n",
    "            val_loss += loss_1.item()\n",
    "            val_loss += loss_2.item()\n",
    "            n += len(bat_X_1)\n",
    "            n += len(bat_X_2)\n",
    "            y_pool.append(bat_y)\n",
    "            y_pool.append(bat_y)  # same label, different view.\n",
    "            p_pool.append(torch.exp(bat_logp_1))\n",
    "            p_pool.append(torch.exp(bat_logp_2))\n",
    "        if return_auc:\n",
    "            ys = torch.cat(y_pool)\n",
    "            ps = torch.cat(p_pool)\n",
    "            ys = ys.cpu().detach().numpy()\n",
    "            ps = ps.cpu().detach().numpy()\n",
    "            val_auc = roc_auc_score(ys, ps[:, 1])\n",
    "            return val_loss/n, val_auc\n",
    "    return val_loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "train_loss_, train_auc_ = val_loss(model, train_loader, return_auc=True)\n",
    "print('Init avg train loss={:.3f}, auc={:.3f}'.format(train_loss_, train_auc_))\n",
    "val_loss_, val_auc_ = val_loss(model, val_loader, return_auc=True)\n",
    "print('Init avg val loss={:.3f}, auc={:.3f}'.format(val_loss_, val_auc_))\n",
    "test_loss_, test_auc_ = val_loss(model, test_loader, return_auc=True)\n",
    "print('Init avg test loss={:.3f}, auc={:.3f}'.format(test_loss_, test_auc_))\n",
    "duration = time() - start\n",
    "print('Time elapsed={:.1f}'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization Notes:**\n",
    "- Adam with lr=1e-5 worked fine. Train and val loss showed robust decrease. Val AUC somehow decreased in the beginning but clearly trended upward. Best val AUC=0.55; test AUC increased to 0.59.\n",
    "- Adam with lr=1e-4 clearly sped up learning with training loss quickly driven towards zero. This led to overfitting but the val AUCs all looked good with the best val AUC=0.61. The test AUC was 0.62 although the test loss was high.\n",
    "- 2nd try with lr=1e-4, best model saved, led to val AUC=0.63 but test AUC=0.52.\n",
    "- 2nd try with lr=1e-5, best model saved, led to val AUC=0.56; test AUC=0.57.\n",
    "- Adam with lr=1e-6 led to slow decrease of train and val loss; val AUC hovers around 0.5. Test AUC was 0.59.\n",
    "- Increased lr to 3e-6, got similar results.\n",
    "\n",
    "Based on 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finetuning test run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "epochs = 20\n",
    "train_loss = 0\n",
    "check_iters = 10\n",
    "best_name = 'best_model.pt'\n",
    "best_auc = .0\n",
    "start = time()\n",
    "for i in range(start_epoch, start_epoch + epochs):\n",
    "    for j, batch in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Each iteration is equivalent to 2*batch_size.\n",
    "        bat_X_1 = batch['images']['same-cc'].float().to(device)\n",
    "        bat_X_2 = batch['images']['same-mlo'].float().to(device)\n",
    "        bat_y = batch['label'].long().to(device)\n",
    "        # forward-backward CC view images.\n",
    "        bat_logit_1, _, _ = model(bat_X_1)\n",
    "        bat_logp_1 = F.log_softmax(bat_logit_1)\n",
    "        loss = criterion(bat_logp_1, bat_y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()/2\n",
    "        # forward-backward MLO view images.\n",
    "        bat_logit_2, _, _ = model(bat_X_2)\n",
    "        bat_logp_2 = F.log_softmax(bat_logit_2)\n",
    "        loss = criterion(bat_logp_2, bat_y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()/2\n",
    "        # accumulate gradients from both CC and MLO images and \n",
    "        # then take a step to update.\n",
    "        optimizer.step()\n",
    "        total_iters = i*len(train_loader) + j + 1\n",
    "        if total_iters%check_iters == 0:\n",
    "            avg_val_loss, val_auc = val_loss(model, val_loader, return_auc=True)\n",
    "            avg_train_loss = train_loss/check_iters\n",
    "            print(\"Iter={}, avg train loss={:.3f}, avg val loss={:.3f}, auc={:.3f}\".format(\n",
    "                total_iters, avg_train_loss, avg_val_loss, val_auc))\n",
    "            if val_auc > best_auc:\n",
    "                best_auc = val_auc\n",
    "                torch.save(model.state_dict(), best_name)\n",
    "                print(\"Best model saved.\")\n",
    "            train_loss = 0\n",
    "duration = time() - start\n",
    "print('Time elapsed={:.1f}'.format(duration))\n",
    "model.load_state_dict(torch.load(best_name))\n",
    "print('Best model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_loss, test_auc = val_loss(model, test_loader, return_auc=True)\n",
    "print('Finetuned avg test loss={:.3f}, auc={:.3f}'.format(avg_test_loss, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to calculate AUC based on the max score of the four views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_max_auc(model, test_loader):\n",
    "    '''AUC based on max prediction of 4 views\n",
    "    '''\n",
    "    model.eval()\n",
    "    y_pool, p_pool = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            bat_X_1 = batch['images']['same-cc'].float().to(device)\n",
    "            bat_X_2 = batch['images']['same-mlo'].float().to(device)\n",
    "            bat_X_3 = batch['images']['opposite-cc'].float().to(device)\n",
    "            bat_X_4 = batch['images']['opposite-mlo'].float().to(device)\n",
    "            bat_y = batch['label'].long().to(device)\n",
    "            bat_logit_1, _, _ = model(bat_X_1)\n",
    "            bat_logit_2, _, _ = model(bat_X_2)\n",
    "            bat_logit_3, _, _ = model(bat_X_3)\n",
    "            bat_logit_4, _, _ = model(bat_X_4)\n",
    "            bat_p_1 = F.softmax(bat_logit_1)[:, 1]\n",
    "            bat_p_2 = F.softmax(bat_logit_2)[:, 1]\n",
    "            bat_p_3 = F.softmax(bat_logit_3)[:, 1]\n",
    "            bat_p_4 = F.softmax(bat_logit_4)[:, 1]\n",
    "            bat_p = torch.stack(\n",
    "                [bat_p_1, bat_p_2, bat_p_3, bat_p_4], \n",
    "                dim=1)\n",
    "            p_pool.append(bat_p)\n",
    "            y_pool.append(bat_y)\n",
    "        ys = torch.cat(y_pool)\n",
    "        ps = torch.cat(p_pool)\n",
    "        max_ps = ps.max(1).values\n",
    "        ys = ys.cpu().detach().numpy()\n",
    "        max_ps = max_ps.cpu().detach().numpy()\n",
    "        test_auc = roc_auc_score(ys, max_ps)\n",
    "        return test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auc_m = test_max_auc(model, test_loader)\n",
    "print('Finetuned max-score-based auc={:.3f}'.format(test_auc_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up train-test routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, best_name, \n",
    "          epochs=3, lr=1e-6, weight_decay=1e-4, \n",
    "          check_iters=30, log_name=None):\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    writer = SummaryWriter(log_name) if log_name is not None else None\n",
    "    train_loss = 0\n",
    "    best_auc = .0\n",
    "    for i in range(epochs):\n",
    "        for j, batch in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Each iteration is equivalent to 2*batch_size.\n",
    "            bat_X_1 = batch['images']['same-cc'].float().to(device)\n",
    "            bat_X_2 = batch['images']['same-mlo'].float().to(device)\n",
    "            bat_y = batch['label'].long().to(device)\n",
    "            # forward-backward CC view images.\n",
    "            bat_logit_1, _, _ = model(bat_X_1)\n",
    "            bat_logp_1 = F.log_softmax(bat_logit_1)\n",
    "            loss = criterion(bat_logp_1, bat_y)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()/2\n",
    "            # forward-backward MLO view images.\n",
    "            bat_logit_2, _, _ = model(bat_X_2)\n",
    "            bat_logp_2 = F.log_softmax(bat_logit_2)\n",
    "            loss = criterion(bat_logp_2, bat_y)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()/2\n",
    "            optimizer.step()\n",
    "            total_iters = i*len(train_loader) + j + 1\n",
    "            if total_iters%check_iters == 0:\n",
    "                avg_val_loss, val_auc = val_loss(model, val_loader, return_auc=True)\n",
    "                avg_train_loss = train_loss/check_iters\n",
    "                print(\"Iter={}, avg train loss={:.3f}, \"\n",
    "                      \"avg val loss={:.3f}, auc={:.3f}\".format(\n",
    "                    total_iters, avg_train_loss, avg_val_loss, val_auc))\n",
    "                if val_auc > best_auc:\n",
    "                    best_auc = val_auc\n",
    "                    torch.save(model.state_dict(), best_name)\n",
    "                    print(\"Best model saved.\")\n",
    "                if writer is not None:\n",
    "                    writer.add_scalar('Loss/train', avg_train_loss, total_iters)\n",
    "                    writer.add_scalar('Loss/val', avg_val_loss, total_iters)\n",
    "                train_loss = 0\n",
    "#                 break\n",
    "    print(\"Best model loaded.\")\n",
    "    model.load_state_dict(torch.load(best_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_test(model, test_loader):\n",
    "    '''Avg loss and AUC for an entire data loader\n",
    "    '''\n",
    "    model.eval()\n",
    "    subject_list, exam_list = [], []\n",
    "    pred_list, label_list, machine_list = [], [], []\n",
    "    age_list, race_list, bmi_list = [], [], []\n",
    "    birads_list, libra_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            bat_X_1 = batch['images']['same-cc'].float().to(device)\n",
    "            bat_X_2 = batch['images']['same-mlo'].float().to(device)\n",
    "            bat_X_3 = batch['images']['opposite-cc'].float().to(device)\n",
    "            bat_X_4 = batch['images']['opposite-mlo'].float().to(device)\n",
    "            bat_y = batch['label'].long().to(device)\n",
    "            bat_logit_1, _, _ = model(bat_X_1)\n",
    "            bat_logit_2, _, _ = model(bat_X_2)\n",
    "            bat_logit_3, _, _ = model(bat_X_3)\n",
    "            bat_logit_4, _, _ = model(bat_X_4)\n",
    "            bat_p_1 = F.softmax(bat_logit_1)[:, 1]\n",
    "            bat_p_2 = F.softmax(bat_logit_2)[:, 1]\n",
    "            bat_p_3 = F.softmax(bat_logit_3)[:, 1]\n",
    "            bat_p_4 = F.softmax(bat_logit_4)[:, 1]\n",
    "            bat_p = torch.stack(\n",
    "                [bat_p_1, bat_p_2, bat_p_3, bat_p_4], \n",
    "                dim=1)\n",
    "            subject_list.append(batch['subject'])\n",
    "            exam_list.append(batch['exam'])\n",
    "            pred_list.append(bat_p)\n",
    "            label_list.append(batch['label'])            \n",
    "            machine_list.append(batch['machine'])\n",
    "            age_list.append(batch['age'])\n",
    "            race_list.append(batch['race'])\n",
    "            bmi_list.append(batch['bmi'])\n",
    "            birads_list.append(batch['birads'])\n",
    "            libra_list.append(batch['libra'])            \n",
    "    return(subject_list, exam_list, pred_list, label_list, \n",
    "           machine_list, age_list, race_list, bmi_list, \n",
    "           birads_list, libra_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risk prediction 3+ years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "cpu_threads = 4\n",
    "batch_size = 4\n",
    "n_folds = 5\n",
    "epochs = 20\n",
    "subject_pool, exam_pool = [], []\n",
    "pred_pool, label_pool, machine_pool = [], [], []\n",
    "age_pool, race_pool, bmi_pool = [], [], []\n",
    "birads_pool, libra_pool = [], []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=12345)\n",
    "fold = 0\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t3), 1)), ys_t3):\n",
    "    fold += 1\n",
    "    # train-val-test idx.\n",
    "    train_y = ys_t3[train_ix_]\n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(\n",
    "        train_ix_, test_size=test_prop, \n",
    "        stratify=train_y, random_state=12345)\n",
    "    # subset.\n",
    "    train_dataset = Subset(risk_dataset_t3, train_ix)\n",
    "    val_dataset = Subset(risk_dataset_t3, val_ix)\n",
    "    test_dataset = Subset(risk_dataset_t3, test_ix)\n",
    "    train_y = ys_t3[train_ix]\n",
    "    val_y = ys_t3[val_ix]\n",
    "    test_y = ys_t3[test_ix]\n",
    "    # weighted sampler.\n",
    "    f0, f1 = np.bincount(train_y)\n",
    "    train_w = np.zeros_like(train_y, dtype='float')\n",
    "    train_w[train_y==0] = 1/f0\n",
    "    train_w[train_y==1] = 1/f1\n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        train_w, len(train_y)//batch_size*batch_size, \n",
    "        replacement=True)\n",
    "    # data loaders.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "        collate_fn=mammo_collate)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size*2, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size*2, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    # Reset model before training.\n",
    "    model = init_model(multi_gpu=False)\n",
    "    # train & test.\n",
    "    best_name_ = 'best_model_{}.pt'.format(fold)\n",
    "    print('='*10, 'Fold', fold, '='*10)\n",
    "    _, start_auc = val_loss(model, test_loader, return_auc=True)\n",
    "    start_auc_m = test_max_auc(model, test_loader)\n",
    "    print('Test AUC at start={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        start_auc, start_auc_m))\n",
    "    train(model, train_loader, val_loader, best_name_, \n",
    "          epochs=epochs, lr=1e-5, check_iters=10)\n",
    "    print('Predicting on the test set...', end='')\n",
    "    subject_list, exam_list, \\\n",
    "    pred_list, label_list, machine_list, \\\n",
    "    age_list, race_list, bmi_list, \\\n",
    "    birads_list, libra_list = do_test(model, test_loader)\n",
    "    subject_pool.extend(subject_list)\n",
    "    exam_pool.extend(exam_list)\n",
    "    pred_pool.extend(pred_list)\n",
    "    label_pool.extend(label_list)\n",
    "    machine_pool.extend(machine_list)\n",
    "    age_pool.extend(age_list)\n",
    "    race_pool.extend(race_list)\n",
    "    bmi_pool.extend(bmi_list)\n",
    "    birads_pool.extend(birads_list)\n",
    "    libra_pool.extend(libra_list)\n",
    "    # test AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the results from all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t3 = np.concatenate(subject_pool)\n",
    "all_exam_t3 = np.concatenate(exam_pool)\n",
    "all_preds_t3 = torch.cat(pred_pool)\n",
    "all_labels_t3 = torch.cat(label_pool)\n",
    "all_preds_t3 = all_preds_t3.cpu().numpy()\n",
    "all_labels_t3 = all_labels_t3.numpy()\n",
    "all_probs_max_t3 = all_preds_t3.max(1)\n",
    "all_machines_t3 = np.concatenate(machine_pool)\n",
    "all_ages_t3 = np.concatenate(age_pool)\n",
    "all_races_t3 = np.concatenate(race_pool)\n",
    "all_bmis_t3 = np.concatenate(bmi_pool)\n",
    "all_birads_t3 = np.concatenate(birads_pool)\n",
    "all_libras_t3 = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t3)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t3, all_probs_max_t3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t3 = [ '{:06d}'.format(s) for s in all_subj_t3]\n",
    "all_subj_t3[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_t3 = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t3, 'exam': all_exam_t3, 'is_ge': all_machines_t3, \n",
    "     'age': all_ages_t3, 'race': all_races_t3, 'bmi': all_bmis_t3, \n",
    "     'birads': all_birads_t3,})\n",
    "df_t3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = np.concatenate([all_preds_t3, all_labels_t3[:, np.newaxis]], axis=1)\n",
    "d_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t3 = pd.concat([df_t3, pd.DataFrame(d_)], axis=1)\n",
    "df_t3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t3 = df_t3.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t3 = df_t3.astype({'is_case': 'int8'})\n",
    "df_t3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t3.to_csv('time_set/finetuned_pred_score_4view_T3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also calculate the max score based AUC using the model without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_threads = 4\n",
    "batch_size = 4\n",
    "\n",
    "all_loader_t3 = DataLoader(\n",
    "    risk_dataset_t3, batch_size=batch_size*2, \n",
    "    num_workers=cpu_threads, drop_last=False,\n",
    "    collate_fn=mammo_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = init_model(multi_gpu=False)\n",
    "all_auc_m = test_max_auc(model_0, all_loader_t3)\n",
    "print('MIT unchanged max-score-based auc={:.3f}'.format(all_auc_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risk prediction 1+ years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_dataset_t1p = torch.load('time_set/risk_pred_GEHolo_4view_matchedSepCase_T1+.pt')\n",
    "len(risk_dataset_t1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys_t1p = np.array([ sample['label'].item() for sample in risk_dataset_t1p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('tmp/ys_t1p.npy', ys_t1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_t1p = np.load('tmp/ys_t1p.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ys_t1p, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "cpu_threads = 3\n",
    "batch_size = 4\n",
    "n_folds = 5\n",
    "epochs = 20\n",
    "subject_pool, exam_pool = [], []\n",
    "pred_pool, label_pool, machine_pool = [], [], []\n",
    "age_pool, race_pool, bmi_pool = [], [], []\n",
    "birads_pool, libra_pool = [], []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1234)\n",
    "fold = 0\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t1p), 1)), ys_t1p):\n",
    "    fold += 1\n",
    "    # train-val-test idx.\n",
    "    train_y = ys_t1p[train_ix_]\n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(\n",
    "        train_ix_, test_size=test_prop, \n",
    "        stratify=train_y, random_state=12345)\n",
    "    # subset.\n",
    "    train_dataset = Subset(risk_dataset_t1p, train_ix)\n",
    "    val_dataset = Subset(risk_dataset_t1p, val_ix)\n",
    "    test_dataset = Subset(risk_dataset_t1p, test_ix)\n",
    "    train_y = ys_t1p[train_ix]\n",
    "    val_y = ys_t1p[val_ix]\n",
    "    test_y = ys_t1p[test_ix]\n",
    "    # weighted sampler.\n",
    "    f0, f1 = np.bincount(train_y)\n",
    "    train_w = np.zeros_like(train_y, dtype='float')\n",
    "    train_w[train_y==0] = 1/f0\n",
    "    train_w[train_y==1] = 1/f1\n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        train_w, len(train_y)//batch_size*batch_size, \n",
    "        replacement=True)\n",
    "    # data loaders.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "        collate_fn=mammo_collate)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size*2, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size*2, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    # Reset model before training.\n",
    "    model = init_model(multi_gpu=False)\n",
    "    # train & test.\n",
    "    best_name_ = 'tmp/best_model_{}.pt'.format(fold)\n",
    "    print('='*10, 'Fold', fold, '='*10)\n",
    "    _, start_auc = val_loss(model, test_loader, return_auc=True)\n",
    "    start_auc_m = test_max_auc(model, test_loader)\n",
    "    print('Test AUC at start={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        start_auc, start_auc_m))\n",
    "    train(model, train_loader, val_loader, best_name_, \n",
    "          epochs=epochs, lr=1e-5, check_iters=10)\n",
    "    print('Predicting on the test set...', end='')\n",
    "    subject_list, exam_list, \\\n",
    "    pred_list, label_list, machine_list, \\\n",
    "    age_list, race_list, bmi_list, \\\n",
    "    birads_list, libra_list = do_test(model, test_loader)\n",
    "    subject_pool.extend(subject_list)\n",
    "    exam_pool.extend(exam_list)\n",
    "    pred_pool.extend(pred_list)\n",
    "    label_pool.extend(label_list)\n",
    "    machine_pool.extend(machine_list)\n",
    "    age_pool.extend(age_list)\n",
    "    race_pool.extend(race_list)\n",
    "    bmi_pool.extend(bmi_list)\n",
    "    birads_pool.extend(birads_list)\n",
    "    libra_pool.extend(libra_list)\n",
    "    # test AUC.\n",
    "    _, fold_auc = val_loss(model, test_loader, return_auc=True)\n",
    "    fold_auc_m = test_max_auc(model, test_loader)\n",
    "    print('Done')\n",
    "    print('Test AUC after training={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        fold_auc, fold_auc_m))\n",
    "    if fold < n_folds:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1p = np.concatenate(subject_pool)\n",
    "all_exam_t1p = np.concatenate(exam_pool)\n",
    "all_preds_t1p = torch.cat(pred_pool)\n",
    "all_labels_t1p = torch.cat(label_pool)\n",
    "all_preds_t1p = all_preds_t1p.cpu().numpy()\n",
    "all_labels_t1p = all_labels_t1p.numpy()\n",
    "all_probs_max_t1p = all_preds_t1p.max(1)\n",
    "all_machines_t1p = np.concatenate(machine_pool)\n",
    "all_ages_t1p = np.concatenate(age_pool)\n",
    "all_races_t1p = np.concatenate(race_pool)\n",
    "all_bmis_t1p = np.concatenate(bmi_pool)\n",
    "all_birads_t1p = np.concatenate(birads_pool)\n",
    "all_libras_t1p = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1p)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1p, all_probs_max_t1p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1p = [ '{:06d}'.format(s) for s in all_subj_t1p]\n",
    "all_subj_t1p[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_t1p = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t1p, 'exam': all_exam_t1p, 'is_ge': all_machines_t1p, \n",
    "     'age': all_ages_t1p, 'race': all_races_t1p, 'bmi': all_bmis_t1p, \n",
    "     'birads': all_birads_t1p,})\n",
    "d_ = np.concatenate([all_preds_t1p, all_labels_t1p[:, np.newaxis]], axis=1)\n",
    "df_t1p = pd.concat([df_t1p, pd.DataFrame(d_)], axis=1)\n",
    "df_t1p = df_t1p.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t1p = df_t1p.astype({'is_case': 'int8'})\n",
    "df_t1p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1p.to_csv('time_set/finetuned_pred_score_4view_T1p_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_threads = 4\n",
    "batch_size = 4\n",
    "\n",
    "all_loader_t1p = DataLoader(\n",
    "    risk_dataset_t1p, batch_size=batch_size*2, \n",
    "    num_workers=cpu_threads, drop_last=False,\n",
    "    collate_fn=mammo_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = init_model(multi_gpu=False)\n",
    "all_auc_m = test_max_auc(model_0, all_loader_t1p)\n",
    "print('MIT unchanged max-score-based auc={:.3f}'.format(all_auc_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risk prediction <1 year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_dataset_t1 = torch.load('time_set/risk_pred_GEHolo_4view_matchedSepCase_T1.pt')\n",
    "len(risk_dataset_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys_t1 = np.array([ sample['label'].item() for sample in risk_dataset_t1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('tmp/ys_t1.npy', ys_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_t1 = np.load('tmp/ys_t1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ys_t1, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "cpu_threads = 3\n",
    "batch_size = 4\n",
    "n_folds = 5\n",
    "epochs = 20\n",
    "subject_pool, exam_pool = [], []\n",
    "pred_pool, label_pool, machine_pool = [], [], []\n",
    "age_pool, race_pool, bmi_pool = [], [], []\n",
    "birads_pool, libra_pool = [], []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=12345)\n",
    "fold = 0\n",
    "for train_ix_, test_ix in skf.split(np.ones((len(ys_t1), 1)), ys_t1):\n",
    "    fold += 1\n",
    "    # train-val-test idx.\n",
    "    train_y = ys_t1[train_ix_]\n",
    "    test_prop = (1/n_folds)/(1 - 1/n_folds)\n",
    "    train_ix, val_ix = train_test_split(\n",
    "        train_ix_, test_size=test_prop, \n",
    "        stratify=train_y, random_state=12345)\n",
    "    # subset.\n",
    "    train_dataset = Subset(risk_dataset_t1, train_ix)\n",
    "    val_dataset = Subset(risk_dataset_t1, val_ix)\n",
    "    test_dataset = Subset(risk_dataset_t1, test_ix)\n",
    "    train_y = ys_t1[train_ix]\n",
    "    val_y = ys_t1[val_ix]\n",
    "    test_y = ys_t1[test_ix]\n",
    "    # weighted sampler.\n",
    "    f0, f1 = np.bincount(train_y)\n",
    "    train_w = np.zeros_like(train_y, dtype='float')\n",
    "    train_w[train_y==0] = 1/f0\n",
    "    train_w[train_y==1] = 1/f1\n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        train_w, len(train_y)//batch_size*batch_size, \n",
    "        replacement=True)\n",
    "    # data loaders.\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, \n",
    "        num_workers=cpu_threads, sampler=weighted_sampler,\n",
    "        collate_fn=mammo_collate)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size*2, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size*2, \n",
    "        num_workers=cpu_threads, drop_last=False,\n",
    "        collate_fn=mammo_collate)\n",
    "    # Reset model before training.\n",
    "    model = init_model(multi_gpu=False)\n",
    "    # train & test.\n",
    "    best_name_ = 'tmp/best_model_{}.pt'.format(fold)\n",
    "    print('='*10, 'Fold', fold, '='*10)\n",
    "    _, start_auc = val_loss(model, test_loader, return_auc=True)\n",
    "    start_auc_m = test_max_auc(model, test_loader)\n",
    "    print('Test AUC at start={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        start_auc, start_auc_m))\n",
    "    train(model, train_loader, val_loader, best_name_, \n",
    "          epochs=epochs, lr=1e-5, check_iters=10)\n",
    "    print('Predicting on the test set...', end='')\n",
    "    subject_list, exam_list, \\\n",
    "    pred_list, label_list, machine_list, \\\n",
    "    age_list, race_list, bmi_list, \\\n",
    "    birads_list, libra_list = do_test(model, test_loader)\n",
    "    subject_pool.extend(subject_list)\n",
    "    exam_pool.extend(exam_list)\n",
    "    pred_pool.extend(pred_list)\n",
    "    label_pool.extend(label_list)\n",
    "    machine_pool.extend(machine_list)\n",
    "    age_pool.extend(age_list)\n",
    "    race_pool.extend(race_list)\n",
    "    bmi_pool.extend(bmi_list)\n",
    "    birads_pool.extend(birads_list)\n",
    "    libra_pool.extend(libra_list)\n",
    "    # test AUC.\n",
    "    _, fold_auc = val_loss(model, test_loader, return_auc=True)\n",
    "    fold_auc_m = test_max_auc(model, test_loader)\n",
    "    print('Done')\n",
    "    print('Test AUC after training={:.3f}, max-score-based AUC={:.3f}'.format(\n",
    "        fold_auc, fold_auc_m))\n",
    "    if fold < n_folds:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1 = np.concatenate(subject_pool)\n",
    "all_exam_t1 = np.concatenate(exam_pool)\n",
    "all_preds_t1 = torch.cat(pred_pool)\n",
    "all_labels_t1 = torch.cat(label_pool)\n",
    "all_preds_t1 = all_preds_t1.cpu().numpy()\n",
    "all_labels_t1 = all_labels_t1.numpy()\n",
    "all_probs_max_t1 = all_preds_t1.max(1)\n",
    "all_machines_t1 = np.concatenate(machine_pool)\n",
    "all_ages_t1 = np.concatenate(age_pool)\n",
    "all_races_t1 = np.concatenate(race_pool)\n",
    "all_bmis_t1 = np.concatenate(bmi_pool)\n",
    "all_birads_t1 = np.concatenate(birads_pool)\n",
    "all_libras_t1 = np.concatenate(libra_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('N={}'.format(len(all_labels_t1)))\n",
    "print('4view max AUC={:.3f}'.format(roc_auc_score(all_labels_t1, all_probs_max_t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_t1 = [ '{:06d}'.format(s) for s in all_subj_t1]\n",
    "all_subj_t1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_t1 = pd.DataFrame.from_dict(\n",
    "    {'subject': all_subj_t1, 'exam': all_exam_t1, 'is_ge': all_machines_t1, \n",
    "     'age': all_ages_t1, 'race': all_races_t1, 'bmi': all_bmis_t1, \n",
    "     'birads': all_birads_t1,})\n",
    "d_ = np.concatenate([all_preds_t1, all_labels_t1[:, np.newaxis]], axis=1)\n",
    "df_t1 = pd.concat([df_t1, pd.DataFrame(d_)], axis=1)\n",
    "df_t1 = df_t1.rename(\n",
    "    columns={0: 'ips-cc', 1: 'ips-mlo', 2: 'contra-cc', \n",
    "             3: 'contra-mlo', 4: 'is_case'})\n",
    "df_t1 = df_t1.astype({'is_case': 'int8'})\n",
    "df_t1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1.to_csv('time_set/finetuned_pred_score_4view_T1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_threads = 4\n",
    "batch_size = 4\n",
    "\n",
    "all_loader_t1 = DataLoader(\n",
    "    risk_dataset_t1, batch_size=batch_size*2, \n",
    "    num_workers=cpu_threads, drop_last=False,\n",
    "    collate_fn=mammo_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = init_model(multi_gpu=False)\n",
    "all_auc_m = test_max_auc(model_0, all_loader_t1)\n",
    "print('MIT unchanged max-score-based auc={:.3f}'.format(all_auc_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
